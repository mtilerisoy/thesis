{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Notebook For Quantizing VLP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Distributed Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=2\n",
      "2025-02-07 11:25:40.736683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738923940.753245 2446998 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738923940.758352 2446998 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import configs\n",
    "import copy\n",
    "from sensitivity_utils import get_quantization_config, print_size_of_model, SmallMTDataModuleMETER, SmallMTDataModuleVILT\n",
    "# from vilt.datamodules.multitask_datamodule import MTDataModule as MTDataModuleVILT\n",
    "# from meter.datamodules.multitask_datamodule import MTDataModule as MTDataModuleMeter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "# Limit the number of CPUs\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"  # Set this to the number of CPUs you want to use\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"  # Set this to the number of CPUs you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"promote has been superseded by promote_options='default'\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.helpers is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.layers is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Importing from timm.models.registry is deprecated\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_small_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch16_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_patch32_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch16_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_large_patch32_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_huge_patch14_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50_224_in21k in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50_384 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_small_resnet26d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet26d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Overwriting vit_base_resnet50d_224 in registry\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"You are using `torch.load` with `weights_only=False`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Configuration to Initialize the Datamodule and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the configuration\n",
    "_config = configs.meter_config_nlvr2\n",
    "_config[\"batch_size\"] = 64\n",
    "_config[\"per_gpu_batchsize\"] = 64\n",
    "\n",
    "\n",
    "pl.seed_everything(_config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Datamodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the configuration and initialize the test and full datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['nlvr2_vlue_test']\n",
      "Loaded names: ['nlvr2_vlue_test']\n",
      "Loaded names: ['nlvr2_vlue_test']\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ========= Create full datamodule =========\n",
    "# ==========================================\n",
    "if \"meter\" in _config[\"model\"]:\n",
    "    infer_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=5, start_idx=0)\n",
    "    infer_dm.setup(\"test\", is_random=True)\n",
    "    infer_dataloader = infer_dm.test_dataloader()\n",
    "\n",
    "elif \"vilt\" in _config[\"model\"]:\n",
    "    infer_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=5, start_idx=0)\n",
    "    infer_dm.setup(\"test\", is_random=True)\n",
    "    infer_dataloader = infer_dm.test_dataloader()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported: \", _config[\"model\"])\n",
    "\n",
    "print(f\"Batch size: {_config['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized METER model\n"
     ]
    }
   ],
   "source": [
    "from vilt.modules import ViLTransformerSS\n",
    "from meter.modules import METERTransformerSS\n",
    "\n",
    "if _config[\"model\"] == \"vilt\":\n",
    "    model = ViLTransformerSS(_config)\n",
    "    print(\"Initialized ViLT model\")\n",
    "\n",
    "elif _config[\"model\"] == \"meter\":\n",
    "    model = METERTransformerSS(_config)\n",
    "    print(\"Initialized METER model\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported: \", _config[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['table_name', 'answers', 'text', 'image_1', 'image_0', 'text_ids', 'text_labels', 'text_ids_mlm', 'text_labels_mlm', 'text_masks'])\n",
      "Number of batches: 1\n",
      "Smaples in a batch: 5\n"
     ]
    }
   ],
   "source": [
    "input_ = next(iter(infer_dataloader))\n",
    "print(input_.keys())\n",
    "# print(input_[\"answers\"])\n",
    "print(f\"Number of batches: {len(infer_dataloader)}\")\n",
    "print(f\"Smaples in a batch: {len(input_['answers'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision Model:\n",
      "Size of the model (MB): 1296.258138\n",
      "Size after quantization:\n",
      "Size of the model (MB): 1041.51393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quantization_config, embedding_config = get_quantization_config(2)\n",
    "\n",
    "layer_to_quantize = \"text_transformer.encoder.layer\"\n",
    "# layer_to_quantize = \"transformer.blocks\"\n",
    "\n",
    "\n",
    "model_dynamic = copy.deepcopy(model)\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_dynamic, {layer_to_quantize: quantization_config}, inplace=True\n",
    ")\n",
    "\n",
    "# print(model_dynamic)\n",
    "print(\"Size after quantization:\")\n",
    "print_size_of_model(model_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_module_by_path(model, path):\n",
    "#     \"\"\"\n",
    "#     Access a module in the model by specifying the path as a string.\n",
    "    \n",
    "#     Args:\n",
    "#         model (nn.Module): The PyTorch model.\n",
    "#         path (str): The path to the module, e.g., \"transformer.blocks[0].attn.qkv\".\n",
    "    \n",
    "#     Returns:\n",
    "#         nn.Module: The module at the specified path.\n",
    "#     \"\"\"\n",
    "#     parts = path.split('.')\n",
    "#     current_module = model\n",
    "    \n",
    "#     for part in parts:\n",
    "#         if '[' in part and ']' in part:\n",
    "#             # Handle list indexing, e.g., \"blocks[0]\"\n",
    "#             module_name, index = part.split('[')\n",
    "#             index = int(index[:-1])  # Remove the closing bracket and convert to int\n",
    "#             current_module = getattr(current_module, module_name)[index]\n",
    "#         else:\n",
    "#             # Handle regular attribute access\n",
    "#             current_module = getattr(current_module, part)\n",
    "    \n",
    "#     return current_module\n",
    "\n",
    "# # Example usage\n",
    "# module_path = \"transformer.blocks[0].mlp.fc2\"\n",
    "# qkv_module = get_module_by_path(model, module_path)\n",
    "# print(qkv_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quant vs FP32 Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def plot_weight_distributions(model, model_quant, block):\n",
    "#     \"\"\"Plot weight distributions for fc1 and fc2 layers.\"\"\"\n",
    "#     # Full-precision weights\n",
    "#     fc1_full = model.transformer.blocks[block].mlp.fc1.weight.detach().flatten().cpu().numpy()\n",
    "#     fc2_full = model.transformer.blocks[block].mlp.fc2.weight.detach().flatten().cpu().numpy()\n",
    "    \n",
    "#     # Quantized weights\n",
    "#     fc1_quant = model_quant.transformer.blocks[block].mlp.fc1.weight().dequantize().detach().flatten().cpu().numpy()\n",
    "#     fc2_quant = model_quant.transformer.blocks[block].mlp.fc2.weight().dequantize().detach().flatten().cpu().numpy()\n",
    "    \n",
    "#     # Plot fc1\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     # sns.kdeplot(fc1_full, label='Full Precision', color='blue')\n",
    "#     # sns.kdeplot(fc1_quant, label='Quantized', color='red')\n",
    "#     plt.hist(fc1_full, bins=10, alpha=0.5, label='Full Precision', color='blue')\n",
    "#     plt.hist(fc1_quant, bins=10, alpha=0.5, label='Quantized', color='red')\n",
    "#     plt.title(f'Block {block} MLP.fc1 Weight Distribution')\n",
    "#     plt.xlabel('Weight Value')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # plt.xlim(-2, 2)\n",
    "#     # plt.ylim(0,10)\n",
    "    \n",
    "    \n",
    "#     # Plot fc2\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     # sns.kdeplot(fc2_full, label='Full Precision', color='blue')\n",
    "#     # sns.kdeplot(fc2_quant, label='Quantized', color='red')\n",
    "#     plt.hist(fc2_full, bins=10, alpha=0.5, label='Full Precision', color='blue')\n",
    "#     plt.hist(fc2_quant, bins=10, alpha=0.5, label='Quantized', color='red')\n",
    "#     plt.title(f'Block {block} MLP.fc2 Weight Distribution')\n",
    "#     plt.xlabel('Weight Value')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # plt.xlim(-2, 2)\n",
    "#     # plt.ylim(0,10)\n",
    "    \n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_attn_outs(model, batch, block):\n",
    "    \"\"\"Get attention outputs for a specific block.\"\"\"\n",
    "    co_embeds, co_masks = model.get_co_embeds(batch, block)\n",
    "    \n",
    "    return co_embeds, co_masks\n",
    "\n",
    "def weighted_variance_change(full_var, quant_var):\n",
    "    \"\"\"Calculate weighted variance change.\"\"\"\n",
    "    percent_change = ((quant_var - full_var) / full_var) * 100\n",
    "    return percent_change * math.log(full_var)\n",
    "\n",
    "def analyze_mlp_activations(model, model_quant, batch, block):\n",
    "    \"\"\"Compare activations of MLP layers (fc1 and fc2) for a specific block.\"\"\"\n",
    "\n",
    "    if _config[\"model\"] == \"vilt\":\n",
    "        x, co_masks = model.get_co_embeds(batch, block)\n",
    "        x_quant, co_masks_quant = model_quant.get_co_embeds(batch, block)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Full-precision activations\n",
    "            x_fc1_full = model.transformer.blocks[block].mlp.fc1(x)\n",
    "            x_fc2_full = model.transformer.blocks[block].mlp.fc2(x_fc1_full)\n",
    "            \n",
    "            # Quantized activations\n",
    "            x_fc1_quant = model_quant.transformer.blocks[block].mlp.fc1(x_quant)\n",
    "            x_fc2_quant = model_quant.transformer.blocks[block].mlp.fc2(x_fc1_quant)\n",
    "    \n",
    "    elif _config[\"model\"] == \"meter\":\n",
    "        x_fc1_full, x_fc2_full, final_outputs = model.get_mlp_outputs(batch, block)\n",
    "\n",
    "        x_fc1_quant, x_fc2_quant, final_outputs_quant = model_quant.get_mlp_outputs(batch, block)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Compute metrics for fc1\n",
    "    mse_fc1 = F.mse_loss(x_fc1_full, x_fc1_quant).item()\n",
    "    cosine_fc1 = F.cosine_similarity(x_fc1_full.flatten(), x_fc1_quant.flatten(), dim=0).item()\n",
    "    \n",
    "    # Compute metrics for fc2\n",
    "    mse_fc2 = F.mse_loss(x_fc2_full, x_fc2_quant).item()\n",
    "    cosine_fc2 = F.cosine_similarity(x_fc2_full.flatten(), x_fc2_quant.flatten(), dim=0).item()\n",
    "    \n",
    "    # Activation statistics (mean and variance)\n",
    "    fc1_full_mean = x_fc1_full.mean().item()\n",
    "    fc1_full_var = x_fc1_full.var().item()\n",
    "    fc1_quant_mean = x_fc1_quant.mean().item()\n",
    "    fc1_quant_var = x_fc1_quant.var().item()\n",
    "    \n",
    "    fc2_full_mean = x_fc2_full.mean().item()\n",
    "    fc2_full_var = x_fc2_full.var().item()\n",
    "    fc2_quant_mean = x_fc2_quant.mean().item()\n",
    "    fc2_quant_var = x_fc2_quant.var().item()\n",
    "    \n",
    "    # Percent changes in means and variances\n",
    "    def percent_change(full, quant):\n",
    "        return ((quant - full) / full) * 100\n",
    "    \n",
    "    fc1_mean_change = percent_change(fc1_full_mean, fc1_quant_mean)\n",
    "    fc1_var_change = percent_change(fc1_full_var, fc1_quant_var)\n",
    "    fc2_mean_change = percent_change(fc2_full_mean, fc2_quant_mean)\n",
    "    fc2_var_change = percent_change(fc2_full_var, fc2_quant_var)\n",
    "    \n",
    "    # Weighted variance change\n",
    "    fc1_var_weighted_change = weighted_variance_change(fc1_full_var, fc1_quant_var)\n",
    "    fc2_var_weighted_change = weighted_variance_change(fc2_full_var, fc2_quant_var)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'mse_fc1': mse_fc1,\n",
    "        'cosine_fc1': cosine_fc1,\n",
    "        'mse_fc2': mse_fc2,\n",
    "        'cosine_fc2': cosine_fc2,\n",
    "        'stats': {\n",
    "            'fc1_full_mean': fc1_full_mean,\n",
    "            'fc1_full_var': fc1_full_var,\n",
    "            'fc1_quant_mean': fc1_quant_mean,\n",
    "            'fc1_quant_var': fc1_quant_var,\n",
    "            'fc2_full_mean': fc2_full_mean,\n",
    "            'fc2_full_var': fc2_full_var,\n",
    "            'fc2_quant_mean': fc2_quant_mean,\n",
    "            'fc2_quant_var': fc2_quant_var,\n",
    "            'fc1_mean_change': fc1_mean_change,\n",
    "            'fc1_var_change': fc1_var_change,\n",
    "            'fc2_mean_change': fc2_mean_change,\n",
    "            'fc2_var_change': fc2_var_change,\n",
    "            'fc1_var_weighted_change': fc1_var_weighted_change,\n",
    "            'fc2_var_weighted_change': fc2_var_weighted_change,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# def visualize_mlp_outputs(model, model_quant, batch, block, layer='fc2'):\n",
    "#     \"\"\"Visualize outputs of fc1/fc2 using PCA.\"\"\"\n",
    "#     x, co_masks = model.get_co_embeds(batch)\n",
    "#     x_quant, co_masks_quant = model_quant.get_co_embeds(batch)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Full-precision outputs\n",
    "#         x_fc1_full = model.transformer.blocks[block].mlp.fc1(x)\n",
    "#         x_fc2_full = model.transformer.blocks[block].mlp.fc2(x_fc1_full)\n",
    "        \n",
    "#         # Quantized outputs\n",
    "#         x_fc1_quant = model_quant.transformer.blocks[block].mlp.fc1(x_quant)\n",
    "#         x_fc2_quant = model_quant.transformer.blocks[block].mlp.fc2(x_fc1_quant)\n",
    "    \n",
    "#     # Select layer to visualize\n",
    "#     if layer == 'fc1':\n",
    "#         full_output = x_fc1_full.cpu().numpy()\n",
    "#         quant_output = x_fc1_quant.cpu().numpy()\n",
    "#     else:\n",
    "#         full_output = x_fc2_full.cpu().numpy()\n",
    "#         quant_output = x_fc2_quant.cpu().numpy()\n",
    "    \n",
    "#     # Apply PCA\n",
    "#     pca = PCA(n_components=2)\n",
    "#     pca_full = pca.fit_transform(full_output.reshape(full_output.shape[0], -1))\n",
    "#     pca_quant = pca.transform(quant_output.reshape(quant_output.shape[0], -1))\n",
    "    \n",
    "#     # Plot\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.scatter(pca_full[:, 0], pca_full[:, 1], label='Full Precision', alpha=0.5)\n",
    "#     plt.scatter(pca_quant[:, 0], pca_quant[:, 1], label='Quantized', alpha=0.5)\n",
    "#     plt.title(f'Block {block} MLP.{layer} Output Similarity (PCA)')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 fc1, 0.139047, 0.067519, -288.44, -95.62, 191.245444\n",
      "Block 1 fc1, 0.039054, 0.154413, 8.03, -64.82, 223.946385\n",
      "Block 2 fc1, 0.040365, 0.290057, -44.90, 28.20, -106.651927\n",
      "Block 3 fc1, 0.043985, 0.266469, -62.50, 1.91, -6.864872\n",
      "Block 4 fc1, 0.051086, 0.221976, -68.61, 15.86, -56.574007\n",
      "Block 5 fc1, 0.051729, 0.247183, -83.36, 60.03, -222.085718\n",
      "Block 6 fc1, 0.041326, 0.226783, -67.10, 12.70, -47.937606\n",
      "Block 7 fc1, 0.045406, 0.262018, -73.03, 11.80, -42.618263\n",
      "Block 8 fc1, 0.077738, 0.639988, -72.00, -10.16, 22.231513\n",
      "Block 9 fc1, 0.048419, 0.320433, -73.59, 20.74, -72.635251\n",
      "Block 10 fc1, 0.029230, 0.244010, -75.99, -13.81, 56.665651\n",
      "Block 11 fc1, 0.025074, 0.229656, -72.94, 44.59, -213.854694\n",
      "Block 0 fc2, 0.116811, 0.859665, -6.35, 23.79, -24.368496\n",
      "Block 1 fc2, 0.050451, 0.966949, 8.45, 5.62, -1.752675\n",
      "Block 2 fc2, 0.038324, 0.972023, 1.31, 4.27, -1.748230\n",
      "Block 3 fc2, 0.030557, 0.976623, -0.43, 2.07, -0.912058\n",
      "Block 4 fc2, 0.034972, 0.978451, -0.95, 3.15, -0.732017\n",
      "Block 5 fc2, 0.030341, 0.981707, 1.56, 3.20, -0.676057\n",
      "Block 6 fc2, 0.037307, 0.979241, 1.30, 3.59, -0.481169\n",
      "Block 7 fc2, 0.028336, 0.984288, -6.29, 2.57, -0.316746\n",
      "Block 8 fc2, 0.073110, 0.960426, 0.99, 4.66, -0.516111\n",
      "Block 9 fc2, 0.053113, 0.970644, -0.37, 3.61, -0.453429\n",
      "Block 10 fc2, 0.029468, 0.981278, -0.51, 2.09, -0.531206\n",
      "Block 11 fc2, 0.044801, 0.986759, 64.86, 52.05, -34.760248\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "for i in range(12):\n",
    "\n",
    "    # Quantize MLP layers (fc1 and fc2)\n",
    "    # layer_to_quantize = f\"transformer.blocks.{i}.mlp\"\n",
    "    layer_to_quantize1 = \"text_transformer.encoder.layer.\" + str(i) + \".intermediate\"\n",
    "    \n",
    "    \n",
    "    model_dynamic = deepcopy(model)\n",
    "    torch.quantization.quantize_dynamic(\n",
    "        model_dynamic, {layer_to_quantize1: quantization_config, layer_to_quantize1: quantization_config}, inplace=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    # # Compare weights\n",
    "    # plot_weight_distributions(model, model_dynamic, block=i)\n",
    "    \n",
    "    # Compare activations\n",
    "    results = analyze_mlp_activations(model, model_dynamic, batch=input_, block=i)\n",
    "    print(f\"Block {i} fc1, {results['mse_fc1']:.6f}, {results['cosine_fc1']:.6f}, {results['stats']['fc1_mean_change']:.2f}, {results['stats']['fc1_var_change']:.2f}, {results['stats']['fc1_var_weighted_change']:.6f}\")\n",
    "\n",
    "    # print(f\"Block {i}:\")\n",
    "    # print(f\"  FC1: MSE={results['mse_fc1']:.6f}, Cosine={results['cosine_fc1']:.6f}\")\n",
    "    # print(f\"  FC2: MSE={results['mse_fc2']:.6f}, Cosine={results['cosine_fc2']:.6f}\")\n",
    "    # print(\"  Activation Stats:\")\n",
    "    # print(f\"    FC1 Mean: Full={results['stats']['fc1_full_mean']:.6f}, Quant={results['stats']['fc1_quant_mean']:.6f}, Change={results['stats']['fc1_mean_change']:.2f}%\")\n",
    "    # print(f\"    FC1 Var: Full={results['stats']['fc1_full_var']:.6f}, Quant={results['stats']['fc1_quant_var']:.6f}, Change={results['stats']['fc1_var_change']:.2f}%\")\n",
    "    # print(f\"    FC1 Weighted Var Change: {results['stats']['fc1_var_weighted_change']:.6f}\")\n",
    "    # print(f\"    FC2 Mean: Full={results['stats']['fc2_full_mean']:.6f}, Quant={results['stats']['fc2_quant_mean']:.6f}, Change={results['stats']['fc2_mean_change']:.2f}%\")\n",
    "    # print(f\"    FC2 Var: Full={results['stats']['fc2_full_var']:.6f}, Quant={results['stats']['fc2_quant_var']:.6f}, Change={results['stats']['fc2_var_change']:.2f}%\")\n",
    "    # print(f\"    FC2 Weighted Var Change: {results['stats']['fc2_var_weighted_change']:.6f}\")\n",
    "    # print(\"==========================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "    # # Visualize outputs\n",
    "    # visualize_mlp_outputs(model, model_dynamic, batch=input_, block=i, layer='fc1')\n",
    "    # visualize_mlp_outputs(model, model_dynamic, batch=input_, block=i, layer='fc2')\n",
    "\n",
    "for i in range(12):\n",
    "    layer_to_quantize2 = \"text_transformer.encoder.layer.\" + str(i) + \".output\"\n",
    "    model_dynamic = deepcopy(model)\n",
    "    torch.quantization.quantize_dynamic(\n",
    "        model_dynamic, {layer_to_quantize1: quantization_config, layer_to_quantize2: quantization_config}, inplace=True\n",
    "    )\n",
    "\n",
    "    # Compare activations\n",
    "    results = analyze_mlp_activations(model, model_dynamic, batch=input_, block=i)\n",
    "    print(f\"Block {i} fc2, {results['mse_fc2']:.6f}, {results['cosine_fc2']:.6f}, {results['stats']['fc2_mean_change']:.2f}, {results['stats']['fc2_var_change']:.2f}, {results['stats']['fc2_var_weighted_change']:.6f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_attention_maps(model, batch):\n",
    "    \"\"\"Extract attention weights from all blocks.\"\"\"\n",
    "    attention_maps = {}\n",
    "    \n",
    "    # Register hooks to capture attention weights\n",
    "    hooks = []\n",
    "    for i in range(12):\n",
    "        block = model.transformer.blocks[i].attn\n",
    "        hook = block.register_forward_hook(\n",
    "            lambda module, input, output, block_idx=i: attention_maps.__setitem__(f'block_{block_idx}', output[1].detach())\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Forward pass (replace with your ViLT input format)\n",
    "    with torch.no_grad():\n",
    "        infer1 = model.infer( \n",
    "            batch, mask_text=False, mask_image=False, image_token_type_idx=1\n",
    "        )\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return attention_maps\n",
    "\n",
    "# Get attention maps for both models\n",
    "attn_full = get_attention_maps(model, input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69740/2502827313.py:139: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, attn_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 1\n",
      "Block 2\n",
      "Block 3\n",
      "Block 4\n",
      "Block 5\n",
      "Block 6\n",
      "Block 7\n",
      "Block 8\n",
      "Block 9\n",
      "Block 10\n",
      "Block 11\n",
      "      Layer          MSE    Cosine    Mean (%)    Var (%)  Weighted Var\n",
      "0    0 attn     0.000165  0.938830    0.000006   2.176645    -14.453415\n",
      "1     0 fc1    11.346266  0.880878  -17.475320  -7.426643    -28.702832\n",
      "2     0 fc2  2157.084229  0.538112  -26.656825 -81.284223   -650.490038\n",
      "3    1 attn     0.000194  0.905887    0.000000   1.051419     -7.254471\n",
      "4     1 fc1     8.785393  0.875470  -18.713199  -0.345244     -1.202759\n",
      "5     1 fc2   744.901123  0.374956 -166.489665 -90.500806   -611.745766\n",
      "6    2 attn     0.000248  0.855842    0.000000   8.616831    -61.385724\n",
      "7     2 fc1     4.771626  0.900590   -2.276569   2.643329      8.112577\n",
      "8     2 fc2   438.183472  0.230853  -32.945192 -94.845369   -582.099789\n",
      "9    3 attn     0.000126  0.866064    0.000000  11.308649    -87.736169\n",
      "10    3 fc1     4.602452  0.868584    0.129894   5.296170     14.472023\n",
      "11    3 fc2   126.995857  0.822448  143.814773  16.753210     96.918773\n",
      "12   4 attn     0.000096  0.865098    0.000000  11.744038    -94.591601\n",
      "13    4 fc1     4.727646  0.844956   -2.577616   7.155956     18.320329\n",
      "14    4 fc2   185.469421  0.761142  117.349034  13.067246     76.993718\n",
      "15   5 attn     0.000115  0.839617   -0.000006  14.876296   -119.905457\n",
      "16    5 fc1     3.372925  0.872917    0.093421  -2.161907     -5.360440\n",
      "17    5 fc2   147.922256  0.843579   35.780168  11.151175     67.986146\n",
      "18   6 attn     0.000128  0.817910    0.000000  16.135009   -130.531652\n",
      "19    6 fc1     4.486562  0.813180   -0.924067   7.333305     17.693743\n",
      "20    6 fc2   435.164185  0.523255    9.239637 -61.593633   -393.142628\n",
      "21   7 attn     0.000200  0.758074    0.000000  14.486069   -114.481668\n",
      "22    7 fc1    10.563840  0.462577  -35.745052 -31.209065    -76.011471\n",
      "23    7 fc2   557.677856  0.814602   -2.289057 -24.675242   -182.699496\n",
      "24   8 attn     0.000212  0.694310    0.000000  18.069532   -146.392621\n",
      "25    8 fc1    11.142048  0.313862  -48.653341 -84.353266   -211.438029\n",
      "26    8 fc2   583.579346  0.763807    7.752346 -28.246167   -204.145172\n",
      "27   9 attn     0.000363  0.597641    0.000000   2.475057    -19.178692\n",
      "28    9 fc1    12.439770  0.400703  270.432744 -42.139132   -107.512728\n",
      "29    9 fc2   997.087585  0.357573    8.402698 -72.884746   -511.034509\n",
      "30  10 attn     0.000531  0.361949   -0.000013  44.033658   -353.402610\n",
      "31   10 fc1     6.162435  0.827995   13.206904   2.273885      6.530099\n",
      "32   10 fc2  1049.191895  0.793990  -51.883580   0.861475      6.752194\n",
      "33  11 attn     0.000449  0.508559    0.000000   4.346428    -33.663875\n",
      "34   11 fc1     8.574916  0.753947  -29.156867  15.835638     43.548995\n",
      "35   11 fc2  2216.704346  0.781266  -58.168076 -58.775948   -504.450622\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# def weighted_variance_change(full_var, quant_var):\n",
    "#     \"\"\"Calculate weighted variance change.\"\"\"\n",
    "#     percent_change = ((quant_var - full_var) / full_var) * 100\n",
    "#     return percent_change * math.log(full_var)\n",
    "\n",
    "# def analyze_mlp_activations(model, model_quant, batch, block):\n",
    "#     \"\"\"Compare activations of MLP layers (fc1 and fc2) for a specific block.\"\"\"\n",
    "#     x, co_masks = model.get_co_embeds(batch)\n",
    "#     x_quant, co_masks_quant = model_quant.get_co_embeds(batch)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Full-precision activations\n",
    "#         x_fc1_full = model.transformer.blocks[block].mlp.fc1(x)\n",
    "#         x_fc2_full = model.transformer.blocks[block].mlp.fc2(x_fc1_full)\n",
    "        \n",
    "#         # Quantized activations\n",
    "#         x_fc1_quant = model_quant.transformer.blocks[block].mlp.fc1(x_quant)\n",
    "#         x_fc2_quant = model_quant.transformer.blocks[block].mlp.fc2(x_fc1_quant)\n",
    "    \n",
    "#     # Compute metrics for fc1\n",
    "#     mse_fc1 = F.mse_loss(x_fc1_full, x_fc1_quant).item()\n",
    "#     cosine_fc1 = F.cosine_similarity(x_fc1_full.flatten(), x_fc1_quant.flatten(), dim=0).item()\n",
    "    \n",
    "#     # Compute metrics for fc2\n",
    "#     mse_fc2 = F.mse_loss(x_fc2_full, x_fc2_quant).item()\n",
    "#     cosine_fc2 = F.cosine_similarity(x_fc2_full.flatten(), x_fc2_quant.flatten(), dim=0).item()\n",
    "    \n",
    "#     # Activation statistics (mean and variance)\n",
    "#     fc1_full_mean = x_fc1_full.mean().item()\n",
    "#     fc1_full_var = x_fc1_full.var().item()\n",
    "#     fc1_quant_mean = x_fc1_quant.mean().item()\n",
    "#     fc1_quant_var = x_fc1_quant.var().item()\n",
    "    \n",
    "#     fc2_full_mean = x_fc2_full.mean().item()\n",
    "#     fc2_full_var = x_fc2_full.var().item()\n",
    "#     fc2_quant_mean = x_fc2_quant.mean().item()\n",
    "#     fc2_quant_var = x_fc2_quant.var().item()\n",
    "    \n",
    "#     # Percent changes in means and variances\n",
    "#     def percent_change(full, quant):\n",
    "#         return ((quant - full) / full) * 100\n",
    "    \n",
    "#     fc1_mean_change = percent_change(fc1_full_mean, fc1_quant_mean)\n",
    "#     fc1_var_change = percent_change(fc1_full_var, fc1_quant_var)\n",
    "#     fc2_mean_change = percent_change(fc2_full_mean, fc2_quant_mean)\n",
    "#     fc2_var_change = percent_change(fc2_full_var, fc2_quant_var)\n",
    "    \n",
    "#     # Weighted variance change\n",
    "#     fc1_var_weighted_change = weighted_variance_change(fc1_full_var, fc1_quant_var)\n",
    "#     fc2_var_weighted_change = weighted_variance_change(fc2_full_var, fc2_quant_var)\n",
    "    \n",
    "#     # Return results in a dictionary\n",
    "#     return {\n",
    "#         'fc1': {\n",
    "#             'mse': mse_fc1,\n",
    "#             'cosine': cosine_fc1,\n",
    "#             'mean_change': fc1_mean_change,\n",
    "#             'var_change': fc1_var_change,\n",
    "#             'var_weighted_change': fc1_var_weighted_change,\n",
    "#         },\n",
    "#         'fc2': {\n",
    "#             'mse': mse_fc2,\n",
    "#             'cosine': cosine_fc2,\n",
    "#             'mean_change': fc2_mean_change,\n",
    "#             'var_change': fc2_var_change,\n",
    "#             'var_weighted_change': fc2_var_weighted_change,\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "def analyze_attention_activations(model, model_quant, batch, block):\n",
    "    \"\"\"Compare activations of attention sub-modules for a specific block.\"\"\"\n",
    "    # Get attention maps for full-precision and quantized models\n",
    "    attn_full = get_attention_maps(model, batch)\n",
    "    attn_quant = get_attention_maps(model_quant, batch)\n",
    "    \n",
    "    # Extract attention outputs for the specified block\n",
    "    attn_full_block = attn_full[f'block_{block}']\n",
    "    attn_quant_block = attn_quant[f'block_{block}']\n",
    "    \n",
    "    # Compute MSE and Cosine Similarity\n",
    "    mse = F.mse_loss(attn_full_block, attn_quant_block).item()\n",
    "    cosine = F.cosine_similarity(attn_full_block.flatten(), attn_quant_block.flatten(), dim=0).item()\n",
    "    \n",
    "    # Compute mean and variance\n",
    "    full_mean = attn_full_block.mean().item()\n",
    "    full_var = attn_full_block.var().item()\n",
    "    quant_mean = attn_quant_block.mean().item()\n",
    "    quant_var = attn_quant_block.var().item()\n",
    "    \n",
    "    # Percent changes in mean and variance\n",
    "    def percent_change(full, quant):\n",
    "        return ((quant - full) / full) * 100\n",
    "    \n",
    "    mean_change = percent_change(full_mean, quant_mean)\n",
    "    var_change = percent_change(full_var, quant_var)\n",
    "    \n",
    "    # Weighted variance change\n",
    "    var_weighted_change = weighted_variance_change(full_var, quant_var)\n",
    "    \n",
    "    # Return results in a dictionary\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'cosine': cosine,\n",
    "        'mean_change': mean_change,\n",
    "        'var_change': var_change,\n",
    "        'var_weighted_change': var_weighted_change,\n",
    "    }\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "columns = ['Layer', 'MSE', 'Cosine', 'Mean (%)', 'Var (%)', 'Weighted Var']\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Analyze each block\n",
    "for i in range(12):\n",
    "    print(f\"Block {i}\")\n",
    "\n",
    "    # Quantize the attention block\n",
    "    layer_to_quantize = f\"transformer.blocks.{i}.attn\"\n",
    "    model_dynamic = deepcopy(model)\n",
    "    torch.quantization.quantize_dynamic(\n",
    "        model_dynamic, {layer_to_quantize: quantization_config}, inplace=True\n",
    "    )\n",
    "    \n",
    "    # Analyze attention outputs\n",
    "    attn_results = analyze_attention_activations(model, model_dynamic, batch=input_, block=i)\n",
    "    attn_row = pd.DataFrame({\n",
    "        'Layer': [f\"{i} attn\"],\n",
    "        'MSE': [attn_results['mse']],\n",
    "        'Cosine': [attn_results['cosine']],\n",
    "        'Mean (%)': [attn_results['mean_change']],\n",
    "        'Var (%)': [attn_results['var_change']],\n",
    "        'Weighted Var': [attn_results['var_weighted_change']],\n",
    "    })\n",
    "    results_df = pd.concat([results_df, attn_row], ignore_index=True)\n",
    "\n",
    "    # Quantize MLP layers (fc1 and fc2)\n",
    "    layer_to_quantize = f\"transformer.blocks.{i}.mlp\"\n",
    "    model_dynamic = deepcopy(model)\n",
    "    torch.quantization.quantize_dynamic(\n",
    "        model_dynamic, {layer_to_quantize: quantization_config}, inplace=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Analyze MLP outputs\n",
    "    mlp_results = analyze_mlp_activations(model, model_dynamic, batch=input_, block=i)\n",
    "    fc1_row = pd.DataFrame({\n",
    "        'Layer': [f\"{i} fc1\"],\n",
    "        'MSE': [mlp_results['mse_fc1']],\n",
    "        'Cosine': [mlp_results['cosine_fc1']],\n",
    "        'Mean (%)': [mlp_results['stats']['fc1_mean_change']],\n",
    "        'Var (%)': [mlp_results['stats']['fc1_var_change']],\n",
    "        'Weighted Var': [mlp_results['stats']['fc1_var_weighted_change']],\n",
    "    })\n",
    "    results_df = pd.concat([results_df, fc1_row], ignore_index=True)\n",
    "    \n",
    "    fc2_row = pd.DataFrame({\n",
    "        'Layer': [f\"{i} fc2\"],\n",
    "        'MSE': [mlp_results['mse_fc2']],\n",
    "        'Cosine': [mlp_results['cosine_fc2']],\n",
    "        'Mean (%)': [mlp_results['stats']['fc2_mean_change']],\n",
    "        'Var (%)': [mlp_results['stats']['fc2_var_change']],\n",
    "        'Weighted Var': [mlp_results['stats']['fc2_var_weighted_change']],\n",
    "    })\n",
    "    results_df = pd.concat([results_df, fc2_row], ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('quantization_results.csv', index=False)\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_attention_maps(model, batch):\n",
    "    \"\"\"Extract attention weights from all blocks.\"\"\"\n",
    "    attention_maps = {}\n",
    "    \n",
    "    # Register hooks to capture attention weights\n",
    "    hooks = []\n",
    "    for i in range(12):\n",
    "        block = model.transformer.blocks[i].attn\n",
    "        hook = block.register_forward_hook(\n",
    "            lambda module, input, output, block_idx=i: attention_maps.__setitem__(f'block_{block_idx}', output[1].detach())\n",
    "        )\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    # Forward pass (replace with your ViLT input format)\n",
    "    with torch.no_grad():\n",
    "        infer1 = model.infer( \n",
    "            batch, mask_text=False, mask_image=False, image_token_type_idx=1\n",
    "        )\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return attention_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 Attention Metrics:\n",
      "  MSE: 0.000098\n",
      "  Cosine Similarity: 0.965466\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.001314, Quant=0.001405, Change=6.90%\n",
      "  Weighted Variance Change: -45.754820\n",
      "Block 1 Attention Metrics:\n",
      "  MSE: 0.000189\n",
      "  Cosine Similarity: 0.908517\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000969, Quant=0.001045, Change=7.84%\n",
      "  Weighted Variance Change: -54.400835\n",
      "Block 2 Attention Metrics:\n",
      "  MSE: 0.000237\n",
      "  Cosine Similarity: 0.861100\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000803, Quant=0.000862, Change=7.43%\n",
      "  Weighted Variance Change: -52.929109\n",
      "Block 3 Attention Metrics:\n",
      "  MSE: 0.000112\n",
      "  Cosine Similarity: 0.879773\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000426, Quant=0.000467, Change=9.62%\n",
      "  Weighted Variance Change: -74.649181\n",
      "Block 4 Attention Metrics:\n",
      "  MSE: 0.000086\n",
      "  Cosine Similarity: 0.877802\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000314, Quant=0.000352, Change=12.35%\n",
      "  Weighted Variance Change: -99.625283\n",
      "Block 5 Attention Metrics:\n",
      "  MSE: 0.000105\n",
      "  Cosine Similarity: 0.852148\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000325, Quant=0.000354, Change=9.16%\n",
      "  Weighted Variance Change: -73.584098\n",
      "Block 6 Attention Metrics:\n",
      "  MSE: 0.000114\n",
      "  Cosine Similarity: 0.834056\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000308, Quant=0.000346, Change=12.12%\n",
      "  Weighted Variance Change: -97.987314\n",
      "Block 7 Attention Metrics:\n",
      "  MSE: 0.000188\n",
      "  Cosine Similarity: 0.771094\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000372, Quant=0.000416, Change=11.92%\n",
      "  Weighted Variance Change: -94.156343\n",
      "Block 8 Attention Metrics:\n",
      "  MSE: 0.000201\n",
      "  Cosine Similarity: 0.722721\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000332, Quant=0.000361, Change=8.56%\n",
      "  Weighted Variance Change: -68.537806\n",
      "Block 9 Attention Metrics:\n",
      "  MSE: 0.000321\n",
      "  Cosine Similarity: 0.627473\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000389, Quant=0.000443, Change=13.86%\n",
      "  Weighted Variance Change: -108.823385\n",
      "Block 10 Attention Metrics:\n",
      "  MSE: 0.000427\n",
      "  Cosine Similarity: 0.446199\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000360, Quant=0.000383, Change=6.40%\n",
      "  Weighted Variance Change: -50.786434\n",
      "Block 11 Attention Metrics:\n",
      "  MSE: 0.000444\n",
      "  Cosine Similarity: 0.517007\n",
      "  Mean: Full=0.003717, Quant=0.003717, Change=0.00%\n",
      "  Variance: Full=0.000421, Quant=0.000468, Change=11.12%\n",
      "  Weighted Variance Change: -86.419898\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def weighted_variance_change(full_var, quant_var):\n",
    "    \"\"\"Calculate weighted variance change.\"\"\"\n",
    "    percent_change = ((quant_var - full_var) / full_var) * 100\n",
    "    return percent_change * math.log(full_var)\n",
    "\n",
    "def analyze_attention_outputs(model, model_quant, batch):\n",
    "    \"\"\"Compare attention outputs for all blocks.\"\"\"\n",
    "    # Get attention maps for full-precision and quantized models\n",
    "    attn_full = get_attention_maps(model, batch)\n",
    "    attn_quant = get_attention_maps(model_quant, batch)\n",
    "    \n",
    "    results = {}\n",
    "    for i in range(12):\n",
    "        # Extract attention outputs for block i\n",
    "        attn_full_block = attn_full[f'block_{i}']\n",
    "        attn_quant_block = attn_quant[f'block_{i}']\n",
    "        \n",
    "        # Compute MSE and Cosine Similarity\n",
    "        mse = F.mse_loss(attn_full_block, attn_quant_block).item()\n",
    "        cosine = F.cosine_similarity(attn_full_block.flatten(), attn_quant_block.flatten(), dim=0).item()\n",
    "        \n",
    "        # Compute mean and variance\n",
    "        full_mean = attn_full_block.mean().item()\n",
    "        full_var = attn_full_block.var().item()\n",
    "        quant_mean = attn_quant_block.mean().item()\n",
    "        quant_var = attn_quant_block.var().item()\n",
    "        \n",
    "        # Percent changes in mean and variance\n",
    "        def percent_change(full, quant):\n",
    "            return ((quant - full) / full) * 100\n",
    "        \n",
    "        mean_change = percent_change(full_mean, quant_mean)\n",
    "        var_change = percent_change(full_var, quant_var)\n",
    "        \n",
    "        # Weighted variance change\n",
    "        var_weighted_change = weighted_variance_change(full_var, quant_var)\n",
    "        \n",
    "        # Store results for block i\n",
    "        results[f'block_{i}'] = {\n",
    "            'mse': mse,\n",
    "            'cosine': cosine,\n",
    "            'full_mean': full_mean,\n",
    "            'full_var': full_var,\n",
    "            'quant_mean': quant_mean,\n",
    "            'quant_var': quant_var,\n",
    "            'mean_change': mean_change,\n",
    "            'var_change': var_change,\n",
    "            'var_weighted_change': var_weighted_change,\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = analyze_attention_outputs(model, model_dynamic, batch=input_)\n",
    "\n",
    "# Print results\n",
    "for i in range(12):\n",
    "    block_results = results[f'block_{i}']\n",
    "    print(f\"Block {i} Attention Metrics:\")\n",
    "    print(f\"  MSE: {block_results['mse']:.6f}\")\n",
    "    print(f\"  Cosine Similarity: {block_results['cosine']:.6f}\")\n",
    "    print(f\"  Mean: Full={block_results['full_mean']:.6f}, Quant={block_results['quant_mean']:.6f}, Change={block_results['mean_change']:.2f}%\")\n",
    "    print(f\"  Variance: Full={block_results['full_var']:.6f}, Quant={block_results['quant_var']:.6f}, Change={block_results['var_change']:.2f}%\")\n",
    "    print(f\"  Weighted Variance Change: {block_results['var_weighted_change']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
