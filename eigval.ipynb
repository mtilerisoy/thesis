{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=2\n",
      "2025-03-06 16:00:32.026443: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741273232.110286 2765597 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741273232.141552 2765597 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:960: UserWarning: Overwriting vit_small_patch16_224 in registry with vilt.modules.vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:982: UserWarning: Overwriting vit_base_patch16_224 in registry with vilt.modules.vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:994: UserWarning: Overwriting vit_base_patch32_224 in registry with vilt.modules.vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_224(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1005: UserWarning: Overwriting vit_base_patch16_384 in registry with vilt.modules.vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1017: UserWarning: Overwriting vit_base_patch32_384 in registry with vilt.modules.vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_384(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1029: UserWarning: Overwriting vit_large_patch16_224 in registry with vilt.modules.vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1041: UserWarning: Overwriting vit_large_patch32_224 in registry with vilt.modules.vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_224(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1052: UserWarning: Overwriting vit_large_patch16_384 in registry with vilt.modules.vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1064: UserWarning: Overwriting vit_large_patch32_384 in registry with vilt.modules.vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_384(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1076: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vilt.modules.vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224_in21k(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1095: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vilt.modules.vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_224_in21k(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1114: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vilt.modules.vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224_in21k(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vilt.modules.vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_224_in21k(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1152: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vilt.modules.vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_huge_patch14_224_in21k(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1172: UserWarning: Overwriting vit_base_resnet50_224_in21k in registry with vilt.modules.vision_transformer.vit_base_resnet50_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_resnet50_224_in21k(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1201: UserWarning: Overwriting vit_base_resnet50_384 in registry with vilt.modules.vision_transformer.vit_base_resnet50_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_resnet50_384(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1225: UserWarning: Overwriting vit_small_resnet26d_224 in registry with vilt.modules.vision_transformer.vit_small_resnet26d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_resnet26d_224(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1273: UserWarning: Overwriting vit_base_resnet26d_224 in registry with vilt.modules.vision_transformer.vit_base_resnet26d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_resnet26d_224(pretrained=False, **kwargs):\n",
      "/home/mileriso/thesis/vilt/modules/vision_transformer.py:1292: UserWarning: Overwriting vit_base_resnet50d_224 in registry with vilt.modules.vision_transformer.vit_base_resnet50d_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_resnet50d_224(pretrained=False, **kwargs):\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Limit the number of CPUs\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"10\"  # Set this to the number of CPUs you want to use\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"10\"  # Set this to the number of CPUs you want to use\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "random.seed(42)\n",
    "from torch.utils.data import Subset\n",
    "from vilt.datamodules.multitask_datamodule import MTDataModule as MTDataModuleVILT\n",
    "from meter.datamodules.multitask_datamodule import MTDataModule as MTDataModuleMeter\n",
    "\n",
    "from vilt.modules import ViLTransformerSS\n",
    "from meter.modules import METERTransformerSS\n",
    "\n",
    "# Set the configuration\n",
    "import pytorch_lightning as pl\n",
    "import configs\n",
    "_config = configs.meter_config_nlvr2_original\n",
    "_config[\"batch_size\"] = 10\n",
    "_config[\"per_gpu_batchsize\"] = 10\n",
    "pl.seed_everything(_config[\"seed\"])\n",
    "\n",
    "# Set the GPU device\n",
    "gpu_id = 0\n",
    "# device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "torch.cuda.set_device(gpu_id)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "\n",
    "\n",
    "class SmallMTDataModuleVILT(MTDataModuleVILT):\n",
    "    def __init__(self, _config, dist=False, num_samples=5, start_idx=100):\n",
    "        super().__init__(_config, dist)\n",
    "        self.num_samples = num_samples\n",
    "        self.start_idx = start_idx\n",
    "\n",
    "    def setup(self, stage, is_random):\n",
    "        super().setup(stage)\n",
    "        \n",
    "        # Limit the number of samples in the datasets\n",
    "        if is_random:\n",
    "            self.train_dataset = self._get_random_subset(self.train_dataset, self.num_samples)\n",
    "            self.val_dataset = self._get_random_subset(self.val_dataset, self.num_samples)\n",
    "            self.test_dataset = self._get_random_subset(self.test_dataset, self.num_samples)\n",
    "        else:    \n",
    "            self.train_dataset = Subset(self.train_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "            self.val_dataset = Subset(self.val_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "            self.test_dataset = Subset(self.test_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        \n",
    "    def _get_random_subset(self, dataset, num_samples):\n",
    "        indices = random.sample(range(len(dataset)), num_samples)\n",
    "        return Subset(dataset, indices)\n",
    "\n",
    "class SmallMTDataModuleMETER(MTDataModuleMeter):\n",
    "    def __init__(self, _config, dist=False, num_samples=10, start_idx=100):\n",
    "        super().__init__(_config, dist)\n",
    "        self.num_samples = num_samples\n",
    "        self.start_idx = start_idx\n",
    "\n",
    "    def setup(self, stage, is_random):\n",
    "        super().setup(stage)\n",
    "        \n",
    "        # Limit the number of samples in the datasets\n",
    "        if is_random:\n",
    "            self.train_dataset = self._get_random_subset(self.train_dataset, self.num_samples)\n",
    "            self.val_dataset = self._get_random_subset(self.val_dataset, self.num_samples)\n",
    "            self.test_dataset = self._get_random_subset(self.test_dataset, self.num_samples)\n",
    "        else:    \n",
    "            self.train_dataset = Subset(self.train_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "            self.val_dataset = Subset(self.val_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "            self.test_dataset = Subset(self.test_dataset, range(self.start_idx, self.start_idx+self.num_samples))\n",
    "        \n",
    "    \n",
    "    def _get_random_subset(self, dataset, num_samples):\n",
    "        indices = random.sample(range(len(dataset)), num_samples)\n",
    "        return Subset(dataset, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hessian analysis\n",
    "def compute_gradients(pl_module, batch, layer):\n",
    "    pl_module.zero_grad()\n",
    "\n",
    "    infer1 = pl_module.infer(\n",
    "        batch, mask_text=False, mask_image=False, image_token_type_idx=1\n",
    "    )\n",
    "    infer2 = pl_module.infer(\n",
    "        batch, mask_text=False, mask_image=False, image_token_type_idx=2\n",
    "    )\n",
    "\n",
    "    cls_feats = torch.cat([infer1[\"cls_feats\"], infer2[\"cls_feats\"]], dim=-1)\n",
    "    nlvr2_logits = pl_module.nlvr2_classifier(cls_feats)\n",
    "\n",
    "    nlvr2_labels = batch[\"answers\"]\n",
    "    nlvr2_labels = torch.tensor(nlvr2_labels).to(device).long()  # Move labels to GPU\n",
    "    loss = torch.nn.functional.cross_entropy(nlvr2_logits, nlvr2_labels)\n",
    "\n",
    "    grad_params = torch.autograd.grad(loss, layer.parameters(), create_graph=True)\n",
    "    return grad_params\n",
    "\n",
    "def hvp(layer, grad_params, v):\n",
    "    # Flatten gradients and vector v\n",
    "    grads = torch.cat([g.contiguous().view(-1) for g in grad_params])\n",
    "    v = torch.cat([vi.contiguous().view(-1) for vi in v])\n",
    "    \n",
    "    # Compute g^T * v\n",
    "    gTv = torch.dot(grads, v)\n",
    "    \n",
    "    # Compute Hv = ∇(g^T v)\n",
    "    Hv = torch.autograd.grad(gTv, layer.parameters(), retain_graph=True)\n",
    "    Hv = [h.detach() for h in Hv]  # Detach to stop gradient tracking\n",
    "    return Hv\n",
    "\n",
    "def compute_top_eigenvalue(model, layer, input, num_iterations=50):\n",
    "    grad_params = compute_gradients(model, input, layer)\n",
    "    \n",
    "    # Initialize random vector v with same shape as parameters\n",
    "    params = list(layer.parameters())\n",
    "    v = [torch.randn_like(p).to(device) for p in params]  # Move v to GPU\n",
    "    \n",
    "    # Normalize v\n",
    "    v_flat = torch.cat([vi.view(-1) for vi in v])\n",
    "    v_norm = torch.norm(v_flat)\n",
    "    v = [vi / v_norm for vi in v]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # if i % 5 == 0:\n",
    "        #     print(f\"Iteration: {i}\")\n",
    "        \n",
    "        Hv = hvp(layer, grad_params, v)\n",
    "        Hv_flat = torch.cat([hvi.view(-1) for hvi in Hv])\n",
    "        \n",
    "        # Update v and eigenvalue estimate\n",
    "        v_norm = torch.norm(Hv_flat)\n",
    "        v = [hvi / v_norm for hvi in Hv]\n",
    "        eigenvalue = v_norm.item()\n",
    "    \n",
    "    return eigenvalue\n",
    "\n",
    "def compute_layer_eigenvalues(model, input, num_iterations=50):\n",
    "    eigenvalues = {}\n",
    "\n",
    "    for name, layer in model.named_modules():\n",
    "        \n",
    "        if isinstance(layer, (torch.nn.Linear)):\n",
    "            # print(f\"Layer: {name}\")\n",
    "            if \"encoder\" not in name or \"intermediate\" not in name or \"output\" in name or \"attention\" in name:\n",
    "                continue\n",
    "            print(f\"Computing eigenvalue for layer: {name}\")\n",
    "            eigenvalue = compute_top_eigenvalue(model, layer, input, num_iterations)\n",
    "\n",
    "            eigenvalues[name] = eigenvalue   \n",
    "\n",
    "            print(\"==============================================\")\n",
    "            print(f\"Computed eigenvalue for layer {name} : {eigenvalue}\")\n",
    "            print(\"All eigenvalues computed so far:\")\n",
    "            print(f\"{eigenvalues}\")\n",
    "            print(\"==============================================\")\n",
    "\n",
    "    return eigenvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['nlvr2_train']\n",
      "Loaded names: ['nlvr2_dev', 'nlvr2_test1']\n",
      "Loaded names: ['nlvr2_dev', 'nlvr2_test1']\n",
      "Batch size: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mileriso/thesis/meter/datasets/nlvr2_dataset.py:18: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  super().__init__(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/mileriso/thesis/meter/modules/meter_module.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.hparams.config[\"load_path\"], map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized METER model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "METERTransformerSS(\n",
       "  (cross_modal_text_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (cross_modal_image_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (token_type_embeddings): Embedding(3, 768)\n",
       "  (vit_model): CLIP(\n",
       "    (visual): VisualTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_transformer): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cross_modal_image_layers): ModuleList(\n",
       "    (0-5): 6 x BertCrossLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (crossattention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cross_modal_text_layers): ModuleList(\n",
       "    (0-5): 6 x BertCrossLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (crossattention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cross_modal_image_pooler): Pooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (cross_modal_text_pooler): Pooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (nlvr2_classifier): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1536, out_features=2, bias=True)\n",
       "  )\n",
       "  (train_nlvr2_accuracy): Accuracy()\n",
       "  (train_nlvr2_loss): Scalar()\n",
       "  (dev_nlvr2_accuracy): Accuracy()\n",
       "  (dev_nlvr2_loss): Scalar()\n",
       "  (test_nlvr2_accuracy): Accuracy()\n",
       "  (test_nlvr2_loss): Scalar()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ========= Create full datamodule =========\n",
    "# ==========================================\n",
    "if \"meter\" in _config[\"model\"]:\n",
    "    # full_dm = MTDataModuleMeter(_config, dist=False)\n",
    "    \n",
    "    # calibrarte_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=5, start_idx=100)\n",
    "    \n",
    "    infer_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=10, start_idx=0)\n",
    "    infer_dm.setup(\"test\", is_random=True)\n",
    "    infer_dataloader = infer_dm.test_dataloader()\n",
    "\n",
    "elif \"vilt\" in _config[\"model\"]:\n",
    "    # full_dm = MTDataModuleVILT(_config, dist=False)\n",
    "\n",
    "    # calibrarte_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=5)\n",
    "    \n",
    "    infer_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=1, start_idx=0)\n",
    "    infer_dm.setup(\"test\", is_random=True)\n",
    "    infer_dataloader = infer_dm.test_dataloader()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported: \", _config[\"model\"])\n",
    "\n",
    "print(f\"Batch size: {_config['batch_size']}\")\n",
    "\n",
    "# ==========================================\n",
    "# ========= Initialize the model ===========\n",
    "# ==========================================\n",
    "if _config[\"model\"] == \"vilt\":\n",
    "    model = ViLTransformerSS(_config)\n",
    "    print(\"Initialized ViLT model\")\n",
    "\n",
    "elif _config[\"model\"] == \"meter\":\n",
    "    model = METERTransformerSS(_config)\n",
    "    print(\"Initialized METER model\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported: \", _config[\"model\"])\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'image_1', 'image_0', 'table_name', 'answers', 'text_ids', 'text_labels', 'text_ids_mlm', 'text_labels_mlm', 'text_masks'])\n",
      "Number of batches: 1\n",
      "Samples in a batch: 10\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ======= Initialize the dataloader ========\n",
    "# ==========================================\n",
    "input_batch = next(iter(infer_dataloader))\n",
    "num_batches = len(infer_dataloader)\n",
    "\n",
    "print(input_batch.keys())\n",
    "print(f\"Number of batches: {num_batches}\")\n",
    "print(f\"Samples in a batch: {len(input_batch['answers'])}\")\n",
    "\n",
    "# Move input data to GPU\n",
    "for key in input_batch:\n",
    "    if isinstance(input_batch[key], torch.Tensor):\n",
    "        input_batch[key] = input_batch[key].to(device)\n",
    "\n",
    "input_batch[\"image_0\"][0] = input_batch[\"image_0\"][0].to(device)\n",
    "input_batch[\"image_1\"][0] = input_batch[\"image_1\"][0].to(device)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# ========= Compute eigenvalues ============\n",
    "# ==========================================\n",
    "# for i in range(20):\n",
    "#     infer_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=1, start_idx=0)\n",
    "#     infer_dm.setup(\"test\", is_random=True)\n",
    "#     infer_dataloader = infer_dm.test_dataloader()\n",
    "#     eigenvalues = compute_layer_eigenvalues(model, input_batch, num_iterations=50)\n",
    "\n",
    "#     # Save the eigenvalues to a txt file\n",
    "#     with open(f\"eigenvalues_meter_{i}.txt\", \"w\") as f:\n",
    "#         f.write(str(eigenvalues))\n",
    "# # eigenvalues = compute_averaged_eigenvalues(model, infer_dataloader, num_batches, num_iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian Eigenvalue Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantization_utils import get_quantization_config\n",
    "from copy import deepcopy\n",
    "\n",
    "def quantize_weights(weight, bits=8):\n",
    "    \"\"\"\n",
    "    Symmetric uniform quantization for weights.\n",
    "    Args:\n",
    "        weight (Tensor): Full-precision weights of a layer.\n",
    "        bits (int): Number of quantization bits.\n",
    "    Returns:\n",
    "        dequantized_weight (Tensor): Quantized/dequantized weights.\n",
    "    \"\"\"\n",
    "    # # Symmetric range based on max absolute value\n",
    "    # max_val = torch.max(torch.abs(weight))\n",
    "    # scale = max_val / (2 ** (bits - 1) - 1)\n",
    "    \n",
    "    # # Quantize and dequantize\n",
    "    # quantized = torch.clamp(torch.round(weight / scale), -2**(bits-1), 2**(bits-1)-1)\n",
    "    # dequantized_weight = quantized * scale\n",
    "    # return dequantized_weight, scale\n",
    "    min_val = weight.min()\n",
    "    max_val = weight.max()\n",
    "    scale = (max_val - min_val) / (2**bits - 1)\n",
    "    zero_point = torch.round(-min_val / scale)\n",
    "    quantized = torch.clamp(torch.round(weight / scale + zero_point), 0, 2**bits - 1)\n",
    "    dequantized = (quantized - zero_point) * scale\n",
    "\n",
    "    return dequantized, scale, zero_point\n",
    "\n",
    "def compute_quantization_perturbation(model, eigenvalues, bits=4):\n",
    "    \"\"\"\n",
    "    Compute quantization perturbation metric Ω_i for each layer.\n",
    "    Args:\n",
    "        model (nn.Module): PyTorch model.\n",
    "        eigenvalues (dict): Precomputed top eigenvalues (λ_i) for each layer.\n",
    "        bits (int): Number of quantization bits.\n",
    "    Returns:\n",
    "        omega_dict (dict): Ω_i values for each layer.\n",
    "    \"\"\"\n",
    "    omega_dict = {}\n",
    "    perturbation_dict = {}\n",
    "    model_dynamic = deepcopy(model)\n",
    "    # Quantize weights\n",
    "    quantization_config, _ = get_quantization_config(4)\n",
    "    torch.quantization.quantize_dynamic(\n",
    "        model_dynamic, {\"text_transformer.encoder.layer\": quantization_config}, inplace=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    for i in range(12):\n",
    "        layer_name = \"text_transformer.encoder.layer.\" + str(i)\n",
    "        if layer_name + \".output.dense\" in eigenvalues:\n",
    "            layer_name = layer_name + \".output.dense\"\n",
    "        elif layer_name + \".intermediate.dense\" in eigenvalues:\n",
    "            layer_name = layer_name + \".intermediate.dense\"\n",
    "        else:\n",
    "            break\n",
    "        # Get original weights\n",
    "        weight = model.text_transformer.encoder.layer[i].intermediate.dense.weight\n",
    "        quantized_weight = model_dynamic.text_transformer.encoder.layer[i].intermediate.dense.weight()\n",
    "        \n",
    "\n",
    "        q_weigths, scale, zero = quantize_weights(weight, bits)\n",
    "\n",
    "        # print(weight)\n",
    "        # print(f\"Min: {weight.min()}, Max: {weight.max()}, Mean: {weight.mean()}\")\n",
    "        \n",
    "        # print(quantized_weight)\n",
    "        # print(f\"Min: {quantized_weight.data.min()}, Max: {quantized_weight.data.max()}, Mean: {quantized_weight.data.mean()}\")\n",
    "        \n",
    "        # print(quantized_weight.dequantize().data)\n",
    "        # print(f\"Min: {quantized_weight.dequantize().data.min()}, Max: {quantized_weight.dequantize().data.max()}, Mean: {quantized_weight.dequantize().data.mean()}\")\n",
    "        # print(q_weigths)\n",
    "        # print(f\"Scale: {scale}\")\n",
    "        # print(f\"Zero Point: {zero}\")\n",
    "        # print(\"==============================================\")\n",
    "\n",
    "        # Compute L2 perturbation: ‖Q(W_i) - W_i‖²\n",
    "        # perturbation = torch.norm(quantized_weight.dequantize().data - weight, p=2) ** 2\n",
    "        perturbation = torch.norm(quantized_weight.dequantize().data - weight, p=2) ** 2\n",
    "        \n",
    "        # Compute Ω_i = λ_i * perturbation\n",
    "        lambda_i = eigenvalues[layer_name]\n",
    "        omega_i = lambda_i * perturbation.item()\n",
    "        \n",
    "        perturbation_dict[layer_name] = perturbation.item()\n",
    "        omega_dict[layer_name] = omega_i\n",
    "\n",
    "\n",
    "    return omega_dict, perturbation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average eigenvalues: {'text_transformer.encoder.layer.0.output.dense': 24.300703048706055, 'text_transformer.encoder.layer.1.output.dense': 27.49468231201172, 'text_transformer.encoder.layer.2.output.dense': 23.84470558166504, 'text_transformer.encoder.layer.3.output.dense': 27.41619300842285, 'text_transformer.encoder.layer.4.output.dense': 33.994407653808594, 'text_transformer.encoder.layer.5.output.dense': 36.02847671508789, 'text_transformer.encoder.layer.6.output.dense': 37.94231414794922, 'text_transformer.encoder.layer.7.output.dense': 38.986351013183594, 'text_transformer.encoder.layer.8.output.dense': 40.81344223022461, 'text_transformer.encoder.layer.9.output.dense': 45.41384506225586, 'text_transformer.encoder.layer.10.output.dense': 32.24785232543945, 'text_transformer.encoder.layer.11.output.dense': 40.361717224121094}\n"
     ]
    }
   ],
   "source": [
    "# eigenvalues0 = {'text_transformer.encoder.layer.0.intermediate.dense': 0.16790370643138885, 'text_transformer.encoder.layer.1.intermediate.dense': 0.1432534158229828, 'text_transformer.encoder.layer.2.intermediate.dense': 0.2982516884803772, 'text_transformer.encoder.layer.3.intermediate.dense': 0.2224932760000229, 'text_transformer.encoder.layer.4.intermediate.dense': 0.3737144470214844, 'text_transformer.encoder.layer.5.intermediate.dense': 0.5271583795547485, 'text_transformer.encoder.layer.6.intermediate.dense': 1.0165506601333618, 'text_transformer.encoder.layer.7.intermediate.dense': 1.4828656911849976, 'text_transformer.encoder.layer.8.intermediate.dense': 1.0849188566207886, 'text_transformer.encoder.layer.9.intermediate.dense': 1.3699719905853271, 'text_transformer.encoder.layer.10.intermediate.dense': 0.655511736869812, 'text_transformer.encoder.layer.11.intermediate.dense': 0.6610674858093262}\n",
    "# eigenvalues1 = {'text_transformer.encoder.layer.0.intermediate.dense': 0.17076769471168518, 'text_transformer.encoder.layer.1.intermediate.dense': 0.2954607605934143, 'text_transformer.encoder.layer.2.intermediate.dense': 0.3319089114665985, 'text_transformer.encoder.layer.3.intermediate.dense': 0.22153879702091217, 'text_transformer.encoder.layer.4.intermediate.dense': 0.3949551284313202, 'text_transformer.encoder.layer.5.intermediate.dense': 0.414730429649353, 'text_transformer.encoder.layer.6.intermediate.dense': 0.5701264142990112, 'text_transformer.encoder.layer.7.intermediate.dense': 0.34018513560295105, 'text_transformer.encoder.layer.8.intermediate.dense': 1.1660056114196777, 'text_transformer.encoder.layer.9.intermediate.dense': 0.4433540105819702, 'text_transformer.encoder.layer.10.intermediate.dense': 0.37822774052619934, 'text_transformer.encoder.layer.11.intermediate.dense': 0.9691262245178223}\n",
    "# eigenvalues2 = {'text_transformer.encoder.layer.0.intermediate.dense': 0.14164957404136658, 'text_transformer.encoder.layer.1.intermediate.dense': 0.1395464688539505, 'text_transformer.encoder.layer.2.intermediate.dense': 0.22589385509490967, 'text_transformer.encoder.layer.3.intermediate.dense': 0.184654101729393, 'text_transformer.encoder.layer.4.intermediate.dense': 0.5432092547416687, 'text_transformer.encoder.layer.5.intermediate.dense': 0.4408581256866455, 'text_transformer.encoder.layer.6.intermediate.dense': 0.598903238773346, 'text_transformer.encoder.layer.7.intermediate.dense': 0.4705699384212494, 'text_transformer.encoder.layer.8.intermediate.dense': 0.41199880838394165, 'text_transformer.encoder.layer.9.intermediate.dense': 1.2278794050216675, 'text_transformer.encoder.layer.10.intermediate.dense': 0.3707928955554962, 'text_transformer.encoder.layer.11.intermediate.dense': 0.2581954598426819}\n",
    "# eigenvalues3 = {'text_transformer.encoder.layer.0.intermediate.dense': 0.16633661091327667, 'text_transformer.encoder.layer.1.intermediate.dense': 0.11317455768585205, 'text_transformer.encoder.layer.2.intermediate.dense': 0.32719480991363525, 'text_transformer.encoder.layer.3.intermediate.dense': 0.3101462423801422, 'text_transformer.encoder.layer.4.intermediate.dense': 0.4569089412689209, 'text_transformer.encoder.layer.5.intermediate.dense': 0.8330849409103394, 'text_transformer.encoder.layer.6.intermediate.dense': 0.7549309134483337, 'text_transformer.encoder.layer.7.intermediate.dense': 0.3048100471496582, 'text_transformer.encoder.layer.8.intermediate.dense': 0.36305004358291626, 'text_transformer.encoder.layer.9.intermediate.dense': 1.6126809120178223, 'text_transformer.encoder.layer.10.intermediate.dense': 0.4344247877597809, 'text_transformer.encoder.layer.11.intermediate.dense': 0.4249117970466614}\n",
    "# eigenvalues4 = {'text_transformer.encoder.layer.0.intermediate.dense': 0.12828055024147034, 'text_transformer.encoder.layer.1.intermediate.dense': 0.12925875186920166, 'text_transformer.encoder.layer.2.intermediate.dense': 0.30379724502563477, 'text_transformer.encoder.layer.3.intermediate.dense': 0.34414660930633545, 'text_transformer.encoder.layer.4.intermediate.dense': 0.32475945353507996, 'text_transformer.encoder.layer.5.intermediate.dense': 0.3463519215583801, 'text_transformer.encoder.layer.6.intermediate.dense': 0.49876466393470764, 'text_transformer.encoder.layer.7.intermediate.dense': 0.5240191221237183, 'text_transformer.encoder.layer.8.intermediate.dense': 0.6521978378295898, 'text_transformer.encoder.layer.9.intermediate.dense': 1.50875723361969, 'text_transformer.encoder.layer.10.intermediate.dense': 0.5115640163421631, 'text_transformer.encoder.layer.11.intermediate.dense': 0.45940256118774414}\n",
    "# eigenvalues5 = {'text_transformer.encoder.layer.0.output.dense': 24.300703048706055, 'text_transformer.encoder.layer.1.output.dense': 27.49468231201172, 'text_transformer.encoder.layer.2.output.dense': 23.84470558166504, 'text_transformer.encoder.layer.3.output.dense': 27.41619300842285, 'text_transformer.encoder.layer.4.output.dense': 33.994407653808594, 'text_transformer.encoder.layer.5.output.dense': 36.02847671508789, 'text_transformer.encoder.layer.6.output.dense': 37.94231414794922, 'text_transformer.encoder.layer.7.output.dense': 38.986351013183594, 'text_transformer.encoder.layer.8.output.dense': 40.81344223022461, 'text_transformer.encoder.layer.9.output.dense': 45.41384506225586, 'text_transformer.encoder.layer.10.output.dense': 32.24785232543945, 'text_transformer.encoder.layer.11.output.dense': 40.361717224121094}\n",
    "# # Average the eigenvalues\n",
    "# eigenvalues = {}\n",
    "# for key in eigenvalues5.keys():\n",
    "#     eigenvalues[key] = (eigenvalues0[key] + eigenvalues1[key] + eigenvalues2[key] + eigenvalues3[key] + eigenvalues4[key]) / 5\n",
    "\n",
    "eigenvalues = {\"text_transformer.encoder.layer.0.output.dense\": 23.591533660888672, \"text_transformer.encoder.layer.1.output.dense\": 28.929523468017578, \"text_transformer.encoder.layer.2.output.dense\": 24.528627395629883, \"text_transformer.encoder.layer.3.output.dense\": 26.392732620239258, \"text_transformer.encoder.layer.4.output.dense\": 34.20820236206055, \"text_transformer.encoder.layer.5.output.dense\": 34.652042388916016, \"text_transformer.encoder.layer.6.output.dense\": 37.35274887084961, \"text_transformer.encoder.layer.7.output.dense\": 38.30513000488281, \"text_transformer.encoder.layer.8.output.dense\": 43.141258239746094, \"text_transformer.encoder.layer.9.output.dense\": 45.42149353027344, \"text_transformer.encoder.layer.10.output.dense\": 32.83196258544922, \"text_transformer.encoder.layer.11.output.dense\": 43.240238189697266}\n",
    "eigenvalues = {'text_transformer.encoder.layer.0.output.dense': 24.300703048706055, 'text_transformer.encoder.layer.1.output.dense': 27.49468231201172, 'text_transformer.encoder.layer.2.output.dense': 23.84470558166504, 'text_transformer.encoder.layer.3.output.dense': 27.41619300842285, 'text_transformer.encoder.layer.4.output.dense': 33.994407653808594, 'text_transformer.encoder.layer.5.output.dense': 36.02847671508789, 'text_transformer.encoder.layer.6.output.dense': 37.94231414794922, 'text_transformer.encoder.layer.7.output.dense': 38.986351013183594, 'text_transformer.encoder.layer.8.output.dense': 40.81344223022461, 'text_transformer.encoder.layer.9.output.dense': 45.41384506225586, 'text_transformer.encoder.layer.10.output.dense': 32.24785232543945, 'text_transformer.encoder.layer.11.output.dense': 40.361717224121094}\n",
    "print(f\"Average eigenvalues: {eigenvalues}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning order (descending Ω_i):\n",
      "text_transformer.encoder.layer.1.output.dense: 83961.5639 || 3053.7383 || 27.4947\n",
      "text_transformer.encoder.layer.0.output.dense: 73070.9029 || 3006.9460 || 24.3007\n",
      "text_transformer.encoder.layer.7.output.dense: 71301.5530 || 1828.8850 || 38.9864\n",
      "text_transformer.encoder.layer.6.output.dense: 57149.1147 || 1506.2106 || 37.9423\n",
      "text_transformer.encoder.layer.4.output.dense: 52108.8623 || 1532.8657 || 33.9944\n",
      "text_transformer.encoder.layer.8.output.dense: 47796.0532 || 1171.0861 || 40.8134\n",
      "text_transformer.encoder.layer.9.output.dense: 44863.6595 || 987.8851 || 45.4138\n",
      "text_transformer.encoder.layer.5.output.dense: 44211.4665 || 1227.1256 || 36.0285\n",
      "text_transformer.encoder.layer.3.output.dense: 44066.4031 || 1607.3130 || 27.4162\n",
      "text_transformer.encoder.layer.10.output.dense: 39600.2013 || 1227.9950 || 32.2479\n",
      "text_transformer.encoder.layer.2.output.dense: 34007.0793 || 1426.1899 || 23.8447\n",
      "text_transformer.encoder.layer.11.output.dense: 21534.3286 || 533.5335 || 40.3617\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute eigenvalues for all layers\n",
    "# eigenvalues = compute_layer_eigenvalues(model, input_batch, num_iterations=50)\n",
    "\n",
    "# 2. Compute quantization perturbation metrics (Ω_i)\n",
    "bits = 4  # Target quantization precision\n",
    "omega_dict, perturbation_dict = compute_quantization_perturbation(model, eigenvalues, bits=bits)\n",
    "\n",
    "# 3. Sort layers by Ω_i to determine fine-tuning order\n",
    "sorted_layers = sorted(omega_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Fine-tuning order (descending Ω_i):\")\n",
    "for layer, omega in sorted_layers:\n",
    "    print(f\"{layer}: {omega:.4f} || {perturbation_dict[layer]:.4f} || {eigenvalues[layer]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning order (descending Ω_i):\n",
      "text_transformer.encoder.layer.1.output.dense: 88343.1933 || 3053.7383 || 28.9295\n",
      "text_transformer.encoder.layer.0.output.dense: 70938.4688 || 3006.9460 || 23.5915\n",
      "text_transformer.encoder.layer.7.output.dense: 70055.6781 || 1828.8850 || 38.3051\n",
      "text_transformer.encoder.layer.6.output.dense: 56261.1052 || 1506.2106 || 37.3527\n",
      "text_transformer.encoder.layer.4.output.dense: 52436.5808 || 1532.8657 || 34.2082\n",
      "text_transformer.encoder.layer.8.output.dense: 50522.1261 || 1171.0861 || 43.1413\n",
      "text_transformer.encoder.layer.9.output.dense: 44871.2154 || 987.8851 || 45.4215\n",
      "text_transformer.encoder.layer.5.output.dense: 42522.4087 || 1227.1256 || 34.6520\n",
      "text_transformer.encoder.layer.3.output.dense: 42421.3819 || 1607.3130 || 26.3927\n",
      "text_transformer.encoder.layer.10.output.dense: 40317.4857 || 1227.9950 || 32.8320\n",
      "text_transformer.encoder.layer.2.output.dense: 34982.4817 || 1426.1899 || 24.5286\n",
      "text_transformer.encoder.layer.11.output.dense: 23070.1160 || 533.5335 || 43.2402\n"
     ]
    }
   ],
   "source": [
    "eigenvalues = {'text_transformer.encoder.layer.0.output.dense': 0.135345126, 'text_transformer.encoder.layer.1.output.dense': 0.088416712, 'text_transformer.encoder.layer.2.output.dense': 0.096227547, 'text_transformer.encoder.layer.3.output.dense': 0.107769994, 'text_transformer.encoder.layer.4.output.dense': 0.188330392, 'text_transformer.encoder.layer.5.output.dense': 0.191571182, 'text_transformer.encoder.layer.6.output.dense': 0.257129359, 'text_transformer.encoder.layer.7.output.dense': 0.202422408, 'text_transformer.encoder.layer.8.output.dense': 0.278533794, 'text_transformer.encoder.layer.9.output.dense': 0.280634147, 'text_transformer.encoder.layer.10.output.dense': 0.202529321, 'text_transformer.encoder.layer.11.output.dense': 0.368850504}\n",
    "eigenvalues = {\"text_transformer.encoder.layer.0.output.dense\": 23.591533660888672, \"text_transformer.encoder.layer.1.output.dense\": 28.929523468017578, \"text_transformer.encoder.layer.2.output.dense\": 24.528627395629883, \"text_transformer.encoder.layer.3.output.dense\": 26.392732620239258, \"text_transformer.encoder.layer.4.output.dense\": 34.20820236206055, \"text_transformer.encoder.layer.5.output.dense\": 34.652042388916016, \"text_transformer.encoder.layer.6.output.dense\": 37.35274887084961, \"text_transformer.encoder.layer.7.output.dense\": 38.30513000488281, \"text_transformer.encoder.layer.8.output.dense\": 43.141258239746094, \"text_transformer.encoder.layer.9.output.dense\": 45.42149353027344, \"text_transformer.encoder.layer.10.output.dense\": 32.83196258544922, \"text_transformer.encoder.layer.11.output.dense\": 43.240238189697266}\n",
    "eigenvalues = {'text_transformer.encoder.layer.0.output.dense': 24.300703048706055, 'text_transformer.encoder.layer.1.output.dense': 27.49468231201172, 'text_transformer.encoder.layer.2.output.dense': 23.84470558166504, 'text_transformer.encoder.layer.3.output.dense': 27.41619300842285, 'text_transformer.encoder.layer.4.output.dense': 33.994407653808594, 'text_transformer.encoder.layer.5.output.dense': 36.02847671508789, 'text_transformer.encoder.layer.6.output.dense': 37.94231414794922, 'text_transformer.encoder.layer.7.output.dense': 38.986351013183594, 'text_transformer.encoder.layer.8.output.dense': 40.81344223022461, 'text_transformer.encoder.layer.9.output.dense': 45.41384506225586, 'text_transformer.encoder.layer.10.output.dense': 32.24785232543945, 'text_transformer.encoder.layer.11.output.dense': 40.361717224121094}\n",
    "# 1. Compute eigenvalues for all layers\n",
    "# eigenvalues = compute_layer_eigenvalues(model, input_batch, num_iterations=50)\n",
    "\n",
    "# 2. Compute quantization perturbation metrics (Ω_i)\n",
    "bits = 4  # Target quantization precision\n",
    "omega_dict, perturbation_dict = compute_quantization_perturbation(model, eigenvalues, bits=bits)\n",
    "\n",
    "# 3. Sort layers by Ω_i to determine fine-tuning order\n",
    "sorted_layers = sorted(omega_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "# sorted_layers = sorted(eigenvalues.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Fine-tuning order (descending Ω_i):\")\n",
    "for layer, omega in sorted_layers:\n",
    "    print(f\"{layer}: {omega:.4f} || {perturbation_dict[layer]:.4f} || {eigenvalues[layer]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ablated_loss(pl_module, batch, target_layer_name):\n",
    "    \"\"\"\n",
    "    Compute loss when a specific layer is ablated (zeroed out).\n",
    "    Integrates with your existing pipeline.\n",
    "    \"\"\"\n",
    "    original_loss = None\n",
    "    delta_loss = None\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Step 1: Compute original loss with all layers\n",
    "    # ----------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        infer1 = pl_module.infer(\n",
    "            batch, mask_text=False, mask_image=False, image_token_type_idx=1\n",
    "        )\n",
    "        infer2 = pl_module.infer(\n",
    "            batch, mask_text=False, mask_image=False, image_token_type_idx=2\n",
    "        )\n",
    "\n",
    "        cls_feats = torch.cat([infer1[\"cls_feats\"], infer2[\"cls_feats\"]], dim=-1)\n",
    "        nlvr2_logits = pl_module.nlvr2_classifier(cls_feats)\n",
    "        \n",
    "        nlvr2_labels = batch[\"answers\"]\n",
    "        nlvr2_labels = torch.tensor(nlvr2_labels).to(device).long()\n",
    "        original_loss = torch.nn.functional.cross_entropy(nlvr2_logits, nlvr2_labels).item()\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Step 2: Compute loss with target layer ablated\n",
    "    # ----------------------------------------------------\n",
    "    def zero_output_hook(module, input, output):\n",
    "        \"\"\"Hook function to zero the layer's output\"\"\"\n",
    "        return torch.zeros_like(output)\n",
    "\n",
    "    # Register hook on target layer\n",
    "    target_layer = dict(pl_module.named_modules())[target_layer_name]\n",
    "    handle = target_layer.register_forward_hook(zero_output_hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        infer1_ablated = pl_module.infer(\n",
    "            batch, mask_text=False, mask_image=False, image_token_type_idx=1\n",
    "        )\n",
    "        infer2_ablated = pl_module.infer(\n",
    "            batch, mask_text=False, mask_image=False, image_token_type_idx=2\n",
    "        )\n",
    "\n",
    "        cls_feats_ablated = torch.cat([infer1_ablated[\"cls_feats\"], infer2_ablated[\"cls_feats\"]], dim=-1)\n",
    "        nlvr2_logits_ablated = pl_module.nlvr2_classifier(cls_feats_ablated)\n",
    "        \n",
    "        ablated_loss = torch.nn.functional.cross_entropy(nlvr2_logits_ablated, nlvr2_labels).item()\n",
    "\n",
    "    # Cleanup and return\n",
    "    handle.remove()\n",
    "    delta_loss = ablated_loss - original_loss\n",
    "\n",
    "    return {\n",
    "        \"layer\": target_layer_name,\n",
    "        \"original_loss\": original_loss,\n",
    "        \"ablated_loss\": ablated_loss,\n",
    "        \"delta_loss\": delta_loss\n",
    "    }\n",
    "\n",
    "def ablation_analysis(pl_module, dataloader, layer_names):\n",
    "    \"\"\"\n",
    "    Full ablation study pipeline\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        for key in batch:\n",
    "            if isinstance(batch[key], torch.Tensor):\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "        batch[\"image_0\"][0] = batch[\"image_0\"][0].to(device)\n",
    "        batch[\"image_1\"][0] = batch[\"image_1\"][0].to(device)\n",
    "        print(\"==============================================\")\n",
    "        print(f\"Batch: {idx}\")\n",
    "        \n",
    "        for layer_name in layer_names:\n",
    "            if any(substr in layer_name for substr in [\"encoder\", \"intermediate\", \"output\", \"attention\"]):\n",
    "                result = compute_ablated_loss(pl_module, batch, layer_name)\n",
    "                results.append(result)\n",
    "                print(f\"Ablated {layer_name} | ΔLoss: {result['delta_loss']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "Batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablated text_transformer.encoder.layer.0.intermediate.dense | ΔLoss: 0.0790\n",
      "Ablated text_transformer.encoder.layer.0.output.dense | ΔLoss: 0.0510\n",
      "Ablated text_transformer.encoder.layer.1.intermediate.dense | ΔLoss: 0.0227\n",
      "Ablated text_transformer.encoder.layer.1.output.dense | ΔLoss: 0.0438\n",
      "Ablated text_transformer.encoder.layer.2.intermediate.dense | ΔLoss: 0.0411\n",
      "Ablated text_transformer.encoder.layer.2.output.dense | ΔLoss: 0.0567\n",
      "Ablated text_transformer.encoder.layer.3.intermediate.dense | ΔLoss: -0.0165\n",
      "Ablated text_transformer.encoder.layer.3.output.dense | ΔLoss: -0.0236\n",
      "Ablated text_transformer.encoder.layer.8.intermediate.dense | ΔLoss: 0.0570\n",
      "Ablated text_transformer.encoder.layer.8.output.dense | ΔLoss: 0.0575\n",
      "Ablated text_transformer.encoder.layer.9.intermediate.dense | ΔLoss: -0.0088\n",
      "Ablated text_transformer.encoder.layer.9.output.dense | ΔLoss: -0.0140\n",
      "Layer text_transformer.encoder.layer.0.intermediate.dense:\n",
      "  Original Loss: 0.4641\n",
      "  Ablated Loss:  0.5432\n",
      "  ΔLoss:         0.0790\n",
      "\n",
      "Layer text_transformer.encoder.layer.0.output.dense:\n",
      "  Original Loss: 0.4929\n",
      "  Ablated Loss:  0.5438\n",
      "  ΔLoss:         0.0510\n",
      "\n",
      "Layer text_transformer.encoder.layer.1.intermediate.dense:\n",
      "  Original Loss: 0.4656\n",
      "  Ablated Loss:  0.4883\n",
      "  ΔLoss:         0.0227\n",
      "\n",
      "Layer text_transformer.encoder.layer.1.output.dense:\n",
      "  Original Loss: 0.4722\n",
      "  Ablated Loss:  0.5160\n",
      "  ΔLoss:         0.0438\n",
      "\n",
      "Layer text_transformer.encoder.layer.2.intermediate.dense:\n",
      "  Original Loss: 0.4591\n",
      "  Ablated Loss:  0.5002\n",
      "  ΔLoss:         0.0411\n",
      "\n",
      "Layer text_transformer.encoder.layer.2.output.dense:\n",
      "  Original Loss: 0.4648\n",
      "  Ablated Loss:  0.5214\n",
      "  ΔLoss:         0.0567\n",
      "\n",
      "Layer text_transformer.encoder.layer.3.intermediate.dense:\n",
      "  Original Loss: 0.4806\n",
      "  Ablated Loss:  0.4641\n",
      "  ΔLoss:         -0.0165\n",
      "\n",
      "Layer text_transformer.encoder.layer.3.output.dense:\n",
      "  Original Loss: 0.4792\n",
      "  Ablated Loss:  0.4556\n",
      "  ΔLoss:         -0.0236\n",
      "\n",
      "Layer text_transformer.encoder.layer.8.intermediate.dense:\n",
      "  Original Loss: 0.4544\n",
      "  Ablated Loss:  0.5113\n",
      "  ΔLoss:         0.0570\n",
      "\n",
      "Layer text_transformer.encoder.layer.8.output.dense:\n",
      "  Original Loss: 0.4650\n",
      "  Ablated Loss:  0.5225\n",
      "  ΔLoss:         0.0575\n",
      "\n",
      "Layer text_transformer.encoder.layer.9.intermediate.dense:\n",
      "  Original Loss: 0.4727\n",
      "  Ablated Loss:  0.4640\n",
      "  ΔLoss:         -0.0088\n",
      "\n",
      "Layer text_transformer.encoder.layer.9.output.dense:\n",
      "  Original Loss: 0.4760\n",
      "  Ablated Loss:  0.4619\n",
      "  ΔLoss:         -0.0140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define layers to analyze (example for transformer model)\n",
    "target_layers = [\n",
    "    \"text_transformer.encoder.layer.0.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.0.output.dense\",\n",
    "    \"text_transformer.encoder.layer.1.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.1.output.dense\",\n",
    "    \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.2.output.dense\",\n",
    "    \"text_transformer.encoder.layer.3.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.3.output.dense\",\n",
    "    \"text_transformer.encoder.layer.8.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.8.output.dense\",\n",
    "    \"text_transformer.encoder.layer.9.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.9.output.dense\",\n",
    "]\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "\n",
    "# input_batch = next(iter(infer_dataloader))\n",
    "# num_batches = len(infer_dataloader)\n",
    "\n",
    "# print(input_batch.keys())\n",
    "# print(f\"Number of batches: {num_batches}\")\n",
    "# print(f\"Samples in a batch: {len(input_batch['answers'])}\")\n",
    "\n",
    "# # Move input data to GPU\n",
    "# for key in input_batch:\n",
    "#     if isinstance(input_batch[key], torch.Tensor):\n",
    "#         input_batch[key] = input_batch[key].to(device)\n",
    "\n",
    "# input_batch[\"image_0\"][0] = input_batch[\"image_0\"][0].to(device)\n",
    "# input_batch[\"image_1\"][0] = input_batch[\"image_1\"][0].to(device)\n",
    "\n",
    "\n",
    "# Run ablation study\n",
    "ablation_results = ablation_analysis(\n",
    "    pl_module=model,\n",
    "    dataloader=infer_dataloader,\n",
    "    layer_names=target_layers\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "for result in ablation_results:\n",
    "    print(f\"Layer {result['layer']}:\")\n",
    "    print(f\"  Original Loss: {result['original_loss']:.4f}\")\n",
    "    print(f\"  Ablated Loss:  {result['ablated_loss']:.4f}\")\n",
    "    print(f\"  ΔLoss:         {result['delta_loss']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Conn Sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['table_name', 'answers', 'text', 'image_0', 'image_1', 'text_ids', 'text_labels', 'text_ids_mlm', 'text_labels_mlm', 'text_masks'])\n",
      "Number of batches: 1\n",
      "Samples in a batch: 10\n",
      "Connection Sensitivities:\n",
      "Layer: text_transformer.encoder.layer.0.intermediate.dense, Sensitivity: 0.001178910257294774\n",
      "Layer: text_transformer.encoder.layer.0.output.dense, Sensitivity: 0.0014642142923548818\n",
      "Layer: text_transformer.encoder.layer.1.intermediate.dense, Sensitivity: 0.001485239015892148\n",
      "Layer: text_transformer.encoder.layer.1.output.dense, Sensitivity: 0.0016155705088749528\n",
      "Layer: text_transformer.encoder.layer.2.intermediate.dense, Sensitivity: 0.0011649816296994686\n",
      "Layer: text_transformer.encoder.layer.2.output.dense, Sensitivity: 0.0012712652096524835\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense, Sensitivity: 0.001026810728944838\n",
      "Layer: text_transformer.encoder.layer.3.output.dense, Sensitivity: 0.0012590724509209394\n",
      "Layer: text_transformer.encoder.layer.8.intermediate.dense, Sensitivity: 0.0009010705398395658\n",
      "Layer: text_transformer.encoder.layer.8.output.dense, Sensitivity: 0.0011730052065104246\n",
      "Layer: text_transformer.encoder.layer.9.intermediate.dense, Sensitivity: 0.001071440288797021\n",
      "Layer: text_transformer.encoder.layer.9.output.dense, Sensitivity: 0.0013751366641372442\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def compute_connection_sensitivity(pl_module, batch, target_layers):\n",
    "    \"\"\"\n",
    "    Computes the connection sensitivity for specified layers in a PyTorch model\n",
    "    following the user's forward pass implementation.\n",
    "\n",
    "    Args:\n",
    "        pl_module (nn.Module): The PyTorch Lightning Module (or your model).\n",
    "        batch (dict): A dictionary representing the input batch (as expected by pl_module).\n",
    "        target_layers (list of str): List of layer names to compute sensitivity for.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of layer sensitivities, keyed by layer names.\n",
    "    \"\"\"\n",
    "    sensitivities = {}\n",
    "    device = next(pl_module.parameters()).device # Get device from model params\n",
    "\n",
    "    # Make sure gradients are enabled\n",
    "    for param in pl_module.parameters():\n",
    "        param.requires_grad_(True)\n",
    "    pl_module.train() # Set model to training mode for gradient calculation\n",
    "\n",
    "    # Zero out gradients\n",
    "    pl_module.zero_grad()\n",
    "\n",
    "    # Forward pass (following user's implementation)\n",
    "    infer1 = pl_module.infer(\n",
    "        batch, mask_text=False, mask_image=False, image_token_type_idx=1\n",
    "    )\n",
    "    infer2 = pl_module.infer(\n",
    "        batch, mask_text=False, mask_image=False, image_token_type_idx=2\n",
    "    )\n",
    "\n",
    "    cls_feats = torch.cat([infer1[\"cls_feats\"], infer2[\"cls_feats\"]], dim=-1)\n",
    "    nlvr2_logits = pl_module.nlvr2_classifier(cls_feats)\n",
    "\n",
    "    nlvr2_labels = batch[\"answers\"]\n",
    "    target = torch.tensor(nlvr2_labels).to(model.device).long()\n",
    "\n",
    "    # Loss calculation (following user's implementation)\n",
    "    loss = torch.nn.functional.cross_entropy(nlvr2_logits, target)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for layer_name in target_layers:\n",
    "        try:\n",
    "            layer = pl_module.get_submodule(layer_name)\n",
    "            if isinstance(layer, nn.Linear): # Assuming target layers are Linear layers\n",
    "                weight_grad = layer.weight.grad\n",
    "                if weight_grad is not None:\n",
    "                    abs_grad = torch.abs(weight_grad)\n",
    "                    sensitivity = torch.mean(abs_grad).item() # Average absolute gradient\n",
    "                    sensitivities[layer_name] = sensitivity\n",
    "                else:\n",
    "                    print(f\"Warning: Gradients not found for layer: {layer_name}. \"\n",
    "                          f\"Ensure layer parameters are part of the computational graph \"\n",
    "                          f\"and require gradients.\")\n",
    "                    sensitivities[layer_name] = None # Indicate no sensitivity calculated\n",
    "            else:\n",
    "                print(f\"Warning: Layer {layer_name} is not a Linear layer. \"\n",
    "                      f\"Connection sensitivity calculation is implemented for Linear layers in this example.\")\n",
    "                sensitivities[layer_name] = None # Indicate not applicable for this layer type\n",
    "        except AttributeError:\n",
    "            print(f\"Warning: Layer '{layer_name}' not found in the model.\")\n",
    "            sensitivities[layer_name] = None # Indicate layer not found\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "\n",
    "# 1. Define your pl_module (replace DummyTransformerModel with your actual pl_module)\n",
    "pl_module = model\n",
    "\n",
    "# 2. Define target layers\n",
    "target_layers = [\n",
    "    \"text_transformer.encoder.layer.0.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.0.output.dense\",\n",
    "    \"text_transformer.encoder.layer.1.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.1.output.dense\",\n",
    "    \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.2.output.dense\",\n",
    "    \"text_transformer.encoder.layer.3.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.3.output.dense\",\n",
    "    \"text_transformer.encoder.layer.8.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.8.output.dense\",\n",
    "    \"text_transformer.encoder.layer.9.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.9.output.dense\",\n",
    "]\n",
    "\n",
    "# 3. Initialize the dataloader\n",
    "input_batch = next(iter(infer_dataloader))\n",
    "num_batches = len(infer_dataloader)\n",
    "\n",
    "print(input_batch.keys())\n",
    "print(f\"Number of batches: {num_batches}\")\n",
    "print(f\"Samples in a batch: {len(input_batch['answers'])}\")\n",
    "\n",
    "# Move input data to GPU\n",
    "for key in input_batch:\n",
    "    if isinstance(input_batch[key], torch.Tensor):\n",
    "        input_batch[key] = input_batch[key].to(device)\n",
    "\n",
    "input_batch[\"image_0\"][0] = input_batch[\"image_0\"][0].to(device)\n",
    "input_batch[\"image_1\"][0] = input_batch[\"image_1\"][0].to(device)\n",
    "\n",
    "\n",
    "# 4. Compute connection sensitivities\n",
    "layer_sensitivities = compute_connection_sensitivity(pl_module, input_batch, target_layers)\n",
    "\n",
    "# 5. Print the results\n",
    "print(\"Connection Sensitivities:\")\n",
    "for layer_name, sensitivity in layer_sensitivities.items():\n",
    "    print(f\"Layer: {layer_name}, Sensitivity: {sensitivity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['table_name', 'answers', 'text', 'image_0', 'image_1', 'text_ids', 'text_labels', 'text_ids_mlm', 'text_labels_mlm', 'text_masks'])\n",
      "Number of batches: 1\n",
      "Samples in a batch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data-4/users/mileriso/envs/.dev/lib/python3.10/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def compute_connection_sensitivity(pl_module, batch, target_layers):\n",
    "    \"\"\"\n",
    "    Computes the connection sensitivity for specified layers in a PyTorch model\n",
    "    following the user's forward pass implementation.\n",
    "\n",
    "    Args:\n",
    "        pl_module (nn.Module): The PyTorch Lightning Module (or your model).\n",
    "        batch (dict): A dictionary representing the input batch (as expected by pl_module).\n",
    "        target_layers (list of str): List of layer names to compute sensitivity for.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of layer sensitivities, keyed by layer names.\n",
    "    \"\"\"\n",
    "    sensitivities = {}\n",
    "    device = next(pl_module.parameters()).device # Get device from model params\n",
    "\n",
    "    # Make sure gradients are enabled\n",
    "    for param in pl_module.parameters():\n",
    "        param.requires_grad_(True)\n",
    "    pl_module.train() # Set model to training mode for gradient calculation\n",
    "\n",
    "    # Zero out gradients\n",
    "    pl_module.zero_grad()\n",
    "\n",
    "    # Forward pass (following user's implementation)\n",
    "    infer1 = pl_module.infer(\n",
    "        batch, mask_text=False, mask_image=False, image_token_type_idx=1\n",
    "    )\n",
    "    infer2 = pl_module.infer(\n",
    "        batch, mask_text=False, mask_image=False, image_token_type_idx=2\n",
    "    )\n",
    "\n",
    "    cls_feats = torch.cat([infer1[\"cls_feats\"], infer2[\"cls_feats\"]], dim=-1)\n",
    "    nlvr2_logits = pl_module.nlvr2_classifier(cls_feats)\n",
    "\n",
    "    nlvr2_labels = batch[\"answers\"]\n",
    "    target = torch.tensor(nlvr2_labels).to(device).long() # Use 'device' here\n",
    "\n",
    "    # Loss calculation (following user's implementation)\n",
    "    loss = torch.nn.functional.cross_entropy(nlvr2_logits, target)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for layer_name in target_layers:\n",
    "        try:\n",
    "            layer = pl_module.get_submodule(layer_name)\n",
    "            if isinstance(layer, nn.Linear): # Assuming target layers are Linear layers\n",
    "                weight_grad = layer.weight.grad\n",
    "                if weight_grad is not None:\n",
    "                    abs_grad = torch.abs(weight_grad)\n",
    "                    sensitivity = torch.mean(abs_grad).item() # Average absolute gradient\n",
    "                    sensitivities[layer_name] = sensitivity\n",
    "                else:\n",
    "                    print(f\"Warning: Gradients not found for layer: {layer_name}. \"\n",
    "                          f\"Ensure layer parameters are part of the computational graph \"\n",
    "                          f\"and require gradients.\")\n",
    "                    sensitivities[layer_name] = None # Indicate no sensitivity calculated\n",
    "            else:\n",
    "                print(f\"Warning: Layer {layer_name} is not a Linear layer. \"\n",
    "                      f\"Connection sensitivity calculation is implemented for Linear layers in this example.\")\n",
    "                sensitivities[layer_name] = None # Indicate not applicable for this layer type\n",
    "        except AttributeError:\n",
    "            print(f\"Warning: Layer '{layer_name}' not found in the model.\")\n",
    "            sensitivities[layer_name] = None # Indicate layer not found\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Assuming 'model', 'infer_dataloader', and 'device' are already defined in your script\n",
    "\n",
    "    pl_module = model # Use your actual 'model' (pl_module)\n",
    "    device = \"cuda\"\n",
    "    model.to(device)\n",
    "\n",
    "    # 2. Define target layers\n",
    "    target_layers = [\n",
    "        \"text_transformer.encoder.layer.0.intermediate.dense\",\n",
    "        \"text_transformer.encoder.layer.0.output.dense\",\n",
    "        \"text_transformer.encoder.layer.1.intermediate.dense\",\n",
    "        \"text_transformer.encoder.layer.1.output.dense\",\n",
    "        \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "        \"text_transformer.encoder.layer.2.output.dense\",\n",
    "        \"text_transformer.encoder.layer.3.intermediate.dense\",\n",
    "        \"text_transformer.encoder.layer.3.output.dense\",\n",
    "        \"text_transformer.encoder.layer.8.intermediate.dense\",\n",
    "        \"text_transformer.encoder.layer.8.output.dense\",\n",
    "        \"text_transformer.encoder.layer.9.intermediate.dense\",\n",
    "        \"text_transformer.encoder.layer.9.output.dense\",\n",
    "    ]\n",
    "\n",
    "    # 3. Initialize the dataloader (assuming infer_dataloader is already defined)\n",
    "    input_batch = next(iter(infer_dataloader))\n",
    "    num_batches = len(infer_dataloader)\n",
    "\n",
    "    print(input_batch.keys())\n",
    "    print(f\"Number of batches: {num_batches}\")\n",
    "    print(f\"Samples in a batch: {len(input_batch['answers'])}\")\n",
    "\n",
    "    # Move input data to GPU\n",
    "    for key in input_batch:\n",
    "        if isinstance(input_batch[key], torch.Tensor):\n",
    "            input_batch[key] = input_batch[key].to(device)\n",
    "\n",
    "    if \"image_0\" in input_batch and isinstance(input_batch[\"image_0\"], list) and input_batch[\"image_0\"]:\n",
    "        input_batch[\"image_0\"][0] = input_batch[\"image_0\"][0].to(device)\n",
    "    if \"image_1\" in input_batch and isinstance(input_batch[\"image_1\"], list) and input_batch[\"image_1\"]:\n",
    "        input_batch[\"image_1\"][0] = input_batch[\"image_1\"][0].to(device)\n",
    "\n",
    "\n",
    "    # 4. Compute connection sensitivities\n",
    "    layer_sensitivities = compute_connection_sensitivity(pl_module, input_batch, target_layers)\n",
    "\n",
    "    # 5. Print the results in decreasing order of sensitivity\n",
    "    sorted_sensitivities = sorted(layer_sensitivities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    print(\"Connection Sensitivities (Decreasing Order):\")\n",
    "    for layer_name, sensitivity in sorted_sensitivities:\n",
    "        print(f\"Layer: {layer_name}, Sensitivity: {sensitivity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
