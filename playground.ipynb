{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Notebook For Quantizing VLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import torch\n",
    "import torch.quantization\n",
    "import pytorch_lightning as pl\n",
    "import copy\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Model Specific imports\n",
    "from vilt.datamodules.multitask_datamodule import MTDataModule as MTDataModuleVILT\n",
    "from meter.datamodules.multitask_datamodule import MTDataModule as MTDataModuleMETER\n",
    "from vilt.modules import ViLTransformerSS\n",
    "from meter.modules import METERTransformerSS\n",
    "\n",
    "# Custom imports\n",
    "import configs\n",
    "from quantization_utils import get_quantization_config\n",
    "from quantization_utils import  SmallMTDataModuleMETER, SmallMTDataModuleVILT\n",
    "\n",
    "from torchao.quantization.prototype.qat import Int8DynActInt4WeightQATQuantizer\n",
    "import quantization_utils\n",
    "import configs\n",
    "from vilt.modules.kd_module import NLVR2LightningModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Limit the number of CPUs\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"9\"  # Set this to the number of CPUs you want to use\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"9\"  # Set this to the number of CPUs you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "# Set the configuration\n",
    "_config = configs.vilt_config_nlvr2\n",
    "_config[\"batch_size\"] = 18\n",
    "_config[\"per_gpu_batchsize\"] = 18\n",
    "_config[\"learning_rate\"] = 0.001\n",
    "\n",
    "# Set the PyTorch Lightning seed\n",
    "pl.seed_everything(_config[\"seed\"])\n",
    "\n",
    "# Limit the number of CPUs\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"10\"  # Set this to the number of CPUs you want to use\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"10\"  # Set this to the number of CPUs you want to use\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Distributed Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized: True\n"
     ]
    }
   ],
   "source": [
    "import torch.distributed as dist\n",
    "# Initialize the process group\n",
    "dist.init_process_group(backend='gloo', init_method='env://', world_size=1, rank=0)\n",
    "\n",
    "# Verify initialization\n",
    "print(f\"Initialized: {dist.is_initialized()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Function to print the size of the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to get the size\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 18\n",
      "Lenght of the finetune dataloader: 3\n",
      "Length of test dataloader: 3\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ========= Create full datamodule =========\n",
    "# ==========================================\n",
    "if \"meter\" in _config[\"model\"]:\n",
    "    full_dm = MTDataModuleMeter(_config, dist=False)\n",
    "    \n",
    "    test_dm = SmallMTDataModuleMETER(_config, dist=False, percentage=0.1)\n",
    "    test_dm.setup(\"test\", is_random=True)\n",
    "    test_dataloader = test_dm.test_dataloader()\n",
    "    \n",
    "    fine_tune_dm = SmallMTDataModuleMETER(_config, dist=False, percentage=0.5)\n",
    "    # fine_tune_dm = SmallMTDataModuleMETER(_config, dist=False, num_samples=8, start_idx=0)\n",
    "    fine_tune_dm.setup(\"fit\", is_random=True)\n",
    "    fine_tune_dataloader = fine_tune_dm.test_dataloader()\n",
    "    \n",
    "\n",
    "elif \"vilt\" in _config[\"model\"]:\n",
    "    full_dm = MTDataModuleVILT(_config, dist=False)\n",
    "\n",
    "    test_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=50)\n",
    "    test_dm.setup(\"test\", is_random=True)\n",
    "    test_dataloader = test_dm.test_dataloader()\n",
    "\n",
    "    fine_tune_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=50)\n",
    "    fine_tune_dm.setup(\"test\", is_random=True)\n",
    "    fine_tune_dataloader = fine_tune_dm.test_dataloader()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported: \", _config[\"model\"])\n",
    "\n",
    "print(f\"Batch size: {_config['batch_size']}\")\n",
    "print(f\"Lenght of the finetune dataloader: {len(fine_tune_dataloader)}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ViLT model\n"
     ]
    }
   ],
   "source": [
    "if _config[\"model\"] == \"vilt\":\n",
    "    model = ViLTransformerSS(_config)\n",
    "    print(\"Initialized ViLT model\")\n",
    "\n",
    "elif _config[\"model\"] == \"meter\":\n",
    "    model = METERTransformerSS(_config)\n",
    "    print(\"Initialized METER model\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported: \", _config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize The Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "# ========== Initialize the trainer for full precision ==========\n",
    "def init_trainer(_config, accelerator, num_devices, max_epochs, max_steps):\n",
    "    exp_name = f'{_config[\"exp_name\"]}'\n",
    "\n",
    "    os.makedirs(_config[\"log_dir\"], exist_ok=True)\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val/the_metric\",\n",
    "        mode=\"max\",\n",
    "        save_last=True,\n",
    "    )\n",
    "    logger = pl.loggers.TensorBoardLogger(\n",
    "        _config[\"log_dir\"],\n",
    "        name=f'{exp_name}_seed{_config[\"seed\"]}_from_{_config[\"load_path\"].split(\"/\")[-1][:-5]}',\n",
    "    )\n",
    "\n",
    "    lr_callback = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "    # callbacks = [lr_callback]\n",
    "    callbacks = [checkpoint_callback, lr_callback]\n",
    "\n",
    "    num_gpus = (\n",
    "        _config[\"num_gpus\"]\n",
    "        if isinstance(_config[\"num_gpus\"], int)\n",
    "        else len(_config[\"num_gpus\"])\n",
    "    )\n",
    "\n",
    "    grad_steps = max(_config[\"batch_size\"] // (\n",
    "        _config[\"per_gpu_batchsize\"] * num_gpus * _config[\"num_nodes\"]\n",
    "    ), 1)\n",
    "\n",
    "    # max_steps = _config[\"max_steps\"] if _config[\"max_steps\"] is not None else None\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "            accelerator=accelerator,\n",
    "            devices=num_devices,\n",
    "            num_nodes=_config[\"num_nodes\"],\n",
    "            precision=_config[\"precision\"],\n",
    "            # strategy=\"ddp\",\n",
    "            benchmark=True,\n",
    "            deterministic=False,\n",
    "            max_epochs=max_epochs,\n",
    "            max_steps=max_steps,\n",
    "            callbacks=callbacks,\n",
    "            logger=logger,\n",
    "            accumulate_grad_batches=grad_steps,\n",
    "            log_every_n_steps=10,\n",
    "            fast_dev_run=_config[\"fast_dev_run\"],\n",
    "            val_check_interval=_config[\"val_check_interval\"],\n",
    "        )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "trainer = init_trainer(_config, accelerator=\"gpu\", num_devices=1, max_epochs=3, max_steps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantization_config_dict(bits, module_name_list):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of quantization configurations for specific modules in a model.\n",
    "    \n",
    "    Args:\n",
    "        bits (int): The number of bits to quantize the model to. Available options are 8, 4, and 2.\n",
    "        module_name_list (list of str): A list of module names (or dot-separated paths) within the model to quantize.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of quantization configurations for the specified modules.\n",
    "    \"\"\"\n",
    "\n",
    "    quantization_config, embedding_config = get_quantization_config(bits)\n",
    "    modules_config = {}\n",
    "\n",
    "    for module_name in module_name_list:\n",
    "        if \"embedding\" in module_name:\n",
    "            modules_config[module_name] = embedding_config\n",
    "        else:\n",
    "            modules_config[module_name] = quantization_config\n",
    "    \n",
    "    return modules_config\n",
    "\n",
    "\n",
    "def quantize_modules(model, bits, module_name_list, inplace=True):\n",
    "    \"\"\"\n",
    "    Quantizes specific modules in a deep copy of the input model using dynamic quantization.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to quantize.\n",
    "        bits (int): The number of bits to quantize the model to. Available options are 8, 4, and 2.\n",
    "        module_names_to_quantize (list of str): A list of module names (or dot-separated paths)\n",
    "                                                 within the model to apply dynamic quantization to.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: A deep copy of the input model with specified modules dynamically quantized.\n",
    "                         Returns None if no modules are provided to quantize.\n",
    "    \"\"\"\n",
    "\n",
    "    modules_config = create_quantization_config_dict(bits, module_name_list)\n",
    "\n",
    "    model_quantized = deepcopy(model)\n",
    "    \n",
    "    \n",
    "    torch.quantization.quantize_dynamic(\n",
    "        model_quantized, modules_config, inplace=True\n",
    "    )\n",
    "\n",
    "    return model_quantized\n",
    "\n",
    "def print_frozen_layers(model):\n",
    "    \"\"\"\n",
    "    Prints the names of the layers in a PyTorch model and whether they are frozen or not.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to print the frozen status of.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name}, Frozen: {not param.requires_grad}\")\n",
    "\n",
    "\n",
    "def freeze_except_layers(model, layers_to_unfreeze_names):\n",
    "    \"\"\"\n",
    "    Freezes all parameters of a PyTorch model except for the layers specified by their names.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to freeze parameters in.\n",
    "        layers_to_unfreeze_names (list of str): A list of module names that should NOT be frozen.\n",
    "                                             Parameters in modules whose names contain these strings will be unfrozen.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        freeze = True  # Initially assume we should freeze the parameter\n",
    "        for layer_name_to_unfreeze in layers_to_unfreeze_names:\n",
    "            if layer_name_to_unfreeze in name:\n",
    "                freeze = False  # Unfreeze if the name contains a layer to unfreeze\n",
    "                break  # No need to check other layer names if already unfrozen\n",
    "\n",
    "        if freeze:\n",
    "            param.requires_grad = False  # Freeze the parameter\n",
    "        else:\n",
    "            param.requires_grad = True   # Ensure it's unfrozen (explicitly set to True)\n",
    "\n",
    "    # Optional: Print which layers are frozen and unfrozen for verification\n",
    "    print_frozen_layers(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Int8DynActInt4WeightQATQuantizer\n",
      "Inside Int8DynActInt4WeightQATQuantizer.prepare\n",
      "Inside of _replace_linear_8da4w fucntion from GPTQ script\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "==================CHILD: text_transformer.encoder.layer.2.intermediate==================\n",
      "==================TYPE: <class 'transformers.models.roberta.modeling_roberta.RobertaIntermediate'>==================\n",
      "================== child is NOT nn.Linear ==================\n",
      "==================CHILD: text_transformer.encoder.layer.2.intermediate.dense==================\n",
      "==================TYPE: <class 'torch.nn.modules.linear.Linear'>==================\n",
      "================== child is nn.Linear ==================\n",
      "================== child.bias is NOT None ==================\n",
      "================== _check_linear_int4_k(child.in_features, groupsize) ==================\n",
      "================== NOT padding_allowed ==================\n",
      "Inside Int8DynActInt4WeightQATLinear class within replace_fn\n",
      "model is replaced with replacement_fn\n",
      "==================CHILD: text_transformer.encoder.layer.2.intermediate.intermediate_act_fn==================\n",
      "==================TYPE: <class 'transformers.activations.GELUActivation'>==================\n",
      "================== child is NOT nn.Linear ==================\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "==================CHILD: text_transformer.encoder.layer.2.output==================\n",
      "==================TYPE: <class 'transformers.models.roberta.modeling_roberta.RobertaOutput'>==================\n",
      "================== child is NOT nn.Linear ==================\n",
      "==================CHILD: text_transformer.encoder.layer.2.output.dense==================\n",
      "==================TYPE: <class 'torch.nn.modules.linear.Linear'>==================\n",
      "================== child is nn.Linear ==================\n",
      "================== child.bias is NOT None ==================\n",
      "================== _check_linear_int4_k(child.in_features, groupsize) ==================\n",
      "================== NOT padding_allowed ==================\n",
      "Inside Int8DynActInt4WeightQATLinear class within replace_fn\n",
      "model is replaced with replacement_fn\n",
      "==================CHILD: text_transformer.encoder.layer.2.output.LayerNorm==================\n",
      "==================TYPE: <class 'torch.nn.modules.normalization.LayerNorm'>==================\n",
      "================== child is NOT nn.Linear ==================\n",
      "model is NOT replaced with replacement_fn\n",
      "==================CHILD: text_transformer.encoder.layer.2.output.dropout==================\n",
      "==================TYPE: <class 'torch.nn.modules.dropout.Dropout'>==================\n",
      "================== child is NOT nn.Linear ==================\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "model is NOT replaced with replacement_fn\n",
      "Freezing all layers except: ['text_transformer.encoder.layer.2.output.dense', 'text_transformer.encoder.layer.2.intermediate.dense']\n",
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.weight, Frozen: False\n"
     ]
    }
   ],
   "source": [
    "modules_to_quantize = [\n",
    "    \"text_transformer.encoder.layer.2.output.dense\",\n",
    "    \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "    # \"text_transformer.encoder.layer.3.output.dense\",\n",
    "    # \"text_transformer.encoder.layer.3.intermediate.dense\"\n",
    "]\n",
    "\n",
    "\n",
    "# Quantizer for int8 dynamic per token activations +\n",
    "# int4 grouped per channel weights, only for linear layers\n",
    "qat_quantizer = Int8DynActInt4WeightQATQuantizer()\n",
    "\n",
    "# Insert \"fake quantize\" operations into linear layers.\n",
    "# These operations simulate quantization numerics during\n",
    "# training without performing any dtype casting\n",
    "model = qat_quantizer.prepare(model)\n",
    "\n",
    "# Freeze all layers except the ones to be quantized\n",
    "print(f\"Freezing all layers except: {modules_to_quantize}\")\n",
    "freeze_except_layers(model, modules_to_quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METERTransformerSS(\n",
      "  (cross_modal_text_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (cross_modal_image_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (token_type_embeddings): Embedding(3, 768)\n",
      "  (vit_model): CLIP(\n",
      "    (visual): VisualTransformer(\n",
      "      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (transformer): Transformer(\n",
      "        (resblocks): Sequential(\n",
      "          (0): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (9): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (10): ResidualAttentionBlock(\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (text_transformer): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Int8DynActInt4WeightQATLinear(\n",
      "              in_features=768, out_features=3072, bias=False\n",
      "              (activation_fake_quantizer): FakeQuantizer()\n",
      "              (weight_fake_quantizer): FakeQuantizer()\n",
      "            )\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Int8DynActInt4WeightQATLinear(\n",
      "              in_features=3072, out_features=768, bias=False\n",
      "              (activation_fake_quantizer): FakeQuantizer()\n",
      "              (weight_fake_quantizer): FakeQuantizer()\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3-11): 9 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (cross_modal_image_layers): ModuleList(\n",
      "    (0-5): 6 x BertCrossLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (crossattention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_modal_text_layers): ModuleList(\n",
      "    (0-5): 6 x BertCrossLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (crossattention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_modal_image_pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (cross_modal_text_pooler): Pooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (nlvr2_classifier): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Linear(in_features=1536, out_features=2, bias=True)\n",
      "  )\n",
      "  (train_nlvr2_accuracy): Accuracy()\n",
      "  (train_nlvr2_loss): Scalar()\n",
      "  (dev_nlvr2_accuracy): Accuracy()\n",
      "  (dev_nlvr2_loss): Scalar()\n",
      "  (test_nlvr2_accuracy): Accuracy()\n",
      "  (test_nlvr2_loss): Scalar()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in Millons: 4.718592\n"
     ]
    }
   ],
   "source": [
    "# Count the number of parameters in the model that requires gradients\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters in Millons: {num_params/1e6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name                        | Type         | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0  | cross_modal_text_transform  | Linear       | 590 K  | train\n",
      "1  | cross_modal_image_transform | Linear       | 590 K  | train\n",
      "2  | token_type_embeddings       | Embedding    | 2.3 K  | train\n",
      "3  | vit_model                   | CLIP         | 78.9 M | train\n",
      "4  | text_transformer            | RobertaModel | 124 M  | eval \n",
      "5  | cross_modal_image_layers    | ModuleList   | 56.7 M | train\n",
      "6  | cross_modal_text_layers     | ModuleList   | 56.7 M | train\n",
      "7  | cross_modal_image_pooler    | Pooler       | 590 K  | train\n",
      "8  | cross_modal_text_pooler     | Pooler       | 590 K  | train\n",
      "9  | nlvr2_classifier            | Sequential   | 4.7 M  | train\n",
      "10 | train_nlvr2_accuracy        | Accuracy     | 0      | train\n",
      "11 | train_nlvr2_loss            | Scalar       | 0      | train\n",
      "12 | dev_nlvr2_accuracy          | Accuracy     | 0      | train\n",
      "13 | dev_nlvr2_loss              | Scalar       | 0      | train\n",
      "14 | test_nlvr2_accuracy         | Accuracy     | 0      | train\n",
      "15 | test_nlvr2_loss             | Scalar       | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "319 M     Non-trainable params\n",
      "324 M     Total params\n",
      "1,296.017 Total estimated model params size (MB)\n",
      "471       Modules in train mode\n",
      "226       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/89 [00:00<?, ?it/s]                            "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 156.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 102.44 MiB is free. Including non-PyTorch memory, this process has 11.79 GiB memory in use. Of the allocated memory 11.31 GiB is allocated by PyTorch, and 309.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tune_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1277\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/transformers/optimization.py:619\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    617\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     optimizer: Steppable,\n\u001b[1;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:282\u001b[0m, in \u001b[0;36mMETERTransformerSS.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    280\u001b[0m meter_utils\u001b[38;5;241m.\u001b[39mset_task(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# print(f\"Executing training_step for task {self.current_tasks}\")\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# print(f\"Output is obtained: {output.keys()}\")\u001b[39;00m\n\u001b[1;32m    284\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k])\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:267\u001b[0m, in \u001b[0;36mMETERTransformerSS.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Natural Language for Visual Reasoning 2\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlvr2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_tasks:\n\u001b[0;32m--> 267\u001b[0m     ret\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mobjectives\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_nlvr2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# SNLI Visual Entailment\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnli\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_tasks:\n",
      "File \u001b[0;32m~/thesis/meter/modules/objectives.py:184\u001b[0m, in \u001b[0;36mcompute_nlvr2\u001b[0;34m(pl_module, batch)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_nlvr2\u001b[39m(pl_module, batch):\n\u001b[1;32m    181\u001b[0m     infer1 \u001b[38;5;241m=\u001b[39m pl_module\u001b[38;5;241m.\u001b[39minfer(\n\u001b[1;32m    182\u001b[0m         batch, mask_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mask_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, image_token_type_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     infer2 \u001b[38;5;241m=\u001b[39m \u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_token_type_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     cls_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([infer1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m], infer2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    189\u001b[0m     nlvr2_logits \u001b[38;5;241m=\u001b[39m pl_module\u001b[38;5;241m.\u001b[39mnlvr2_classifier(cls_feats)\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:223\u001b[0m, in \u001b[0;36mMETERTransformerSS.infer\u001b[0;34m(self, batch, mask_text, mask_image, image_token_type_idx, img)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_layer, image_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_modal_text_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_modal_image_layers):\n\u001b[1;32m    222\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m text_layer(x, y, extend_text_masks, extend_image_masks)\n\u001b[0;32m--> 223\u001b[0m     y1 \u001b[38;5;241m=\u001b[39m \u001b[43mimage_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextend_image_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextend_text_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x1[\u001b[38;5;241m0\u001b[39m], y1[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    226\u001b[0m text_feats, image_feats \u001b[38;5;241m=\u001b[39m x, y\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/meter/modules/bert_model.py:471\u001b[0m, in \u001b[0;36mBertCrossLayer.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    463\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m ):\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m#past_key_value[:2] if past_key_value is not None else None\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/meter/modules/bert_model.py:406\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    398\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    405\u001b[0m ):\n\u001b[0;32m--> 406\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    416\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/meter/modules/bert_model.py:338\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    334\u001b[0m     attention_probs\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_attn_gradients) \n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 156.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 102.44 MiB is free. Including non-PyTorch memory, this process has 11.79 GiB memory in use. Of the allocated memory 11.31 GiB is allocated by PyTorch, and 309.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, fine_tune_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import torch\n",
    "\n",
    "# torch.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FV1 weight shape: torch.Size([3072, 768])\n",
      "FC2 weight shape: torch.Size([768, 3072])\n",
      "FC2 bias shape: torch.Size([768])\n",
      "Ref weight shape: torch.Size([3072, 768])\n"
     ]
    }
   ],
   "source": [
    "# layer_to_analzye = [\n",
    "#     \"text_transformer.encoder.layer.2.output.dense\",\n",
    "#     \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "# ]\n",
    "\n",
    "# freeze_except_layers(model, layer_to_analzye)\n",
    "\n",
    "fc1_weight = model.text_transformer.encoder.layer[2].intermediate.dense.weight.clone()\n",
    "fc2_weight = model.text_transformer.encoder.layer[2].output.dense.weight.clone()\n",
    "fc2_bias = model.text_transformer.encoder.layer[2].output.dense.bias.clone()\n",
    "\n",
    "ref_weight = model.text_transformer.encoder.layer[0].intermediate.dense.weight.clone()\n",
    "\n",
    "print(f\"FV1 weight shape: {fc1_weight.shape}\")\n",
    "print(f\"FC2 weight shape: {fc2_weight.shape}\")\n",
    "print(f\"FC2 bias shape: {fc2_bias.shape}\")\n",
    "print(f\"Ref weight shape: {ref_weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW, Adam, SGD\n",
    "from transformers import get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n",
    "\n",
    "def print_frozen_layers(model):\n",
    "    \"\"\"\n",
    "    Prints the names of the layers in a PyTorch model and whether they are frozen or not.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to print the frozen status of.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name}, Frozen: {not param.requires_grad}\")\n",
    "\n",
    "\n",
    "def freeze_except_layers(model, layers_to_unfreeze_names):\n",
    "    \"\"\"\n",
    "    Freezes all parameters of a PyTorch model except for the layers specified by their names.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to freeze parameters in.\n",
    "        layers_to_unfreeze_names (list of str): A list of module names that should NOT be frozen.\n",
    "                                             Parameters in modules whose names contain these strings will be unfrozen.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        freeze = True  # Initially assume we should freeze the parameter\n",
    "        for layer_name_to_unfreeze in layers_to_unfreeze_names:\n",
    "            if layer_name_to_unfreeze in name:\n",
    "                freeze = False  # Unfreeze if the name contains a layer to unfreeze\n",
    "                break  # No need to check other layer names if already unfrozen\n",
    "\n",
    "        if freeze:\n",
    "            param.requires_grad = False  # Freeze the parameter\n",
    "        else:\n",
    "            param.requires_grad = True   # Ensure it's unfrozen (explicitly set to True)\n",
    "\n",
    "    # Optional: Print which layers are frozen and unfrozen for verification\n",
    "    print_frozen_layers(model)\n",
    "\n",
    "def compute_nlvr2(pl_module, batch):\n",
    "    infer1 = pl_module.infer(\n",
    "        batch, mask_text=False, mask_image=False, image_token_type_idx=1\n",
    "    )\n",
    "    infer2 = pl_module.infer(\n",
    "        batch, mask_text=False, mask_image=False, image_token_type_idx=2\n",
    "    )\n",
    "\n",
    "    cls_feats = torch.cat([infer1[\"cls_feats\"], infer2[\"cls_feats\"]], dim=-1)\n",
    "    nlvr2_logits = pl_module.nlvr2_classifier(cls_feats)\n",
    "\n",
    "    nlvr2_labels = batch[\"answers\"]\n",
    "    nlvr2_labels = torch.tensor(nlvr2_labels).to(pl_module.device).long()\n",
    "    nlvr2_loss = F.cross_entropy(nlvr2_logits, nlvr2_labels)\n",
    "\n",
    "    ret = {\n",
    "        \"nlvr2_loss\": nlvr2_loss,\n",
    "        \"nlvr2_logits\": nlvr2_logits,\n",
    "        \"nlvr2_labels\": nlvr2_labels,\n",
    "    }\n",
    "    return ret\n",
    "\n",
    "def set_schedule_simple_optimizer(pl_module, max_steps):\n",
    "    lr = pl_module.hparams.config[\"learning_rate\"]\n",
    "    wd = 0.0  # No weight decay for any parameters as requested\n",
    "\n",
    "    optim_type = \"adam\" #pl_module.hparams.config[\"optim_type\"]\n",
    "\n",
    "    # Get all parameters that require gradients\n",
    "    params_to_optimize = [p for p in pl_module.parameters() if p.requires_grad]\n",
    "\n",
    "    if optim_type == \"adamw\":\n",
    "        optimizer = AdamW(\n",
    "            params_to_optimize, lr=lr, eps=1e-8, betas=(0.9, 0.98), weight_decay=wd\n",
    "        )\n",
    "    elif optim_type == \"adam\":\n",
    "        optimizer = Adam(params_to_optimize, lr=lr, weight_decay=wd)\n",
    "    elif optim_type == \"sgd\":\n",
    "        optimizer = SGD(params_to_optimize, lr=lr, momentum=0.9, weight_decay=wd)\n",
    "\n",
    "    max_steps = max_steps\n",
    "\n",
    "    warmup_steps = pl_module.hparams.config[\"warmup_steps\"]\n",
    "    if isinstance(pl_module.hparams.config[\"warmup_steps\"], float):\n",
    "        warmup_steps = int(max_steps * warmup_steps)\n",
    "\n",
    "    decay_power = pl_module.hparams.config[\"decay_power\"]\n",
    "    end_lr = pl_module.hparams.config[\"end_lr\"]\n",
    "\n",
    "    if decay_power == \"cosine\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=max_steps,\n",
    "            lr_end=end_lr,\n",
    "            power=decay_power,\n",
    "        )\n",
    "\n",
    "    print(\"Optimizer: \", optimizer)\n",
    "    print(\"Warmup steps: \", warmup_steps)\n",
    "    print(\"Max steps: \", max_steps)\n",
    "    print(\"End LR: \", end_lr)\n",
    "    print(\"Decay power: \", decay_power)\n",
    "\n",
    "    sched = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "\n",
    "    return (\n",
    "        [optimizer],\n",
    "        [sched],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_nlvr2_loop(pl_module, train_dataloader, val_dataloader, num_epochs, accumulation_steps, max_steps):\n",
    "    \"\"\"\n",
    "    Training loop for NLVR2 task using a simplified optimizer configuration.\n",
    "\n",
    "    Args:\n",
    "        pl_module: The PyTorch Lightning module.\n",
    "        train_dataloader: PyTorch DataLoader for the training dataset.\n",
    "        num_epochs: Number of training epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize optimizer and scheduler using the simplified config\n",
    "    optimizers, schedulers = set_schedule_simple_optimizer(pl_module, max_steps)\n",
    "    print(\"Optimizer: \", optimizers)\n",
    "    optimizer = optimizers[0]  # Assuming only one optimizer is returned\n",
    "    scheduler = schedulers[0]['scheduler'] # Assuming only one scheduler is returned\n",
    "\n",
    "\n",
    "    global_step = 0 # Track global steps for scheduler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        count = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(pl_module.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "            batch[\"image_0\"][0] = batch[\"image_0\"][0].to(pl_module.device)\n",
    "            batch[\"image_1\"][0] = batch[\"image_1\"][0].to(pl_module.device)\n",
    "\n",
    "\n",
    "            # Compute loss and logits\n",
    "            output = compute_nlvr2(pl_module, batch)\n",
    "            loss = output[\"nlvr2_loss\"]\n",
    "            logits = output[\"nlvr2_logits\"]\n",
    "            labels = output[\"nlvr2_labels\"]\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step() # Update learning rate scheduler\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "\n",
    "            if (batch_idx % accumulation_steps) == 0:\n",
    "                count +=1\n",
    "\n",
    "            # Print batch loss and accuracy (optional, print every few batches)\n",
    "            if count % 2 == 0:\n",
    "                batch_accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item() * accumulation_steps:.4f}, Accuracy: {batch_accuracy:.4f}\")\n",
    "        \n",
    "        # Calculate average epoch loss and accuracy\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        avg_epoch_accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Summary: Avg Loss: {avg_epoch_loss:.4f}, Avg Accuracy: {avg_epoch_accuracy:.4f}\")\n",
    "\n",
    "        # Validation after each epoch\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_dataloader):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(pl_module.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "                batch[\"image_0\"][0] = batch[\"image_0\"][0].to(pl_module.device)\n",
    "                batch[\"image_1\"][0] = batch[\"image_1\"][0].to(pl_module.device)\n",
    "\n",
    "\n",
    "                # Compute loss and logits\n",
    "                output = compute_nlvr2(pl_module, batch)\n",
    "                loss = output[\"nlvr2_loss\"]\n",
    "                logits = output[\"nlvr2_logits\"]\n",
    "                labels = output[\"nlvr2_labels\"]\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                correct_predictions += (predictions == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Validation Loss: {running_loss:.4f}\")\n",
    "        \n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.bias, Frozen: False\n"
     ]
    }
   ],
   "source": [
    "# model.train()\n",
    "modules_to_quantize = [\n",
    "    \"text_transformer.encoder.layer.2.output.dense\",\n",
    "    \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "]\n",
    "\n",
    "freeze_except_layers(model, modules_to_quantize)\n",
    "\n",
    "# train_nlvr2_loop(model, fine_tune_dataloader, test_dataloader, num_epochs=3, accumulation_steps=1, max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Quant function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights of fc2:\n",
      " tensor([[ 0.0322,  0.0516,  0.0236,  ..., -0.0364,  0.0050, -0.0298],\n",
      "        [-0.0202, -0.0092,  0.0333,  ...,  0.0504,  0.0159, -0.0043],\n",
      "        [-0.0002,  0.0391, -0.0352,  ..., -0.0041,  0.0267,  0.0590],\n",
      "        ...,\n",
      "        [ 0.0213, -0.0371, -0.0602,  ..., -0.0041,  0.0116,  0.0163],\n",
      "        [ 0.0368, -0.0218, -0.0233,  ..., -0.0277,  0.0425,  0.1078],\n",
      "        [-0.0381, -0.0223,  0.0277,  ..., -0.0283, -0.0504,  0.0436]])\n",
      "Dequantized weights of fc2:\n",
      " tensor([[0., 0., 0.,  ..., -0., 0., -0.],\n",
      "        [-0., -0., 0.,  ..., 0., 0., -0.],\n",
      "        [-0., 0., -0.,  ..., -0., 0., 0.],\n",
      "        ...,\n",
      "        [0., -0., -0.,  ..., -0., 0., 0.],\n",
      "        [0., -0., -0.,  ..., -0., 0., 1.],\n",
      "        [-0., -0., 0.,  ..., -0., -0., 0.]])\n",
      "Min and max of dequantized fc2 weights: -7.0, 7.0\n",
      "Mean Absolute Error (MAE) between original and dequantized weights of fc2: 0.15424813330173492\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from copy import deepcopy\n",
    "\n",
    "# class WeightQuantizer:\n",
    "#     def __init__(self, q_min=0, q_max=255, symmetric=False, dtype=torch.quint8):\n",
    "#         \"\"\"\n",
    "#         Initializes the WeightQuantizer.\n",
    "\n",
    "#         Args:\n",
    "#             q_min (int): Minimum quantization range value. Defaults to 0.\n",
    "#             q_max (int): Maximum quantization range value. Defaults to 255.\n",
    "#             symmetric (bool): Whether to use symmetric quantization. Defaults to False.\n",
    "#             dtype (torch.dtype): Quantized data type, used to determine default zero_point for symmetric quantization.\n",
    "#                                  Defaults to torch.quint8.\n",
    "#         \"\"\"\n",
    "#         self.q_min = q_min\n",
    "#         self.q_max = q_max\n",
    "#         self.symmetric = symmetric\n",
    "#         self.dtype = dtype\n",
    "#         self.x_min = None\n",
    "#         self.x_max = None\n",
    "#         self.scale = None\n",
    "#         self.zero_point = None\n",
    "\n",
    "#     def __call__(self, X):\n",
    "#         \"\"\"\n",
    "#         Applies weight quantization to the input tensor X and returns dequantized version.\n",
    "\n",
    "#         Args:\n",
    "#             X (torch.Tensor): Input tensor to be quantized.\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: Dequantized tensor.\n",
    "#         \"\"\"\n",
    "#         if self.x_min is None:\n",
    "#             self.x_min = torch.min(X)\n",
    "#         else:\n",
    "#             self.x_min = torch.min(self.x_min, torch.min(X))\n",
    "\n",
    "#         if self.x_max is None:\n",
    "#             self.x_max = torch.max(X)\n",
    "#         else:\n",
    "#             self.x_max = torch.max(self.x_max, torch.max(X))\n",
    "\n",
    "#         if self.x_min == self.x_max:\n",
    "#             scale = 1.0\n",
    "#             zero_point = 0\n",
    "#         else:\n",
    "#             if self.symmetric:\n",
    "#                 s = 2 * max(abs(self.x_min), self.x_max) / (self.q_max - self.q_min)\n",
    "#                 z = 0 if self.dtype == torch.qint8 else 128 # Default to 128 for quint8 and other unsigned types.\n",
    "\n",
    "#             else:\n",
    "#                 s = (self.x_max - self.x_min) / (self.q_max - self.q_min)\n",
    "#                 z = self.q_min - round(self.x_min / s)\n",
    "\n",
    "#             scale = s\n",
    "#             zero_point = int(z) # Ensure zero_point is integer as per quantization definition\n",
    "\n",
    "#         if scale == 0: # Handle potential division by zero if range is too small.\n",
    "#             scale = 1.0\n",
    "\n",
    "#         self.scale = scale # Store scale and zero_point for potential later use if needed\n",
    "#         self.zero_point = zero_point\n",
    "\n",
    "#         # Quantize\n",
    "#         quantized_values = torch.round(torch.clamp((X / scale) + zero_point, self.q_min, self.q_max))\n",
    "\n",
    "#         # Dequantize\n",
    "#         dequantized_values = (quantized_values - zero_point) * scale\n",
    "#         return dequantized_values\n",
    "\n",
    "\n",
    "# def apply_weight_quantization(model, q_min=0, q_max=255, symmetric=False, dtype=torch.quint8, modules_to_quantize=None):\n",
    "#     \"\"\"\n",
    "#     Applies weight quantization to specified weight parameters in a PyTorch model\n",
    "#     and returns dequantized weights in place.\n",
    "\n",
    "#     Args:\n",
    "#         model (torch.nn.Module): PyTorch model to quantize weights for.\n",
    "#         q_min (int): Minimum quantization range value. Defaults to 0.\n",
    "#         q_max (int): Maximum quantization range value. Defaults to 255.\n",
    "#         symmetric (bool): Whether to use symmetric quantization. Defaults to False.\n",
    "#         dtype (torch.dtype): Quantized data type. Defaults to torch.quint8.\n",
    "#         modules_to_quantize (list of str, optional): List of module names to quantize.\n",
    "#                                                      If None, quantizes Linear, Conv2d, and ConvTranspose2d layers.\n",
    "#                                                      Defaults to None.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.nn.Module: Model with quantized weights (in terms of values, not actual dtype change).\n",
    "#     \"\"\"\n",
    "#     model_quantized = deepcopy(model)\n",
    "#     quantizer = WeightQuantizer(q_min=q_min, q_max=q_max, symmetric=symmetric, dtype=dtype)\n",
    "#     for name, module in model_quantized.named_modules():\n",
    "#         if modules_to_quantize is None:\n",
    "#             if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d, torch.nn.ConvTranspose2d)): # Common layers with weights\n",
    "#                 if hasattr(module, 'weight'):\n",
    "#                     module.weight.data = quantizer(module.weight.data) # Apply quantizer to weight data\n",
    "#         elif name in modules_to_quantize:\n",
    "#             if hasattr(module, 'weight'):\n",
    "#                 module.weight.data = quantizer(module.weight.data)\n",
    "\n",
    "#     return model_quantized\n",
    "\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "class WeightQuantizer:\n",
    "    def __init__(self, q_min=0, q_max=255, symmetric=False, dtype=torch.quint8):\n",
    "        \"\"\"\n",
    "        Initializes the WeightQuantizer.\n",
    "\n",
    "        Args:\n",
    "            q_min (int): Minimum quantization range value. Defaults to 0.\n",
    "            q_max (int): Maximum quantization range value. Defaults to 255.\n",
    "            symmetric (bool): Whether to use symmetric quantization. Defaults to False.\n",
    "            dtype (torch.dtype): Quantized data type, used to determine default zero_point for symmetric quantization.\n",
    "                                 Defaults to torch.quint8.\n",
    "        \"\"\"\n",
    "        self.q_min = q_min\n",
    "        self.q_max = q_max\n",
    "        self.symmetric = symmetric\n",
    "        self.dtype = dtype\n",
    "        self.x_min = None\n",
    "        self.x_max = None\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Applies weight quantization to the input tensor X and returns dequantized version.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input tensor to be quantized.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Dequantized tensor.\n",
    "        \"\"\"\n",
    "        if self.x_min is None:\n",
    "            self.x_min = torch.min(X)\n",
    "        else:\n",
    "            self.x_min = torch.min(self.x_min, torch.min(X))\n",
    "\n",
    "        if self.x_max is None:\n",
    "            self.x_max = torch.max(X)\n",
    "        else:\n",
    "            self.x_max = torch.max(self.x_max, torch.max(X))\n",
    "\n",
    "        if self.x_min == self.x_max:\n",
    "            scale = 1.0\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            if self.symmetric:\n",
    "                s = 2 * max(abs(self.x_min), self.x_max) / (self.q_max - self.q_min)\n",
    "                z = 0 if self.dtype == torch.qint8 else 128 # Default to 128 for quint8 and other unsigned types.\n",
    "\n",
    "            else:\n",
    "                s = (self.x_max - self.x_min) / (self.q_max - self.q_min)\n",
    "                z = self.q_min - round(self.x_min / s)\n",
    "\n",
    "            scale = s\n",
    "            zero_point = int(z) # Ensure zero_point is integer as per quantization definition\n",
    "\n",
    "        if scale == 0: # Handle potential division by zero if range is too small.\n",
    "            scale = 1.0\n",
    "\n",
    "        self.scale = scale # Store scale and zero_point for potential later use if needed\n",
    "        self.zero_point = zero_point\n",
    "\n",
    "        # Quantize\n",
    "        quantized_values = torch.round(torch.clamp((X / scale) + zero_point, self.q_min, self.q_max))\n",
    "\n",
    "        # Dequantize\n",
    "        dequantized_values = (quantized_values - zero_point) * scale\n",
    "        return quantized_values\n",
    "\n",
    "class ActivationQuantizer(WeightQuantizer): # Reusing WeightQuantizer logic for activations\n",
    "    pass # For now, activations use the same quantization logic as weights\n",
    "\n",
    "\n",
    "def apply_weight_quantization(model, q_min=0, q_max=255, symmetric=False, dtype=torch.quint8, modules_to_quantize=None, quantize_activations=None):\n",
    "    \"\"\"\n",
    "    Applies weight and activation quantization to specified modules in a PyTorch model\n",
    "    and returns a model with dequantized weights and activations (simulated).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): PyTorch model to quantize.\n",
    "        q_min (int): Minimum quantization range value. Defaults to 0.\n",
    "        q_max (int): Maximum quantization range value. Defaults to 255.\n",
    "        symmetric (bool): Whether to use symmetric quantization. Defaults to False.\n",
    "        dtype (torch.dtype): Quantized data type. Defaults to torch.quint8.\n",
    "        modules_to_quantize (list of str, optional): List of module names to quantize weights.\n",
    "                                                     If None, quantizes Linear, Conv2d, and ConvTranspose2d layers.\n",
    "                                                     Defaults to None.\n",
    "        quantize_activations (list of str or bool, optional): List of module names to quantize activations.\n",
    "                                                            If True, quantizes activations for the same modules as weights.\n",
    "                                                            Defaults to None (no activation quantization).\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Model with dequantized weights and activations (simulated).\n",
    "    \"\"\"\n",
    "    model_quantized = deepcopy(model)\n",
    "    weight_quantizer = WeightQuantizer(q_min=q_min, q_max=q_max, symmetric=symmetric, dtype=dtype)\n",
    "    activation_quantizer = ActivationQuantizer(q_min=q_min, q_max=q_max, symmetric=symmetric, dtype=dtype)\n",
    "\n",
    "    modules_weight_quant = modules_to_quantize if modules_to_quantize else []\n",
    "    modules_activation_quant = quantize_activations if isinstance(quantize_activations, list) else (modules_to_quantize if quantize_activations is True else [])\n",
    "\n",
    "\n",
    "    for name, module in model_quantized.named_modules():\n",
    "        if modules_weight_quant is None: # Default weight quantization\n",
    "            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d, torch.nn.ConvTranspose2d)):\n",
    "                if hasattr(module, 'weight'):\n",
    "                    module.weight.data = weight_quantizer(module.weight.data)\n",
    "        elif name in modules_weight_quant: # Selective weight quantization\n",
    "            if hasattr(module, 'weight'):\n",
    "                module.weight.data = weight_quantizer(module.weight.data)\n",
    "\n",
    "        if modules_activation_quant is not None and name in modules_activation_quant: # Selective activation quantization\n",
    "            def forward_hook(module, input, output):\n",
    "                return activation_quantizer(output)\n",
    "            module.register_forward_hook(forward_hook)\n",
    "\n",
    "    return model_quantized\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "modules_to_quantize = [\n",
    "    \"text_transformer.encoder.layer.2.output.dense\",\n",
    "    \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "]\n",
    "\n",
    "\n",
    "# 1. Create a dummy model\n",
    "original_weights_fc1 = model.text_transformer.encoder.layer[2].intermediate.dense.weight.data.clone()\n",
    "original_weights_fc2 = model.text_transformer.encoder.layer[2].output.dense.weight.data.clone()\n",
    "\n",
    "\n",
    "# # 2. Apply weight quantization to specified modules (symmetric, qint8)\n",
    "# quantized_model_selective = apply_weight_quantization(\n",
    "#     model, q_min=-8, q_max=7, symmetric=True, dtype=torch.qint8, modules_to_quantize=modules_to_quantize\n",
    "# )\n",
    "\n",
    "# 3. Example with activation quantization for same layers as weights (quantize_activations=True)\n",
    "quantized_model_selective = apply_weight_quantization(\n",
    "    model, q_min=-8, q_max=7, symmetric=True, dtype=torch.qint8,\n",
    "    modules_to_quantize=modules_to_quantize, quantize_activations=True # Quantize activations for same modules as weights\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Print quantized weights of the specified linear layers\n",
    "dequantized_weights_fc1 = quantized_model_selective.text_transformer.encoder.layer[2].intermediate.dense.weight.data.clone()\n",
    "dequantized_weights_fc2 = quantized_model_selective.text_transformer.encoder.layer[2].output.dense.weight.data.clone()\n",
    "\n",
    "# Print original weights of the fc2 linear layer\n",
    "print(\"Original weights of fc2:\\n\", original_weights_fc2)\n",
    "\n",
    "# Print quantized weights of the fc2 linear layer\n",
    "print(\"Dequantized weights of fc2:\\n\", dequantized_weights_fc2)\n",
    "\n",
    "# 4. Mean absolulte error between original and dequantized weights of the fc2 layer\n",
    "mae_fc2 = torch.mean(torch.abs(original_weights_fc2 - dequantized_weights_fc2))\n",
    "print(f\"Min and max of dequantized fc2 weights: {torch.min(dequantized_weights_fc2)}, {torch.max(dequantized_weights_fc2)}\")\n",
    "print(f\"Mean Absolute Error (MAE) between original and dequantized weights of fc2: {mae_fc2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.PlaceholderObserver'>, dtype=torch.quint8, quant_min=0, quant_max=15, is_dynamic=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, quant_min=-8, quant_max=7){})\n",
      "Quantized weights of fc2:\n",
      " tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1330],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "Min and max of dynamic quantized fc2 weights: -7, 7\n",
      "Mean Absolute Error (MAE) between original and quantized weights of linear1.weight: 0.15424813330173492\n"
     ]
    }
   ],
   "source": [
    "from quantization_utils import quantize_modules\n",
    "model_dynamic = quantize_modules(model, 4, modules_to_quantize)\n",
    "\n",
    "dynamic_quantized_fc1_weight = model_dynamic.text_transformer.encoder.layer[2].intermediate.dense.weight().dequantize().clone()\n",
    "dynamic_quantized_fc2_weight = model_dynamic.text_transformer.encoder.layer[2].output.dense.weight().dequantize().clone()\n",
    "dynamic_quantized_fc2_weight_int_repr = model_dynamic.text_transformer.encoder.layer[2].output.dense.weight().int_repr()\n",
    "\n",
    "print(f\"Quantized weights of fc2:\\n\", dynamic_quantized_fc2_weight)\n",
    "\n",
    "# Mean absolute error between original and quantized weights of the fc2 layer\n",
    "print(f\"Min and max of dynamic quantized fc2 weights: {torch.min(dynamic_quantized_fc2_weight_int_repr)}, {torch.max(dynamic_quantized_fc2_weight_int_repr)}\")\n",
    "print(f\"Mean Absolute Error (MAE) between original and quantized weights of linear1.weight: {torch.mean(torch.abs(original_weights_fc2 - dynamic_quantized_fc2_weight_int_repr))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE between custom quantization and dynamic quantization: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE between custom quantization and dynamic quantization: {torch.mean((dynamic_quantized_fc2_weight_int_repr - dequantized_weights_fc2)**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['nlvr2_vlue_test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['nlvr2_vlue_test']\n",
      "Loaded names: ['nlvr2_vlue_test']\n",
      "Testing DataLoader 0: 100%|██████████| 177/177 [05:56<00:00,  0.50it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "nlvr2/dev/accuracy_epoch            nan\n",
      "  nlvr2/dev/loss_epoch              nan\n",
      "   nlvr2/test/accuracy      0.7368420958518982\n",
      "nlvr2/test/accuracy_epoch   0.7368420958518982\n",
      "     nlvr2/test/loss        0.8043622374534607\n",
      "  nlvr2/test/loss_epoch     0.8043286204338074\n",
      "     val/the_metric         0.7368420958518982\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'nlvr2/test/loss': 0.8043622374534607,\n",
       "  'nlvr2/test/accuracy': 0.7368420958518982,\n",
       "  'nlvr2/dev/accuracy_epoch': nan,\n",
       "  'nlvr2/dev/loss_epoch': nan,\n",
       "  'nlvr2/test/accuracy_epoch': 0.7368420958518982,\n",
       "  'nlvr2/test/loss_epoch': 0.8043286204338074,\n",
       "  'val/the_metric': 0.7368420958518982}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(quantized_model_selective, full_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs difference: 0.000017563685105415\n"
     ]
    }
   ],
   "source": [
    "fc2_after_finetune = model.text_transformer.encoder.layer[2].output.dense.weight.clone()\n",
    "# Print the mean abs difference and std of the weights\n",
    "print(f\"Mean abs difference: {torch.mean(torch.abs(fc2_after_finetune - fc2_weight)):.18f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name                        | Type         | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0  | cross_modal_text_transform  | Linear       | 590 K  | train\n",
      "1  | cross_modal_image_transform | Linear       | 590 K  | train\n",
      "2  | token_type_embeddings       | Embedding    | 2.3 K  | train\n",
      "3  | vit_model                   | CLIP         | 78.9 M | train\n",
      "4  | text_transformer            | RobertaModel | 124 M  | eval \n",
      "5  | cross_modal_image_layers    | ModuleList   | 56.7 M | train\n",
      "6  | cross_modal_text_layers     | ModuleList   | 56.7 M | train\n",
      "7  | cross_modal_image_pooler    | Pooler       | 590 K  | train\n",
      "8  | cross_modal_text_pooler     | Pooler       | 590 K  | train\n",
      "9  | nlvr2_classifier            | Sequential   | 4.7 M  | train\n",
      "10 | train_nlvr2_accuracy        | Accuracy     | 0      | train\n",
      "11 | train_nlvr2_loss            | Scalar       | 0      | train\n",
      "12 | dev_nlvr2_accuracy          | Accuracy     | 0      | train\n",
      "13 | dev_nlvr2_loss              | Scalar       | 0      | train\n",
      "14 | test_nlvr2_accuracy         | Accuracy     | 0      | train\n",
      "15 | test_nlvr2_loss             | Scalar       | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "319 M     Non-trainable params\n",
      "324 M     Total params\n",
      "1,296.033 Total estimated model params size (MB)\n",
      "465       Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=8]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'val/the_metric' reached 0.82090 (best 0.82090), saving model to 'result/finetune_nlvr2_seed0_from_meter_nlvr2/version_8/checkpoints/epoch=0-step=1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valiation loss: 0.8208954930305481\n",
      "Epoch 1: 100%|██████████| 1/1 [00:08<00:00,  0.11it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2: 'val/the_metric' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valiation loss: 0.46268656849861145\n",
      "Epoch 2: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 3: 'val/the_metric' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current valiation loss: 0.5522388219833374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1/1 [00:24<00:00,  0.04it/s, v_num=8]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=fine_tune_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs difference: 0.011859540827572346\n"
     ]
    }
   ],
   "source": [
    "fc2_after_trainer_finetune = model.text_transformer.encoder.layer[2].output.dense.weight.clone()\n",
    "# Print the mean abs difference and std of the weights\n",
    "print(f\"Mean abs difference: {torch.mean(torch.abs(fc2_after_trainer_finetune - fc2_weight)):.18f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC2 weight before fine-tuning:  "
     ]
    }
   ],
   "source": [
    "print(\"FC2 weight before fine-tuning: \", fc2_weight)\n",
    "print(\"FC2 weight after fine-tuning: \", fc2_after_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision Model:\n",
      "Size (MB): 1296.258138\n",
      "The model is on: cpu\n",
      "Quantized Model (4 bits):\n",
      "Size (MB): 1267.950002\n",
      "The model is on: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Precision Model:\")\n",
    "print_size_of_model(model)\n",
    "print(f\"The model is on: {model.device}\")\n",
    "\n",
    "# Quantize the model\n",
    "bits = 4\n",
    "modules_to_quantize = [\n",
    "    \"text_transformer.encoder.layer.2.output.dense\",\n",
    "    \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.3.output.dense\",\n",
    "    \"text_transformer.encoder.layer.3.intermediate.dense\"\n",
    "]\n",
    "\n",
    "model_dynamic = quantize_modules(model, bits, modules_to_quantize)\n",
    "\n",
    "print(f\"Quantized Model ({bits} bits):\")\n",
    "print_size_of_model(model_dynamic)\n",
    "print(f\"The model is on: {model_dynamic.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['nlvr2_train']\n",
      "Loaded names: ['nlvr2_dev', 'nlvr2_test1']\n",
      "Loaded names: ['nlvr2_dev', 'nlvr2_test1']\n",
      "Testing DataLoader 0:   5%|▍         | 27/582 [00:33<11:18,  0.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "error while read file idx 1033 in nlvr2_dev\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:788\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    785\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    787\u001b[0m )\n\u001b[0;32m--> 788\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:424\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:301\u001b[0m, in \u001b[0;36mMETERTransformerSS.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    300\u001b[0m meter_utils\u001b[38;5;241m.\u001b[39mset_task(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:267\u001b[0m, in \u001b[0;36mMETERTransformerSS.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlvr2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_tasks:\n\u001b[0;32m--> 267\u001b[0m     ret\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mobjectives\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_nlvr2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# SNLI Visual Entailment\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/meter/modules/objectives.py:184\u001b[0m, in \u001b[0;36mcompute_nlvr2\u001b[0;34m(pl_module, batch)\u001b[0m\n\u001b[1;32m    181\u001b[0m infer1 \u001b[38;5;241m=\u001b[39m pl_module\u001b[38;5;241m.\u001b[39minfer(\n\u001b[1;32m    182\u001b[0m     batch, mask_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mask_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, image_token_type_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    183\u001b[0m )\n\u001b[0;32m--> 184\u001b[0m infer2 \u001b[38;5;241m=\u001b[39m \u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_token_type_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m cls_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([infer1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m], infer2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:223\u001b[0m, in \u001b[0;36mMETERTransformerSS.infer\u001b[0;34m(self, batch, mask_text, mask_image, image_token_type_idx, img)\u001b[0m\n\u001b[1;32m    222\u001b[0m x1 \u001b[38;5;241m=\u001b[39m text_layer(x, y, extend_text_masks, extend_image_masks)\n\u001b[0;32m--> 223\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[43mimage_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextend_image_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextend_text_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x1[\u001b[38;5;241m0\u001b[39m], y1[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/meter/modules/bert_model.py:484\u001b[0m, in \u001b[0;36mBertCrossLayer.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrossattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/meter/modules/bert_model.py:406\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    398\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    405\u001b[0m ):\n\u001b[0;32m--> 406\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/meter/modules/bert_model.py:329\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m(attention_scores)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m#if True:\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/activation.py:1657\u001b[0m, in \u001b[0;36mSoftmax.__init__\u001b[0;34m(self, dim)\u001b[0m\n\u001b[1;32m   1655\u001b[0m dim: Optional[\u001b[38;5;28mint\u001b[39m]\n\u001b[0;32m-> 1657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1658\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_dm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:748\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.test(model, full_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frozen_layers(model):\n",
    "    \"\"\"\n",
    "    Prints the names of the layers in a PyTorch model and whether they are frozen or not.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to print the frozen status of.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name}, Frozen: {not param.requires_grad}\")\n",
    "\n",
    "\n",
    "def freeze_except_layers(model, layers_to_unfreeze_names):\n",
    "    \"\"\"\n",
    "    Freezes all parameters of a PyTorch model except for the layers specified by their names.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to freeze parameters in.\n",
    "        layers_to_unfreeze_names (list of str): A list of module names that should NOT be frozen.\n",
    "                                             Parameters in modules whose names contain these strings will be unfrozen.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        freeze = True  # Initially assume we should freeze the parameter\n",
    "        for layer_name_to_unfreeze in layers_to_unfreeze_names:\n",
    "            if layer_name_to_unfreeze in name:\n",
    "                freeze = False  # Unfreeze if the name contains a layer to unfreeze\n",
    "                break  # No need to check other layer names if already unfrozen\n",
    "\n",
    "        if freeze:\n",
    "            param.requires_grad = False  # Freeze the parameter\n",
    "        else:\n",
    "            param.requires_grad = True   # Ensure it's unfrozen (explicitly set to True)\n",
    "\n",
    "    # Optional: Print which layers are frozen and unfrozen for verification\n",
    "    print_frozen_layers(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.output.dense.bias, Frozen: False\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization.qconfig import default_dynamic_qat_qconfig\n",
    "from torch.ao.quantization.quantize import convert, propagate_qconfig_, prepare\n",
    "from torch.ao.quantization.quantization_mappings import get_default_qat_module_mappings\n",
    "import copy\n",
    "\n",
    "def prepare_qat(model, qconfig_dict, mapping=None, inplace=False):\n",
    "    r\"\"\"\n",
    "    Prepares a copy of the model for quantization calibration or\n",
    "    quantization-aware training and converts it to quantized version.\n",
    "\n",
    "    Quantization configuration should be assigned preemptively\n",
    "    to individual submodules in `.qconfig` attribute.\n",
    "\n",
    "    Args:\n",
    "        model: input model to be modified in-place\n",
    "        mapping: dictionary that maps float modules to quantized modules to be\n",
    "                 replaced.\n",
    "        inplace: carry out model transformations in-place, the original module\n",
    "                 is mutated\n",
    "    \"\"\"\n",
    "    torch._C._log_api_usage_once(\"quantization_api.quantize.prepare_qat\")\n",
    "    assert model.training, \"prepare_qat only works on models in training mode\"\n",
    "    if mapping is None:\n",
    "        mapping = get_default_qat_module_mappings()\n",
    "\n",
    "    if not inplace:\n",
    "        model = copy.deepcopy(model)\n",
    "\n",
    "    propagate_qconfig_(model, qconfig_dict=qconfig_dict)\n",
    "    convert(model, mapping=mapping, inplace=True, remove_qconfig=False)\n",
    "    prepare(model, observer_non_leaf_module_list=set(mapping.values()), inplace=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the quantization configuration dictionary\n",
    "qconfig_dict = dict()\n",
    "for layer in modules_to_quantize:\n",
    "    qconfig_dict[layer] = default_dynamic_qat_qconfig\n",
    "\n",
    "# Prepare the model for quantization-aware training\n",
    "model_qat = prepare_qat(model, inplace=False, qconfig_dict=qconfig_dict)\n",
    "\n",
    "# Freeze all layers except for the quantized layers\n",
    "freeze_except_layers(model_qat, modules_to_quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_transformer.encoder.layer.2.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.2.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.2.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.2.output.dense.bias 768\n",
      "text_transformer.encoder.layer.3.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.3.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.3.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.3.output.dense.bias 768\n",
      "Total trainable parameters: 9444864\n"
     ]
    }
   ],
   "source": [
    "# Count the trainable parameters\n",
    "sum_param = 0\n",
    "for name, param in model_qat.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())\n",
    "        sum_param += param.numel()\n",
    "    \n",
    "print(f\"Total trainable parameters: {sum_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name                        | Type         | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0  | cross_modal_text_transform  | Linear       | 590 K  | train\n",
      "1  | cross_modal_image_transform | Linear       | 590 K  | train\n",
      "2  | token_type_embeddings       | Embedding    | 2.3 K  | train\n",
      "3  | vit_model                   | CLIP         | 78.9 M | train\n",
      "4  | text_transformer            | RobertaModel | 124 M  | eval \n",
      "5  | cross_modal_image_layers    | ModuleList   | 56.7 M | train\n",
      "6  | cross_modal_text_layers     | ModuleList   | 56.7 M | train\n",
      "7  | cross_modal_image_pooler    | Pooler       | 590 K  | train\n",
      "8  | cross_modal_text_pooler     | Pooler       | 590 K  | train\n",
      "9  | nlvr2_classifier            | Sequential   | 4.7 M  | train\n",
      "10 | train_nlvr2_accuracy        | Accuracy     | 0      | train\n",
      "11 | train_nlvr2_loss            | Scalar       | 0      | train\n",
      "12 | dev_nlvr2_accuracy          | Accuracy     | 0      | train\n",
      "13 | dev_nlvr2_loss              | Scalar       | 0      | train\n",
      "14 | test_nlvr2_accuracy         | Accuracy     | 0      | train\n",
      "15 | test_nlvr2_loss             | Scalar       | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "9.4 M     Trainable params\n",
      "314 M     Non-trainable params\n",
      "324 M     Total params\n",
      "1,296.033 Total estimated model params size (MB)\n",
      "485       Modules in train mode\n",
      "224       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/70 [00:00<?, ?it/s] Executing training_step for task ['nlvr2']\n",
      "Output is obtained: dict_keys(['nlvr2_loss', 'nlvr2_logits', 'nlvr2_labels'])\n",
      "Total loss is 0.0025637000799179077\n",
      "Epoch 0:   1%|▏         | 1/70 [00:00<00:55,  1.25it/s, v_num=18]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'val/the_metric' was not in top 1\n",
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 1/70 [00:06<07:24,  0.16it/s, v_num=18]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model_qat, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_modal_text_transform.weight 589824\n",
      "cross_modal_text_transform.bias 768\n",
      "cross_modal_image_transform.weight 589824\n",
      "cross_modal_image_transform.bias 768\n",
      "token_type_embeddings.weight 2304\n",
      "vit_model.positional_embedding 39424\n",
      "vit_model.visual.class_embedding 768\n",
      "vit_model.visual.positional_embedding 249600\n",
      "vit_model.visual.conv1.weight 589824\n",
      "vit_model.visual.ln_pre.weight 768\n",
      "vit_model.visual.ln_pre.bias 768\n",
      "vit_model.visual.transformer.resblocks.0.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.0.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.0.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.0.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.0.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.0.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.0.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.0.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.0.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.0.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.0.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.0.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.1.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.1.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.1.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.1.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.1.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.1.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.1.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.1.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.1.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.1.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.1.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.1.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.2.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.2.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.2.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.2.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.2.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.2.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.2.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.2.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.2.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.2.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.2.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.2.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.3.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.3.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.3.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.3.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.3.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.3.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.3.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.3.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.3.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.3.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.3.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.3.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.4.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.4.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.4.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.4.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.4.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.4.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.4.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.4.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.4.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.4.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.4.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.4.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.5.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.5.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.5.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.5.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.5.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.5.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.5.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.5.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.5.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.5.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.5.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.5.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.6.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.6.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.6.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.6.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.6.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.6.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.6.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.6.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.6.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.6.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.6.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.6.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.7.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.7.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.7.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.7.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.7.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.7.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.7.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.7.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.7.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.7.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.7.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.7.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.8.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.8.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.8.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.8.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.8.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.8.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.8.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.8.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.8.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.8.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.8.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.8.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.9.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.9.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.9.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.9.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.9.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.9.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.9.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.9.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.9.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.9.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.9.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.9.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.10.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.10.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.10.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.10.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.10.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.10.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.10.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.10.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.10.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.10.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.10.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.10.ln_2.bias 768\n",
      "vit_model.visual.ln_post.weight 768\n",
      "vit_model.visual.ln_post.bias 768\n",
      "vit_model.ln_final.weight 512\n",
      "vit_model.ln_final.bias 512\n",
      "text_transformer.embeddings.word_embeddings.weight 38603520\n",
      "text_transformer.embeddings.position_embeddings.weight 394752\n",
      "text_transformer.embeddings.token_type_embeddings.weight 768\n",
      "text_transformer.embeddings.LayerNorm.weight 768\n",
      "text_transformer.embeddings.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.0.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.0.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.0.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.0.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.0.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.0.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.0.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.0.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.0.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.0.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.0.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.0.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.0.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.0.output.dense.bias 768\n",
      "text_transformer.encoder.layer.0.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.0.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.1.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.1.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.1.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.1.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.1.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.1.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.1.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.1.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.1.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.1.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.1.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.1.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.1.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.1.output.dense.bias 768\n",
      "text_transformer.encoder.layer.1.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.1.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.2.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.2.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.2.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.2.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.2.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.2.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.2.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.2.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.2.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.2.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.2.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.2.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.2.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.2.output.dense.bias 768\n",
      "text_transformer.encoder.layer.2.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.2.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.3.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.3.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.3.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.3.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.3.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.3.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.3.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.3.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.3.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.3.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.3.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.3.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.3.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.3.output.dense.bias 768\n",
      "text_transformer.encoder.layer.3.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.3.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.4.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.4.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.4.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.4.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.4.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.4.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.4.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.4.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.4.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.4.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.4.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.4.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.4.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.4.output.dense.bias 768\n",
      "text_transformer.encoder.layer.4.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.4.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.5.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.5.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.5.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.5.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.5.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.5.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.5.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.5.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.5.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.5.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.5.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.5.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.5.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.5.output.dense.bias 768\n",
      "text_transformer.encoder.layer.5.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.5.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.6.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.6.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.6.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.6.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.6.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.6.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.6.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.6.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.6.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.6.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.6.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.6.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.6.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.6.output.dense.bias 768\n",
      "text_transformer.encoder.layer.6.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.6.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.7.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.7.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.7.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.7.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.7.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.7.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.7.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.7.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.7.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.7.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.7.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.7.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.7.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.7.output.dense.bias 768\n",
      "text_transformer.encoder.layer.7.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.7.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.8.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.8.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.8.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.8.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.8.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.8.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.8.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.8.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.8.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.8.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.8.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.8.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.8.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.8.output.dense.bias 768\n",
      "text_transformer.encoder.layer.8.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.8.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.9.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.9.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.9.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.9.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.9.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.9.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.9.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.9.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.9.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.9.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.9.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.9.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.9.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.9.output.dense.bias 768\n",
      "text_transformer.encoder.layer.9.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.9.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.10.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.10.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.10.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.10.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.10.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.10.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.10.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.10.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.10.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.10.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.10.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.10.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.10.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.10.output.dense.bias 768\n",
      "text_transformer.encoder.layer.10.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.10.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.11.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.11.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.11.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.11.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.11.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.11.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.11.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.11.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.11.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.11.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.11.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.11.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.11.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.11.output.dense.bias 768\n",
      "text_transformer.encoder.layer.11.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.11.output.LayerNorm.bias 768\n",
      "text_transformer.pooler.dense.weight 589824\n",
      "text_transformer.pooler.dense.bias 768\n",
      "cross_modal_image_layers.0.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.0.attention.self.query.bias 768\n",
      "cross_modal_image_layers.0.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.0.attention.self.key.bias 768\n",
      "cross_modal_image_layers.0.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.0.attention.self.value.bias 768\n",
      "cross_modal_image_layers.0.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.0.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.0.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.0.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.0.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.0.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.0.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.0.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.0.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.0.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.0.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.0.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.0.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.0.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.0.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.0.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.0.output.dense.weight 2359296\n",
      "cross_modal_image_layers.0.output.dense.bias 768\n",
      "cross_modal_image_layers.0.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.0.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.1.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.1.attention.self.query.bias 768\n",
      "cross_modal_image_layers.1.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.1.attention.self.key.bias 768\n",
      "cross_modal_image_layers.1.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.1.attention.self.value.bias 768\n",
      "cross_modal_image_layers.1.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.1.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.1.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.1.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.1.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.1.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.1.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.1.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.1.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.1.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.1.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.1.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.1.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.1.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.1.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.1.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.1.output.dense.weight 2359296\n",
      "cross_modal_image_layers.1.output.dense.bias 768\n",
      "cross_modal_image_layers.1.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.1.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.2.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.2.attention.self.query.bias 768\n",
      "cross_modal_image_layers.2.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.2.attention.self.key.bias 768\n",
      "cross_modal_image_layers.2.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.2.attention.self.value.bias 768\n",
      "cross_modal_image_layers.2.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.2.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.2.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.2.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.2.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.2.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.2.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.2.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.2.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.2.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.2.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.2.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.2.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.2.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.2.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.2.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.2.output.dense.weight 2359296\n",
      "cross_modal_image_layers.2.output.dense.bias 768\n",
      "cross_modal_image_layers.2.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.2.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.3.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.3.attention.self.query.bias 768\n",
      "cross_modal_image_layers.3.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.3.attention.self.key.bias 768\n",
      "cross_modal_image_layers.3.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.3.attention.self.value.bias 768\n",
      "cross_modal_image_layers.3.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.3.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.3.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.3.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.3.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.3.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.3.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.3.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.3.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.3.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.3.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.3.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.3.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.3.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.3.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.3.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.3.output.dense.weight 2359296\n",
      "cross_modal_image_layers.3.output.dense.bias 768\n",
      "cross_modal_image_layers.3.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.3.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.4.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.4.attention.self.query.bias 768\n",
      "cross_modal_image_layers.4.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.4.attention.self.key.bias 768\n",
      "cross_modal_image_layers.4.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.4.attention.self.value.bias 768\n",
      "cross_modal_image_layers.4.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.4.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.4.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.4.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.4.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.4.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.4.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.4.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.4.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.4.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.4.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.4.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.4.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.4.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.4.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.4.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.4.output.dense.weight 2359296\n",
      "cross_modal_image_layers.4.output.dense.bias 768\n",
      "cross_modal_image_layers.4.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.4.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.5.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.5.attention.self.query.bias 768\n",
      "cross_modal_image_layers.5.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.5.attention.self.key.bias 768\n",
      "cross_modal_image_layers.5.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.5.attention.self.value.bias 768\n",
      "cross_modal_image_layers.5.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.5.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.5.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.5.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.5.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.5.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.5.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.5.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.5.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.5.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.5.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.5.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.5.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.5.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.5.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.5.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.5.output.dense.weight 2359296\n",
      "cross_modal_image_layers.5.output.dense.bias 768\n",
      "cross_modal_image_layers.5.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.5.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.0.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.0.attention.self.query.bias 768\n",
      "cross_modal_text_layers.0.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.0.attention.self.key.bias 768\n",
      "cross_modal_text_layers.0.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.0.attention.self.value.bias 768\n",
      "cross_modal_text_layers.0.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.0.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.0.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.0.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.0.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.0.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.0.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.0.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.0.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.0.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.0.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.0.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.0.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.0.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.0.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.0.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.0.output.dense.weight 2359296\n",
      "cross_modal_text_layers.0.output.dense.bias 768\n",
      "cross_modal_text_layers.0.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.0.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.1.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.1.attention.self.query.bias 768\n",
      "cross_modal_text_layers.1.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.1.attention.self.key.bias 768\n",
      "cross_modal_text_layers.1.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.1.attention.self.value.bias 768\n",
      "cross_modal_text_layers.1.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.1.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.1.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.1.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.1.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.1.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.1.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.1.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.1.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.1.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.1.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.1.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.1.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.1.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.1.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.1.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.1.output.dense.weight 2359296\n",
      "cross_modal_text_layers.1.output.dense.bias 768\n",
      "cross_modal_text_layers.1.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.1.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.2.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.2.attention.self.query.bias 768\n",
      "cross_modal_text_layers.2.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.2.attention.self.key.bias 768\n",
      "cross_modal_text_layers.2.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.2.attention.self.value.bias 768\n",
      "cross_modal_text_layers.2.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.2.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.2.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.2.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.2.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.2.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.2.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.2.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.2.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.2.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.2.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.2.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.2.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.2.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.2.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.2.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.2.output.dense.weight 2359296\n",
      "cross_modal_text_layers.2.output.dense.bias 768\n",
      "cross_modal_text_layers.2.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.2.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.3.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.3.attention.self.query.bias 768\n",
      "cross_modal_text_layers.3.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.3.attention.self.key.bias 768\n",
      "cross_modal_text_layers.3.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.3.attention.self.value.bias 768\n",
      "cross_modal_text_layers.3.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.3.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.3.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.3.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.3.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.3.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.3.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.3.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.3.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.3.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.3.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.3.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.3.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.3.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.3.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.3.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.3.output.dense.weight 2359296\n",
      "cross_modal_text_layers.3.output.dense.bias 768\n",
      "cross_modal_text_layers.3.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.3.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.4.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.4.attention.self.query.bias 768\n",
      "cross_modal_text_layers.4.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.4.attention.self.key.bias 768\n",
      "cross_modal_text_layers.4.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.4.attention.self.value.bias 768\n",
      "cross_modal_text_layers.4.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.4.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.4.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.4.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.4.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.4.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.4.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.4.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.4.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.4.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.4.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.4.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.4.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.4.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.4.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.4.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.4.output.dense.weight 2359296\n",
      "cross_modal_text_layers.4.output.dense.bias 768\n",
      "cross_modal_text_layers.4.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.4.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.5.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.5.attention.self.query.bias 768\n",
      "cross_modal_text_layers.5.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.5.attention.self.key.bias 768\n",
      "cross_modal_text_layers.5.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.5.attention.self.value.bias 768\n",
      "cross_modal_text_layers.5.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.5.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.5.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.5.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.5.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.5.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.5.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.5.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.5.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.5.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.5.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.5.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.5.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.5.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.5.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.5.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.5.output.dense.weight 2359296\n",
      "cross_modal_text_layers.5.output.dense.bias 768\n",
      "cross_modal_text_layers.5.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.5.output.LayerNorm.bias 768\n",
      "cross_modal_image_pooler.dense.weight 589824\n",
      "cross_modal_image_pooler.dense.bias 768\n",
      "cross_modal_text_pooler.dense.weight 589824\n",
      "cross_modal_text_pooler.dense.bias 768\n",
      "nlvr2_classifier.0.weight 4718592\n",
      "nlvr2_classifier.0.bias 1536\n",
      "nlvr2_classifier.1.weight 1536\n",
      "nlvr2_classifier.1.bias 1536\n",
      "nlvr2_classifier.3.weight 3072\n",
      "nlvr2_classifier.3.bias 2\n",
      "Total trainable parameters: 324008194\n"
     ]
    }
   ],
   "source": [
    "# Count the trainable parameters\n",
    "sum_param = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())\n",
    "        sum_param += param.numel()\n",
    "    \n",
    "print(f\"Total trainable parameters: {sum_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                        | Type         | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0  | cross_modal_text_transform  | Linear       | 590 K  | train\n",
      "1  | cross_modal_image_transform | Linear       | 590 K  | train\n",
      "2  | token_type_embeddings       | Embedding    | 2.3 K  | train\n",
      "3  | vit_model                   | CLIP         | 78.9 M | train\n",
      "4  | text_transformer            | RobertaModel | 124 M  | eval \n",
      "5  | cross_modal_image_layers    | ModuleList   | 56.7 M | train\n",
      "6  | cross_modal_text_layers     | ModuleList   | 56.7 M | train\n",
      "7  | cross_modal_image_pooler    | Pooler       | 590 K  | train\n",
      "8  | cross_modal_text_pooler     | Pooler       | 590 K  | train\n",
      "9  | nlvr2_classifier            | Sequential   | 4.7 M  | train\n",
      "10 | train_nlvr2_accuracy        | Accuracy     | 0      | train\n",
      "11 | train_nlvr2_loss            | Scalar       | 0      | train\n",
      "12 | dev_nlvr2_accuracy          | Accuracy     | 0      | train\n",
      "13 | dev_nlvr2_loss              | Scalar       | 0      | train\n",
      "14 | test_nlvr2_accuracy         | Accuracy     | 0      | train\n",
      "15 | test_nlvr2_loss             | Scalar       | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "324 M     Trainable params\n",
      "0         Non-trainable params\n",
      "324 M     Total params\n",
      "1,296.033 Total estimated model params size (MB)\n",
      "465       Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/70 [00:00<?, ?it/s] Executing training_step for task ['nlvr2']\n",
      "Output is obtained: dict_keys(['nlvr2_loss', 'nlvr2_logits', 'nlvr2_labels'])\n",
      "Total loss is 0.00016162937390618026\n",
      "Epoch 0:   1%|▏         | 1/70 [00:00<01:07,  1.02it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'val/the_metric' was not in top 1\n",
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 1/70 [00:17<19:58,  0.06it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer = init_trainer(_config)\n",
    "trainer.fit(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert(model_qat, inplace=True)\n",
    "\n",
    "# # Save the quantized model ckpt\n",
    "# torch.save(model_qat.state_dict(), \"model_qat.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 70/70 [00:08<00:00,  7.82it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   nlvr2/dev/accuracy       0.7943925261497498\n",
      "nlvr2/dev/accuracy_epoch    0.8194444179534912\n",
      "     nlvr2/dev/loss         0.5969018340110779\n",
      "  nlvr2/dev/loss_epoch      0.5920029282569885\n",
      "   nlvr2/test/accuracy       0.843137264251709\n",
      "nlvr2/test/accuracy_epoch   0.8208954930305481\n",
      "     nlvr2/test/loss        0.4254521429538727\n",
      "  nlvr2/test/loss_epoch     0.4254521429538727\n",
      "     val/the_metric         0.8208954930305481\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'nlvr2/dev/loss': 0.5969018340110779,\n",
       "  'nlvr2/dev/accuracy': 0.7943925261497498,\n",
       "  'nlvr2/test/loss': 0.4254521429538727,\n",
       "  'nlvr2/test/accuracy': 0.843137264251709,\n",
       "  'nlvr2/dev/accuracy_epoch': 0.8194444179534912,\n",
       "  'nlvr2/dev/loss_epoch': 0.5920029282569885,\n",
       "  'nlvr2/test/accuracy_epoch': 0.8208954930305481,\n",
       "  'nlvr2/test/loss_epoch': 0.4254521429538727,\n",
       "  'val/the_metric': 0.8208954930305481}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = init_trainer(_config)\n",
    "trainer.test(model_qat, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 70/70 [01:07<00:00,  1.04it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   nlvr2/dev/accuracy       0.6074766516685486\n",
      "nlvr2/dev/accuracy_epoch           0.625\n",
      "     nlvr2/dev/loss         0.9538049697875977\n",
      "  nlvr2/dev/loss_epoch      0.9449809789657593\n",
      "   nlvr2/test/accuracy      0.6764705777168274\n",
      "nlvr2/test/accuracy_epoch   0.6567164063453674\n",
      "     nlvr2/test/loss        0.8373410105705261\n",
      "  nlvr2/test/loss_epoch     0.8373410105705261\n",
      "     val/the_metric         0.6567164063453674\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'nlvr2/dev/loss': 0.9538049697875977,\n",
       "  'nlvr2/dev/accuracy': 0.6074766516685486,\n",
       "  'nlvr2/test/loss': 0.8373410105705261,\n",
       "  'nlvr2/test/accuracy': 0.6764705777168274,\n",
       "  'nlvr2/dev/accuracy_epoch': 0.625,\n",
       "  'nlvr2/dev/loss_epoch': 0.9449809789657593,\n",
       "  'nlvr2/test/accuracy_epoch': 0.6567164063453674,\n",
       "  'nlvr2/test/loss_epoch': 0.8373410105705261,\n",
       "  'val/the_metric': 0.6567164063453674}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = init_trainer(_config, accelerator=\"cpu\")\n",
    "trainer.test(model_dynamic, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.output.dense.bias, Frozen: False\n"
     ]
    }
   ],
   "source": [
    "print_frozen_layers(model_qat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization | PTQ to Different Bit Precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision Model:\n",
      "Size (MB): 455.903666\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "bit8_linear, bit8_embedding = get_quantization_config(8)\n",
    "bit4_linear, bit4_embedding = get_quantization_config(4)\n",
    "bit2_linear, bit2_embedding = get_quantization_config(2)\n",
    "bit1_linear, bit1_embedding = get_quantization_config(1)\n",
    "\n",
    "\n",
    "print(\"Full Precision Model:\")\n",
    "print_size_of_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mixed = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViLTransformerSS(\n",
       "  (text_embeddings): BertEmbeddings(\n",
       "    (word_embeddings): QuantizedEmbedding(num_embeddings=30522, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
       "    (position_embeddings): QuantizedEmbedding(num_embeddings=40, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
       "    (token_type_embeddings): QuantizedEmbedding(num_embeddings=2, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (token_type_embeddings): QuantizedEmbedding(num_embeddings=3, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
       "  (transformer): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      (quant): QuantStub()\n",
       "      (dequant): DeQuantStub()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): DynamicQuantizedLinear(in_features=768, out_features=2304, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          (quant): QuantStub()\n",
       "          (dequant): DeQuantStub()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (quant): QuantStub()\n",
       "        (dequant): DeQuantStub()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): Pooler(\n",
       "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (nlvr2_classifier): Sequential(\n",
       "    (0): DynamicQuantizedLinear(in_features=1536, out_features=1536, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): DynamicQuantizedLinear(in_features=1536, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       "  (train_nlvr2_accuracy): Accuracy()\n",
       "  (train_nlvr2_loss): Scalar()\n",
       "  (dev_nlvr2_accuracy): Accuracy()\n",
       "  (dev_nlvr2_loss): Scalar()\n",
       "  (test_nlvr2_accuracy): Accuracy()\n",
       "  (test_nlvr2_loss): Scalar()\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantization.quantize_dynamic(\n",
    "    model_mixed, {torch.nn.Embedding: bit2_embedding, \"nlvr2_classifier\": bit2_linear, \"pooler\": bit4_linear, \"transformer\": bit8_linear}, dtype=torch.quint8, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Quantization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision Model:\n",
      "Size of the model (MB): 455.900978\n",
      "Fully Quantized Model:\n",
      "Size of the model (MB): 122.099212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_8 = copy.deepcopy(model)\n",
    "model_4 = copy.deepcopy(model)\n",
    "model_2 = copy.deepcopy(model)\n",
    "model_1 = copy.deepcopy(model)\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_8, {torch.nn.Embedding: bit8_embedding}, dtype=torch.quint8, inplace=True\n",
    ")\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_8, {torch.nn.Linear: bit8_linear, torch.nn.LayerNorm: bit8_linear}, dtype=torch.qint8, inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_4, {torch.nn.Embedding: bit4_embedding}, dtype=torch.quint8, inplace=True\n",
    ")\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_4, {torch.nn.Linear: bit4_linear, torch.nn.LayerNorm: bit2_linear}, dtype=torch.qint8, inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_2, {torch.nn.Embedding: bit2_embedding}, dtype=torch.quint8, inplace=True\n",
    ")\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_2, {torch.nn.Linear: bit2_linear, torch.nn.LayerNorm: bit2_linear}, dtype=torch.qint8, inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_1, {torch.nn.Embedding: bit1_embedding}, dtype=torch.quint8, inplace=True\n",
    ")\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_1, {torch.nn.Linear: bit1_linear, torch.nn.LayerNorm: bit1_linear}, dtype=torch.qint8, inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Fully Quantized Model:\")\n",
    "print_size_of_model(model_8)\n",
    "\n",
    "# print(f\"Quantized Model with only the {layer_to_quantize} layer:\")\n",
    "# print_size_of_model(_model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the block from the model\n",
    "def get_block(model, block_selection):\n",
    "    attrs = block_selection.split('.')\n",
    "    block = model\n",
    "    for attr in attrs:\n",
    "        if '[' in attr and ']' in attr:\n",
    "            attr_name, index = attr[:-1].split('[')\n",
    "            block = getattr(block, attr_name)[int(index)]\n",
    "        else:\n",
    "            block = getattr(block, attr)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit Relative Change (Mean): 0.1676570475101471\n",
      "4-bit Relative Change (Mean): 0.925999104976654\n",
      "2-bit Relative Change (Mean): 0.9999205470085144\n",
      "8-bit Relative Change (Max): 1.0\n",
      "4-bit Relative Change (Max): 1.0\n",
      "2-bit Relative Change (Max): 1.0\n",
      "8-bit Relative Change Percentage (Mean): 16.765705108642578\n",
      "4-bit Relative Change Percentage (Mean): 92.59990692138672\n",
      "2-bit Relative Change Percentage (Mean): 99.9920425415039\n",
      "Top 8-bit weights with largest relative changes: tensor([ 78,  44,  72,  38,  15, 132, 155, 172, 144,  64])\n",
      "Top 4-bit weights with largest relative changes: tensor([ 8,  9,  4,  3,  5,  1,  0, 10,  2,  6])\n",
      "Top 2-bit weights with largest relative changes: tensor([8, 9, 4, 7, 5, 3, 1, 0, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Define the block selection\n",
    "block_selection = 'transformer.blocks[0].mlp.fc1'\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the blocks from each model\n",
    "    block_full_precision = get_block(model, block_selection)\n",
    "    block_8bit = get_block(model_8, block_selection)\n",
    "    block_4bit = get_block(model_4, block_selection)\n",
    "    block_2bit = get_block(model_2, block_selection)\n",
    "\n",
    "    # Dequantize the quantized weights before performing the operation\n",
    "    weight_full_precision = block_full_precision.weight\n",
    "    weight_8bit = block_8bit.weight().dequantize()  # Dequantize 8-bit weights\n",
    "    weight_4bit = block_4bit.weight().dequantize()  # Dequantize 4-bit weights\n",
    "    weight_2bit = block_2bit.weight().dequantize()  # Dequantize 2-bit weights\n",
    "\n",
    "    # Compute relative changes\n",
    "    relative_change_8bit = torch.abs(weight_full_precision - weight_8bit) / torch.abs(weight_full_precision)\n",
    "    relative_change_4bit = torch.abs(weight_full_precision - weight_4bit) / torch.abs(weight_full_precision)\n",
    "    relative_change_2bit = torch.abs(weight_full_precision - weight_2bit) / torch.abs(weight_full_precision)\n",
    "\n",
    "    # Handle division by zero (if any original weight is zero)\n",
    "    relative_change_8bit[torch.isnan(relative_change_8bit)] = 0  # Set NaN to 0\n",
    "    relative_change_4bit[torch.isnan(relative_change_4bit)] = 0  # Set NaN to 0\n",
    "    relative_change_2bit[torch.isnan(relative_change_2bit)] = 0  # Set NaN to 0\n",
    "\n",
    "    # Convert to percentage (optional)\n",
    "    relative_change_8bit_percent = relative_change_8bit * 100\n",
    "    relative_change_4bit_percent = relative_change_4bit * 100\n",
    "    relative_change_2bit_percent = relative_change_2bit * 100\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"8-bit Relative Change (Mean):\", torch.mean(relative_change_8bit).item())\n",
    "    print(\"4-bit Relative Change (Mean):\", torch.mean(relative_change_4bit).item())\n",
    "    print(\"2-bit Relative Change (Mean):\", torch.mean(relative_change_2bit).item())\n",
    "\n",
    "    print(\"8-bit Relative Change (Max):\", torch.max(relative_change_8bit).item())\n",
    "    print(\"4-bit Relative Change (Max):\", torch.max(relative_change_4bit).item())\n",
    "    print(\"2-bit Relative Change (Max):\", torch.max(relative_change_2bit).item())\n",
    "\n",
    "    print(\"8-bit Relative Change Percentage (Mean):\", torch.mean(relative_change_8bit_percent).item())\n",
    "    print(\"4-bit Relative Change Percentage (Mean):\", torch.mean(relative_change_4bit_percent).item())\n",
    "    print(\"2-bit Relative Change Percentage (Mean):\", torch.mean(relative_change_2bit_percent).item())\n",
    "\n",
    "    # Identify weights with the largest relative changes\n",
    "    top_k = 10  # Number of top weights to identify\n",
    "    top_8bit_indices = torch.topk(relative_change_8bit.flatten(), k=top_k).indices\n",
    "    top_4bit_indices = torch.topk(relative_change_4bit.flatten(), k=top_k).indices\n",
    "    top_2bit_indices = torch.topk(relative_change_2bit.flatten(), k=top_k).indices\n",
    "\n",
    "    print(\"Top 8-bit weights with largest relative changes:\", top_8bit_indices)\n",
    "    print(\"Top 4-bit weights with largest relative changes:\", top_4bit_indices)\n",
    "    print(\"Top 2-bit weights with largest relative changes:\", top_2bit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the block selection\n",
    "block_selection = 'transformer.blocks[0].mlp.fc1'\n",
    "with torch.no_grad():\n",
    "    # Get the blocks from each model\n",
    "    block_full_precision = get_block(model, block_selection)\n",
    "    block_8bit = get_block(model_8, block_selection)\n",
    "    block_4bit = get_block(model_4, block_selection)\n",
    "    block_2bit = get_block(model_2, block_selection)\n",
    "\n",
    "    # Dequantize the quantized weights before performing the operation\n",
    "    weight_full_precision = block_full_precision.weight\n",
    "    weight_8bit = block_8bit.weight().dequantize() # .int_repr().float()\n",
    "    weight_4bit = block_4bit.weight().dequantize() # .int_repr().float()\n",
    "    weight_2bit = block_2bit.weight().dequantize() # .int_repr().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.006750397456343222\n",
      "KL Divergence: 17.704822635573848\n",
      "KL Divergence: 24.204427696510965\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Example: Plot histograms\n",
    "plt.hist(weight_full_precision.flatten().detach().numpy(), bins=50, alpha=0.5, label=\"Original\")\n",
    "# plt.hist(weight_8bit.flatten().detach().numpy(), bins=50, alpha=0.5, label=\"8-bit\")\n",
    "# plt.hist(weight_4bit.flatten().detach().numpy(), bins=50, alpha=0.5, label=\"4-bit\")\n",
    "plt.hist(weight_2bit.flatten().detach().numpy(), bins=50, alpha=0.5, label=\"2-bit\")\n",
    "plt.legend()\n",
    "plt.xlim(-1, 1)\n",
    "plt.show()\n",
    "\n",
    "def compute_kl_divergence(original, quantized, bins=50, epsilon=1e-10):\n",
    "    hist_original, _ = np.histogram(original, bins=bins, density=True)\n",
    "    hist_quantized, _ = np.histogram(quantized, bins=bins, density=True)\n",
    "    \n",
    "    # Add epsilon to avoid zero probabilities\n",
    "    hist_original = hist_original + epsilon\n",
    "    hist_quantized = hist_quantized + epsilon\n",
    "    \n",
    "    # Normalize to ensure valid probability distributions\n",
    "    hist_original = hist_original / np.sum(hist_original)\n",
    "    hist_quantized = hist_quantized / np.sum(hist_quantized)\n",
    "    \n",
    "    return entropy(hist_original, hist_quantized)\n",
    "\n",
    "kl_divergence = compute_kl_divergence(weight_full_precision.flatten().detach().numpy(), weight_8bit.flatten().detach().numpy())\n",
    "print(\"KL Divergence:\", kl_divergence)\n",
    "\n",
    "kl_divergence = compute_kl_divergence(weight_full_precision.flatten().detach().numpy(), weight_4bit.flatten().detach().numpy())\n",
    "print(\"KL Divergence:\", kl_divergence)\n",
    "\n",
    "kl_divergence = compute_kl_divergence(weight_full_precision.flatten().detach().numpy(), weight_2bit.flatten().detach().numpy())\n",
    "print(\"KL Divergence:\", kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of the first layer of the transformer blocks:\n",
      "2.4307639598846436\n",
      "\n",
      "Min value of the first layer of the transformer blocks:\n",
      "-3.06199312210083\n",
      "\n",
      "Average of the first layer of the transformer blocks:\n",
      "3.992090205429122e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value of the first layer of the transformer blocks:\")\n",
    "print(weight_full_precision.max().item())\n",
    "print()\n",
    "\n",
    "print(f\"Min value of the first layer of the transformer blocks:\")\n",
    "print(weight_full_precision.min().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average of the first layer of the transformer blocks:\")\n",
    "print(weight_full_precision.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of the first layer of the transformer blocks:\n",
      "2.4255788326263428\n",
      "\n",
      "Min value of the first layer of the transformer blocks:\n",
      "-3.074000835418701\n",
      "\n",
      "Average of the first layer of the transformer blocks:\n",
      "3.338760870974511e-05\n",
      "\n",
      "Average difference between 8-bit version and full precision:\n",
      "0.005997746717184782\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value of the first layer of the transformer blocks:\")\n",
    "print(weight_8bit.max().item())\n",
    "print()\n",
    "\n",
    "print(f\"Min value of the first layer of the transformer blocks:\")\n",
    "print(weight_8bit.min().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average of the first layer of the transformer blocks:\")\n",
    "print(weight_8bit.mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 8-bit version and full precision:\")\n",
    "print(torch.abs(block_8bit.weight().dequantize() - weight_full_precision).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of the first layer of the transformer blocks:\n",
      "2.449594497680664\n",
      "\n",
      "Min value of the first layer of the transformer blocks:\n",
      "-3.2661259174346924\n",
      "\n",
      "Average of the first layer of the transformer blocks:\n",
      "1.4881919923936948e-05\n",
      "\n",
      "Average difference between 4-bit version and full precision:\n",
      "0.04256489500403404\n",
      "\n",
      "Average difference between 4-bit version and 8-bit version:\n",
      "0.042156487703323364\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value of the first layer of the transformer blocks:\")\n",
    "print(weight_4bit.max().item())\n",
    "print()\n",
    "\n",
    "print(f\"Min value of the first layer of the transformer blocks:\")\n",
    "print(weight_4bit.min().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average of the first layer of the transformer blocks:\")\n",
    "print(weight_4bit.mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 4-bit version and full precision:\")\n",
    "print(torch.abs(weight_4bit - weight_full_precision).mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 4-bit version and 8-bit version:\")\n",
    "print(torch.abs(weight_4bit - weight_8bit).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of the first layer of the transformer blocks:\n",
      "2.0413286685943604\n",
      "\n",
      "Min value of the first layer of the transformer blocks:\n",
      "-4.082657337188721\n",
      "\n",
      "Average of the first layer of the transformer blocks:\n",
      "-1.7304555512964725e-06\n",
      "\n",
      "Average difference between 2-bit version and full precision:\n",
      "0.0428580567240715\n",
      "\n",
      "Average difference between 2-bit version and 8-bit version:\n",
      "0.042456429451704025\n",
      "\n",
      "Average difference between 2-bit version and 4-bit version:\n",
      "0.001149368821643293\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value of the first layer of the transformer blocks:\")\n",
    "print(weight_2bit.max().item())\n",
    "print()\n",
    "\n",
    "print(f\"Min value of the first layer of the transformer blocks:\")\n",
    "print(weight_2bit.min().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average of the first layer of the transformer blocks:\")\n",
    "print(weight_2bit.mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 2-bit version and full precision:\")\n",
    "print(torch.abs(weight_2bit - weight_full_precision).mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 2-bit version and 8-bit version:\")\n",
    "print(torch.abs(weight_2bit - weight_8bit).mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 2-bit version and 4-bit version:\")\n",
    "print(torch.abs(weight_2bit - weight_4bit).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after quantization:\n",
      "Size (MB): 122.099212\n",
      "ViLTransformerSS(\n",
      "  (text_embeddings): BertEmbeddings(\n",
      "    (word_embeddings): QuantizedEmbedding(num_embeddings=30522, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (position_embeddings): QuantizedEmbedding(num_embeddings=40, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (token_type_embeddings): QuantizedEmbedding(num_embeddings=2, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "  )\n",
      "  (token_type_embeddings): QuantizedEmbedding(num_embeddings=3, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): DynamicQuantizedLinear(in_features=768, out_features=2304, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (nlvr2_classifier): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=1536, out_features=1536, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): DynamicQuantizedLinear(in_features=1536, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (train_nlvr2_accuracy): Accuracy()\n",
      "  (train_nlvr2_loss): Scalar()\n",
      "  (dev_nlvr2_accuracy): Accuracy()\n",
      "  (dev_nlvr2_loss): Scalar()\n",
      "  (test_nlvr2_accuracy): Accuracy()\n",
      "  (test_nlvr2_loss): Scalar()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import dynamic_quantization as dq\n",
    "\n",
    "default_dynamic = copy.deepcopy(model)\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "        default_dynamic, {torch.nn.Embedding, torch.nn.Conv2d}, dtype=torch.quint8, inplace=True\n",
    "    )\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "        default_dynamic, {torch.nn.Linear, torch.nn.LayerNorm, torch.nn.Conv2d}, dtype=torch.qint8, inplace=True\n",
    "    )\n",
    "\n",
    "print(\"Size after quantization:\")\n",
    "print_size_of_model(default_dynamic)\n",
    "print(default_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Quantizing the model DYNAMIC =========\n",
      "Size after quantization:\n",
      "Size of the model (MB): 122.099212\n"
     ]
    }
   ],
   "source": [
    "import dynamic_quantization as dq\n",
    "\n",
    "custom_8bit = copy.deepcopy(model)\n",
    "custom_8bit = dq.quantize_model_dynamic(custom_8bit, 8)\n",
    "\n",
    "print(\"Size after quantization:\")\n",
    "print_size_of_model(custom_8bit)\n",
    "# print(model_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Suite Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization._numeric_suite as ns\n",
    "\n",
    "def compute_error(x, y):\n",
    "    \"\"\"\n",
    "    Signal to Noise Ratio (SNR)    \n",
    "    \"\"\"\n",
    "    Ps = torch.norm(x)\n",
    "    Pn = torch.norm(x-y)\n",
    "    return 20*torch.log10(Ps/Pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 27.66\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 21.96\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 26.82\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 18.22\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 32.16\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 29.15\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 27.63\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 16.82\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 33.97\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 36.37\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 32.58\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 16.36\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 33.17\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 36.98\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 31.92\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 30.59\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 34.69\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 38.77\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 31.58\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 27.02\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 33.92\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 38.16\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 35.53\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 29.81\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 35.89\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 33.90\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 31.89\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 16.10\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 33.92\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 37.94\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 23.23\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 15.11\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 35.26\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 36.08\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 18.87\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 18.36\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 33.13\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 30.52\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 22.55\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 21.04\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 30.75\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 34.32\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 33.32\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 29.17\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 30.99\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 26.40\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 28.97\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 19.50\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 36.56\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 38.95\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 40.77\n",
      "Total error: 1515.32\n",
      "Total inf: 28\n",
      "Max error: 40.76809310913086\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), custom_8bit.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_batch = next(iter(infer_dataloader))\n",
    "# calibration_batch = next(iter(calibrarte_dm.val_dataloader()))\n",
    "# full_batch = next(iter(full_dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(custom_8bit), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 38.81\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 38.69\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -1.60\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 3.39\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - -2.19\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.20\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 3.85\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - -2.46\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -1.73\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 3.21\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - -1.97\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -0.67\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 3.16\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - -2.50\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -1.57\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 0.13\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - -2.05\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -0.66\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 3.54\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - -2.46\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -1.65\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 0.45\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - -1.99\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -0.67\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 3.90\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -2.23\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -1.68\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 0.99\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - -1.25\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -0.83\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 4.14\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -2.07\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -1.69\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 1.50\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - -1.40\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -0.87\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 4.58\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -1.89\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -1.64\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 2.44\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - -1.32\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -0.91\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 4.66\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -2.15\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -1.52\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 6.07\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - -0.54\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -0.87\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 4.99\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - -1.42\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -1.36\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 8.05\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - -0.38\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -0.91\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 4.71\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - -2.26\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -1.24\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 9.07\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - -0.18\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -0.76\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 6.01\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - -0.98\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -0.34\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 9.05\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - 3.20\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - 0.71\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 8.63\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.76\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - 0.41\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 10.08\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 10.70\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - 1.52\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 8.40\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - 6.50\n",
      "75 - transformer.norm.stats\n",
      "75 - 9.95\n",
      "76 - pooler.dense.stats\n",
      "76 - 11.90\n",
      "Total error: 180.13\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 27.66\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 21.96\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 26.82\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 18.22\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 32.16\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 29.15\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 27.63\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 16.82\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 33.97\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 36.37\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 32.58\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 16.36\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 33.17\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 36.98\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 31.92\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 30.59\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 34.69\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 38.77\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 31.58\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 27.02\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 33.92\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 38.16\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 35.53\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 29.81\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 35.89\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 33.90\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 31.89\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 16.10\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 33.92\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 37.94\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 23.23\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 15.11\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 35.26\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 36.08\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 18.87\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 18.36\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 33.13\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 30.52\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 22.55\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 21.04\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 30.75\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 34.32\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 33.32\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 29.17\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 30.99\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 26.40\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 28.97\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 19.50\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 36.56\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 38.95\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 40.77\n",
      "Total error: 1515.32\n",
      "Total inf: 28\n",
      "Max error: 40.76809310913086\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), model_8.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_8), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 38.81\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 38.69\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -1.87\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 3.13\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - -2.44\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.42\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 3.71\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - -2.41\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -1.86\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 3.06\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - -2.14\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -0.78\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 3.07\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - -2.50\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -1.65\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 0.03\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - -2.07\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -0.73\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 3.47\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - -2.55\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -1.74\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 0.31\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - -2.06\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -0.78\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 3.71\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -2.34\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -1.78\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 0.85\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - -1.29\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -0.95\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 3.95\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -2.10\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -1.77\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 1.38\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - -1.42\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -0.97\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 4.47\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -1.92\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -1.72\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 2.34\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - -1.42\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -1.01\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 4.62\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -4.56\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -1.62\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 5.93\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - -0.88\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -0.98\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 4.90\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - -0.90\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -1.46\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 7.89\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - -0.44\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -1.01\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 4.61\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - -1.31\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -1.34\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 8.92\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - 0.01\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -0.83\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 6.02\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - -0.62\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -0.39\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 8.98\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - 1.76\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - 0.46\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 8.36\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.35\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - 0.15\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 9.63\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 2.55\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - 1.39\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 6.98\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - 4.02\n",
      "75 - transformer.norm.stats\n",
      "75 - 6.09\n",
      "76 - pooler.dense.stats\n",
      "76 - -3.00\n",
      "Total error: 139.56\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-bit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 4.57\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 0.87\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 3.18\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 0.22\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 7.66\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 4.76\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 3.51\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 0.10\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 9.37\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 11.76\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 7.98\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 0.05\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 8.57\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 12.35\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 7.33\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 6.00\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 10.08\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 14.17\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 6.98\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 2.84\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 9.31\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 13.54\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 10.93\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 5.26\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 11.29\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 9.31\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 7.29\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 0.17\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 9.30\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 13.32\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 0.78\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 0.09\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 10.64\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 11.48\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 0.12\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 0.10\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 8.53\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 5.93\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 0.53\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 0.38\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 6.15\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 9.72\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 8.71\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 4.98\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 6.48\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 2.43\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 4.49\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 0.62\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 11.96\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 14.34\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 16.09\n",
      "Total error: 336.60\n",
      "Total inf: 28\n",
      "Max error: 16.087890625\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), model_4.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_batch = next(iter(infer_dataloader))\n",
    "# calibration_batch = next(iter(calibrarte_dm.val_dataloader()))\n",
    "# full_batch = next(iter(full_dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_4), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 14.32\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 14.07\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -1.89\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 3.17\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - -0.62\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.70\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 2.84\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - -0.42\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -2.18\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 3.19\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - -3.70\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -1.32\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 2.90\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - -0.18\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -2.15\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 0.48\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - -1.89\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -1.27\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 3.55\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - -0.07\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -2.28\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 0.64\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - -2.12\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -1.36\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 3.62\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -2.47\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -2.39\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 1.03\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - -2.84\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -1.66\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 3.35\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -3.62\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -2.55\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 1.45\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - -2.82\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -1.95\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 3.00\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -4.29\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -2.63\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 2.43\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - -2.55\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -2.13\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 3.03\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -0.59\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -2.59\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 5.74\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - -2.11\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -2.20\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 2.01\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - 0.53\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -2.59\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 7.28\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - -2.04\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -2.34\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 0.48\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - -2.11\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -2.61\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 7.78\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - -2.57\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -2.36\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 2.87\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - -1.64\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -2.42\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 7.05\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - -1.44\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - -2.54\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 3.15\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - -0.11\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - -2.66\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 6.56\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 3.69\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - -2.45\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 2.79\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - -0.90\n",
      "75 - transformer.norm.stats\n",
      "75 - -0.89\n",
      "76 - pooler.dense.stats\n",
      "76 - -1.28\n",
      "Total error: 17.57\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 0.39\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 0.07\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 0.04\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 0.03\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 0.32\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 0.03\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 0.04\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 0.03\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 0.27\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 0.47\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 0.07\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 0.02\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 0.08\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 0.59\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 0.02\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 0.01\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 0.19\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 1.32\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 0.01\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 0.00\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 0.09\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 0.99\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 0.26\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 0.02\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 0.32\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 0.06\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 0.02\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 0.11\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 0.06\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 0.88\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 0.05\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 0.07\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 0.19\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 0.31\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 0.03\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 0.03\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 0.04\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 0.02\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 0.00\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 0.01\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 0.02\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 0.13\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 0.05\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 0.00\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 0.04\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 0.23\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 0.01\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 0.07\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 1.10\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 1.35\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 2.87\n",
      "Total error: 13.46\n",
      "Total inf: 28\n",
      "Max error: 2.8716225624084473\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), model_2.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_2), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 3.22\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - -0.85\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -2.04\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 4.84\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - 0.19\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.89\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 0.47\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - 0.06\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -2.27\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 4.57\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - 0.09\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -1.63\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 0.43\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - 0.00\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -2.29\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 1.54\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - -0.06\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -1.64\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 0.72\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - 0.03\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -2.42\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 2.12\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - -0.07\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -1.65\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 0.55\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -0.06\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -2.51\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 2.25\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - -0.42\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -1.86\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 0.45\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -0.06\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -2.58\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 3.01\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - -0.18\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -1.97\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 1.75\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -0.05\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -2.68\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 3.46\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - -0.01\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -2.09\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 0.51\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -0.02\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -2.72\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 7.11\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - -0.26\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -2.21\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 0.25\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - -0.34\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -1.79\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 8.48\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - -0.07\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -1.66\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - -0.37\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - -12.75\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -0.56\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 9.11\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - 0.21\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -0.85\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - -0.09\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - -12.50\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -0.30\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 6.60\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - -0.01\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - -0.34\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 0.34\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.26\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - -0.49\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 6.93\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - -2.09\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - -1.66\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 0.11\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - 0.05\n",
      "75 - transformer.norm.stats\n",
      "75 - -0.41\n",
      "76 - pooler.dense.stats\n",
      "76 - -0.39\n",
      "Total error: -3.45\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 0.00\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 0.00\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 0.00\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 0.00\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 0.00\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 0.00\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 0.00\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 0.00\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 0.00\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 0.00\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 0.00\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 0.00\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 0.00\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 0.00\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 0.00\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 0.00\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 0.00\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 0.00\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 0.00\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 0.00\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 0.00\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 0.00\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 0.00\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 0.00\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 0.00\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 0.00\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 0.00\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 0.00\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 0.00\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 0.00\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 0.00\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 0.00\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 0.00\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 0.00\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 0.00\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 0.00\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 0.00\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 0.00\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 0.00\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 0.00\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 0.00\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 0.00\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 0.00\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 0.00\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 0.00\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 0.00\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 0.00\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 0.00\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 0.00\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 0.00\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 0.00\n",
      "Total error: 0.00\n",
      "Total inf: 28\n",
      "Max error: 0\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), model_1.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_1), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 2.06\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - -12.40\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -2.05\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 5.05\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - 0.34\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.95\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 0.40\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - 0.03\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -2.34\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 4.93\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - 0.09\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -1.64\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 0.30\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - 0.00\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -2.38\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 1.82\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - 0.04\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -1.67\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 0.29\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - -0.01\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -2.52\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 2.19\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - 0.03\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -1.71\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 0.28\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -0.05\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -2.60\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 2.58\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - 0.02\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -1.90\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 0.29\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -0.06\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -2.67\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 3.19\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - 0.03\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -2.01\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 0.26\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -0.02\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -2.71\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 4.13\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - 0.02\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -2.14\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 0.23\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -0.00\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -2.71\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 7.23\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - 0.01\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -2.18\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 0.18\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - -0.00\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -2.67\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 8.87\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - 0.00\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -2.26\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 0.15\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - 0.00\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -2.56\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 9.63\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - 0.15\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -2.12\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 0.09\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - 0.03\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -2.13\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 6.19\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - -0.01\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - -1.91\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 0.04\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.03\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - -2.11\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 7.11\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 0.04\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - -4.14\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 0.12\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - -0.02\n",
      "75 - transformer.norm.stats\n",
      "75 - 0.39\n",
      "76 - pooler.dense.stats\n",
      "76 - 0.10\n",
      "Total error: 1.38\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
