{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Notebook For Quantizing VLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mileriso/envs/.py10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Suppress specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import torch\n",
    "import torch.quantization\n",
    "import pytorch_lightning as pl\n",
    "from copy import deepcopy\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Model Specific imports\n",
    "from vilt.datamodules.multitask_datamodule import MTDataModule as MTDataModuleVILT\n",
    "from meter.datamodules.multitask_datamodule import MTDataModule as MTDataModuleMeter\n",
    "from vilt.modules import ViLTransformerSS\n",
    "from meter.modules import METERTransformerSS\n",
    "\n",
    "# Custom imports\n",
    "import configs\n",
    "from quantization_utils import get_quantization_config\n",
    "from quantization_utils import  SmallMTDataModuleMETER, SmallMTDataModuleVILT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "# Set the configuration\n",
    "_config = configs.meter_config_nlvr2_original\n",
    "_config[\"batch_size\"] = 2\n",
    "_config[\"per_gpu_batchsize\"] = 2\n",
    "\n",
    "# Set the PyTorch Lightning seed\n",
    "pl.seed_everything(_config[\"seed\"])\n",
    "\n",
    "# Limit the number of CPUs\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"  # Set this to the number of CPUs you want to use\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"  # Set this to the number of CPUs you want to use\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Distributed Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.distributed as dist\n",
    "# # Initialize the process group\n",
    "# dist.init_process_group(backend='gloo', init_method='env://', world_size=1, rank=0)\n",
    "\n",
    "# # Verify initialization\n",
    "# print(f\"Initialized: {dist.is_initialized()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \"\"\"\n",
    "    Function to print the size of the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to get the size\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: ['nlvr2_train']\n",
      "Loaded names: ['nlvr2_dev', 'nlvr2_test1']\n",
      "Loaded names: ['nlvr2_dev', 'nlvr2_test1']\n",
      "Loaded names: ['nlvr2_train']\n",
      "Loaded names: ['nlvr2_dev', 'nlvr2_test1']\n",
      "Loaded names: ['nlvr2_dev', 'nlvr2_test1']\n",
      "Batch size: 2\n",
      "Lenght of the dataloader: 697\n",
      "Length of calibration dataloader: 70\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ========= Create full datamodule =========\n",
    "# ==========================================\n",
    "if \"meter\" in _config[\"model\"]:\n",
    "    # full_dm = MTDataModuleMeter(_config, dist=False)\n",
    "    \n",
    "    test_dm = SmallMTDataModuleMETER(_config, dist=False, percentage=0.01)\n",
    "    test_dm.setup(\"test\", is_random=True)\n",
    "    test_dataloader = test_dm.test_dataloader()\n",
    "    \n",
    "    fine_tune_dm = SmallMTDataModuleMETER(_config, dist=False, percentage=0.1)\n",
    "    fine_tune_dm.setup(\"test\", is_random=True)\n",
    "    fine_tune_dataloader = fine_tune_dm.test_dataloader()\n",
    "\n",
    "elif \"vilt\" in _config[\"model\"]:\n",
    "    # full_dm = MTDataModuleVILT(_config, dist=False)\n",
    "\n",
    "    test_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=50)\n",
    "    test_dm.setup(\"test\", is_random=True)\n",
    "    test_dataloader = test_dm.test_dataloader()\n",
    "\n",
    "    fine_tune_dm = SmallMTDataModuleVILT(_config, dist=False, num_samples=50)\n",
    "    fine_tune_dm.setup(\"test\", is_random=True)\n",
    "    fine_tune_dataloader = fine_tune_dm.test_dataloader()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported: \", _config[\"model\"])\n",
    "\n",
    "print(f\"Batch size: {_config['batch_size']}\")\n",
    "print(f\"Lenght of the dataloader: {len(fine_tune_dataloader)}\")\n",
    "print(f\"Length of calibration dataloader: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized METER model\n"
     ]
    }
   ],
   "source": [
    "if _config[\"model\"] == \"vilt\":\n",
    "    model = ViLTransformerSS(_config)\n",
    "    print(\"Initialized ViLT model\")\n",
    "\n",
    "elif _config[\"model\"] == \"meter\":\n",
    "    model = METERTransformerSS(_config)\n",
    "    print(\"Initialized METER model\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Model not supported: \", _config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize The Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "# ========== Initialize the trainer for full precision ==========\n",
    "def init_trainer(_config):\n",
    "    exp_name = f'{_config[\"exp_name\"]}'\n",
    "\n",
    "    os.makedirs(_config[\"log_dir\"], exist_ok=True)\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val/the_metric\",\n",
    "        mode=\"max\",\n",
    "        save_last=True,\n",
    "    )\n",
    "    logger = pl.loggers.TensorBoardLogger(\n",
    "        _config[\"log_dir\"],\n",
    "        name=f'{exp_name}_seed{_config[\"seed\"]}_from_{_config[\"load_path\"].split(\"/\")[-1][:-5]}',\n",
    "    )\n",
    "\n",
    "    lr_callback = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "    callbacks = [checkpoint_callback, lr_callback]\n",
    "\n",
    "    num_gpus = (\n",
    "        _config[\"num_gpus\"]\n",
    "        if isinstance(_config[\"num_gpus\"], int)\n",
    "        else len(_config[\"num_gpus\"])\n",
    "    )\n",
    "\n",
    "    grad_steps = max(_config[\"batch_size\"] // (\n",
    "        _config[\"per_gpu_batchsize\"] * num_gpus * _config[\"num_nodes\"]\n",
    "    ), 1)\n",
    "\n",
    "    max_steps = _config[\"max_steps\"] if _config[\"max_steps\"] is not None else None\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "            accelerator=\"gpu\",\n",
    "            devices=1,\n",
    "            num_nodes=_config[\"num_nodes\"],\n",
    "            precision=_config[\"precision\"],\n",
    "            # strategy=\"ddp\",\n",
    "            benchmark=True,\n",
    "            deterministic=False,\n",
    "            max_epochs=_config[\"max_epoch\"] if max_steps is None else 1000,\n",
    "            max_steps=max_steps,\n",
    "            callbacks=callbacks,\n",
    "            logger=logger,\n",
    "            accumulate_grad_batches=grad_steps,\n",
    "            log_every_n_steps=10,\n",
    "            fast_dev_run=_config[\"fast_dev_run\"],\n",
    "            val_check_interval=_config[\"val_check_interval\"],\n",
    "        )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "trainer = init_trainer(_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantization_config_dict(bits, module_name_list):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of quantization configurations for specific modules in a model.\n",
    "    \n",
    "    Args:\n",
    "        bits (int): The number of bits to quantize the model to. Available options are 8, 4, and 2.\n",
    "        module_name_list (list of str): A list of module names (or dot-separated paths) within the model to quantize.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of quantization configurations for the specified modules.\n",
    "    \"\"\"\n",
    "\n",
    "    quantization_config, embedding_config = get_quantization_config(bits)\n",
    "    modules_config = {}\n",
    "\n",
    "    for module_name in module_name_list:\n",
    "        if \"embedding\" in module_name:\n",
    "            modules_config[module_name] = embedding_config\n",
    "        else:\n",
    "            modules_config[module_name] = quantization_config\n",
    "    \n",
    "    return modules_config\n",
    "\n",
    "\n",
    "def quantize_modules(model, bits, module_name_list):\n",
    "    \"\"\"\n",
    "    Quantizes specific modules in a deep copy of the input model using dynamic quantization.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to quantize.\n",
    "        bits (int): The number of bits to quantize the model to. Available options are 8, 4, and 2.\n",
    "        module_names_to_quantize (list of str): A list of module names (or dot-separated paths)\n",
    "                                                 within the model to apply dynamic quantization to.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: A deep copy of the input model with specified modules dynamically quantized.\n",
    "                         Returns None if no modules are provided to quantize.\n",
    "    \"\"\"\n",
    "\n",
    "    modules_config = create_quantization_config_dict(bits, module_name_list)\n",
    "\n",
    "    model_quantized = deepcopy(model)\n",
    "    \n",
    "    \n",
    "    torch.quantization.quantize_dynamic(\n",
    "        model_quantized, modules_config, inplace=True\n",
    "    )\n",
    "\n",
    "    return model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision Model:\n",
      "Size (MB): 1296.258138\n",
      "The model is on: cpu\n",
      "Quantized Model (4 bits):\n",
      "Size (MB): 1267.950002\n",
      "The model is on: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Precision Model:\")\n",
    "print_size_of_model(model)\n",
    "print(f\"The model is on: {model.device}\")\n",
    "\n",
    "# Quantize the model\n",
    "bits = 4\n",
    "modules_to_quantize = [\n",
    "    \"text_transformer.encoder.layer.2.output.dense\",\n",
    "    \"text_transformer.encoder.layer.2.intermediate.dense\",\n",
    "    \"text_transformer.encoder.layer.3.output.dense\",\n",
    "    \"text_transformer.encoder.layer.3.intermediate.dense\"\n",
    "]\n",
    "\n",
    "model_dynamic = quantize_modules(model, bits, modules_to_quantize)\n",
    "\n",
    "print(f\"Quantized Model ({bits} bits):\")\n",
    "print_size_of_model(model_dynamic)\n",
    "print(f\"The model is on: {model_dynamic.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_inputs = next(iter(infer_dataloader))\n",
    "# print(example_inputs.keys())\n",
    "# # print(input_[\"answers\"])\n",
    "# print(f\"Number of batches: {len(infer_dataloader)}\")\n",
    "# print(f\"Smaples in a batch: {len(example_inputs['answers'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 70/70 [00:08<00:00,  8.24it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   nlvr2/dev/accuracy       0.7850467562675476\n",
      "nlvr2/dev/accuracy_epoch    0.8055555820465088\n",
      "     nlvr2/dev/loss         0.5976913571357727\n",
      "  nlvr2/dev/loss_epoch      0.5927587151527405\n",
      "   nlvr2/test/accuracy       0.843137264251709\n",
      "nlvr2/test/accuracy_epoch   0.8208954930305481\n",
      "     nlvr2/test/loss        0.4164871871471405\n",
      "  nlvr2/test/loss_epoch     0.4164871871471405\n",
      "     val/the_metric         0.8208954930305481\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'nlvr2/dev/loss': 0.5976913571357727,\n",
       "  'nlvr2/dev/accuracy': 0.7850467562675476,\n",
       "  'nlvr2/test/loss': 0.4164871871471405,\n",
       "  'nlvr2/test/accuracy': 0.843137264251709,\n",
       "  'nlvr2/dev/accuracy_epoch': 0.8055555820465088,\n",
       "  'nlvr2/dev/loss_epoch': 0.5927587151527405,\n",
       "  'nlvr2/test/accuracy_epoch': 0.8208954930305481,\n",
       "  'nlvr2/test/loss_epoch': 0.4164871871471405,\n",
       "  'val/the_metric': 0.8208954930305481}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frozen_layers(model):\n",
    "    \"\"\"\n",
    "    Prints the names of the layers in a PyTorch model and whether they are frozen or not.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to print the frozen status of.\n",
    "    \"\"\"\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Layer: {name}, Frozen: {not param.requires_grad}\")\n",
    "\n",
    "\n",
    "def freeze_except_layers(model, layers_to_unfreeze_names):\n",
    "    \"\"\"\n",
    "    Freezes all parameters of a PyTorch model except for the layers specified by their names.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to freeze parameters in.\n",
    "        layers_to_unfreeze_names (list of str): A list of module names that should NOT be frozen.\n",
    "                                             Parameters in modules whose names contain these strings will be unfrozen.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        freeze = True  # Initially assume we should freeze the parameter\n",
    "        for layer_name_to_unfreeze in layers_to_unfreeze_names:\n",
    "            if layer_name_to_unfreeze in name:\n",
    "                freeze = False  # Unfreeze if the name contains a layer to unfreeze\n",
    "                break  # No need to check other layer names if already unfrozen\n",
    "\n",
    "        if freeze:\n",
    "            param.requires_grad = False  # Freeze the parameter\n",
    "        else:\n",
    "            param.requires_grad = True   # Ensure it's unfrozen (explicitly set to True)\n",
    "\n",
    "    # Optional: Print which layers are frozen and unfrozen for verification\n",
    "    print_frozen_layers(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.output.dense.bias, Frozen: False\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization.qconfig import default_dynamic_qat_qconfig\n",
    "from torch.ao.quantization.quantize import convert, propagate_qconfig_, prepare\n",
    "from torch.ao.quantization.quantization_mappings import get_default_qat_module_mappings\n",
    "import copy\n",
    "\n",
    "def prepare_qat(model, qconfig_dict, mapping=None, inplace=False):\n",
    "    r\"\"\"\n",
    "    Prepares a copy of the model for quantization calibration or\n",
    "    quantization-aware training and converts it to quantized version.\n",
    "\n",
    "    Quantization configuration should be assigned preemptively\n",
    "    to individual submodules in `.qconfig` attribute.\n",
    "\n",
    "    Args:\n",
    "        model: input model to be modified in-place\n",
    "        mapping: dictionary that maps float modules to quantized modules to be\n",
    "                 replaced.\n",
    "        inplace: carry out model transformations in-place, the original module\n",
    "                 is mutated\n",
    "    \"\"\"\n",
    "    torch._C._log_api_usage_once(\"quantization_api.quantize.prepare_qat\")\n",
    "    assert model.training, \"prepare_qat only works on models in training mode\"\n",
    "    if mapping is None:\n",
    "        mapping = get_default_qat_module_mappings()\n",
    "\n",
    "    if not inplace:\n",
    "        model = copy.deepcopy(model)\n",
    "\n",
    "    propagate_qconfig_(model, qconfig_dict=qconfig_dict)\n",
    "    convert(model, mapping=mapping, inplace=True, remove_qconfig=False)\n",
    "    prepare(model, observer_non_leaf_module_list=set(mapping.values()), inplace=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create the quantization configuration dictionary\n",
    "qconfig_dict = dict()\n",
    "for layer in modules_to_quantize:\n",
    "    qconfig_dict[layer] = default_dynamic_qat_qconfig\n",
    "\n",
    "# Prepare the model for quantization-aware training\n",
    "model_qat = prepare_qat(model, inplace=False, qconfig_dict=qconfig_dict)\n",
    "\n",
    "# Freeze all layers except for the quantized layers\n",
    "freeze_except_layers(model_qat, modules_to_quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_transformer.encoder.layer.2.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.2.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.2.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.2.output.dense.bias 768\n",
      "text_transformer.encoder.layer.3.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.3.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.3.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.3.output.dense.bias 768\n",
      "Total trainable parameters: 9444864\n"
     ]
    }
   ],
   "source": [
    "# Count the trainable parameters\n",
    "sum_param = 0\n",
    "for name, param in model_qat.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())\n",
    "        sum_param += param.numel()\n",
    "    \n",
    "print(f\"Total trainable parameters: {sum_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name                        | Type         | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0  | cross_modal_text_transform  | Linear       | 590 K  | train\n",
      "1  | cross_modal_image_transform | Linear       | 590 K  | train\n",
      "2  | token_type_embeddings       | Embedding    | 2.3 K  | train\n",
      "3  | vit_model                   | CLIP         | 78.9 M | train\n",
      "4  | text_transformer            | RobertaModel | 124 M  | eval \n",
      "5  | cross_modal_image_layers    | ModuleList   | 56.7 M | train\n",
      "6  | cross_modal_text_layers     | ModuleList   | 56.7 M | train\n",
      "7  | cross_modal_image_pooler    | Pooler       | 590 K  | train\n",
      "8  | cross_modal_text_pooler     | Pooler       | 590 K  | train\n",
      "9  | nlvr2_classifier            | Sequential   | 4.7 M  | train\n",
      "10 | train_nlvr2_accuracy        | Accuracy     | 0      | train\n",
      "11 | train_nlvr2_loss            | Scalar       | 0      | train\n",
      "12 | dev_nlvr2_accuracy          | Accuracy     | 0      | train\n",
      "13 | dev_nlvr2_loss              | Scalar       | 0      | train\n",
      "14 | test_nlvr2_accuracy         | Accuracy     | 0      | train\n",
      "15 | test_nlvr2_loss             | Scalar       | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "9.4 M     Trainable params\n",
      "314 M     Non-trainable params\n",
      "324 M     Total params\n",
      "1,296.033 Total estimated model params size (MB)\n",
      "485       Modules in train mode\n",
      "224       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/70 [00:00<?, ?it/s] Executing training_step for task ['nlvr2']\n",
      "Output is obtained: dict_keys(['nlvr2_loss', 'nlvr2_logits', 'nlvr2_labels'])\n",
      "Total loss is 0.0025637000799179077\n",
      "Epoch 0:   1%|▏         | 1/70 [00:00<00:56,  1.23it/s, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'val/the_metric' was not in top 1\n",
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 1/70 [00:06<07:28,  0.15it/s, v_num=8]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model_qat, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_modal_text_transform.weight 589824\n",
      "cross_modal_text_transform.bias 768\n",
      "cross_modal_image_transform.weight 589824\n",
      "cross_modal_image_transform.bias 768\n",
      "token_type_embeddings.weight 2304\n",
      "vit_model.positional_embedding 39424\n",
      "vit_model.visual.class_embedding 768\n",
      "vit_model.visual.positional_embedding 249600\n",
      "vit_model.visual.conv1.weight 589824\n",
      "vit_model.visual.ln_pre.weight 768\n",
      "vit_model.visual.ln_pre.bias 768\n",
      "vit_model.visual.transformer.resblocks.0.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.0.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.0.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.0.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.0.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.0.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.0.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.0.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.0.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.0.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.0.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.0.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.1.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.1.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.1.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.1.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.1.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.1.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.1.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.1.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.1.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.1.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.1.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.1.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.2.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.2.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.2.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.2.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.2.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.2.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.2.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.2.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.2.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.2.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.2.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.2.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.3.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.3.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.3.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.3.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.3.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.3.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.3.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.3.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.3.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.3.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.3.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.3.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.4.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.4.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.4.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.4.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.4.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.4.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.4.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.4.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.4.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.4.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.4.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.4.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.5.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.5.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.5.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.5.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.5.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.5.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.5.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.5.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.5.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.5.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.5.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.5.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.6.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.6.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.6.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.6.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.6.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.6.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.6.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.6.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.6.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.6.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.6.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.6.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.7.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.7.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.7.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.7.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.7.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.7.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.7.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.7.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.7.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.7.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.7.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.7.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.8.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.8.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.8.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.8.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.8.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.8.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.8.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.8.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.8.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.8.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.8.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.8.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.9.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.9.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.9.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.9.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.9.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.9.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.9.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.9.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.9.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.9.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.9.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.9.ln_2.bias 768\n",
      "vit_model.visual.transformer.resblocks.10.attn.in_proj_weight 1769472\n",
      "vit_model.visual.transformer.resblocks.10.attn.in_proj_bias 2304\n",
      "vit_model.visual.transformer.resblocks.10.attn.out_proj.weight 589824\n",
      "vit_model.visual.transformer.resblocks.10.attn.out_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.10.ln_1.weight 768\n",
      "vit_model.visual.transformer.resblocks.10.ln_1.bias 768\n",
      "vit_model.visual.transformer.resblocks.10.mlp.c_fc.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.10.mlp.c_fc.bias 3072\n",
      "vit_model.visual.transformer.resblocks.10.mlp.c_proj.weight 2359296\n",
      "vit_model.visual.transformer.resblocks.10.mlp.c_proj.bias 768\n",
      "vit_model.visual.transformer.resblocks.10.ln_2.weight 768\n",
      "vit_model.visual.transformer.resblocks.10.ln_2.bias 768\n",
      "vit_model.visual.ln_post.weight 768\n",
      "vit_model.visual.ln_post.bias 768\n",
      "vit_model.ln_final.weight 512\n",
      "vit_model.ln_final.bias 512\n",
      "text_transformer.embeddings.word_embeddings.weight 38603520\n",
      "text_transformer.embeddings.position_embeddings.weight 394752\n",
      "text_transformer.embeddings.token_type_embeddings.weight 768\n",
      "text_transformer.embeddings.LayerNorm.weight 768\n",
      "text_transformer.embeddings.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.0.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.0.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.0.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.0.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.0.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.0.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.0.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.0.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.0.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.0.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.0.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.0.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.0.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.0.output.dense.bias 768\n",
      "text_transformer.encoder.layer.0.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.0.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.1.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.1.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.1.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.1.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.1.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.1.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.1.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.1.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.1.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.1.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.1.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.1.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.1.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.1.output.dense.bias 768\n",
      "text_transformer.encoder.layer.1.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.1.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.2.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.2.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.2.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.2.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.2.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.2.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.2.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.2.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.2.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.2.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.2.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.2.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.2.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.2.output.dense.bias 768\n",
      "text_transformer.encoder.layer.2.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.2.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.3.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.3.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.3.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.3.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.3.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.3.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.3.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.3.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.3.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.3.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.3.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.3.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.3.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.3.output.dense.bias 768\n",
      "text_transformer.encoder.layer.3.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.3.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.4.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.4.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.4.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.4.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.4.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.4.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.4.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.4.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.4.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.4.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.4.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.4.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.4.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.4.output.dense.bias 768\n",
      "text_transformer.encoder.layer.4.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.4.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.5.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.5.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.5.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.5.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.5.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.5.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.5.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.5.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.5.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.5.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.5.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.5.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.5.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.5.output.dense.bias 768\n",
      "text_transformer.encoder.layer.5.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.5.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.6.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.6.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.6.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.6.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.6.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.6.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.6.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.6.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.6.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.6.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.6.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.6.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.6.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.6.output.dense.bias 768\n",
      "text_transformer.encoder.layer.6.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.6.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.7.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.7.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.7.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.7.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.7.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.7.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.7.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.7.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.7.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.7.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.7.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.7.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.7.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.7.output.dense.bias 768\n",
      "text_transformer.encoder.layer.7.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.7.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.8.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.8.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.8.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.8.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.8.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.8.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.8.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.8.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.8.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.8.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.8.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.8.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.8.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.8.output.dense.bias 768\n",
      "text_transformer.encoder.layer.8.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.8.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.9.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.9.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.9.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.9.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.9.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.9.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.9.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.9.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.9.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.9.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.9.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.9.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.9.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.9.output.dense.bias 768\n",
      "text_transformer.encoder.layer.9.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.9.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.10.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.10.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.10.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.10.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.10.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.10.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.10.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.10.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.10.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.10.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.10.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.10.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.10.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.10.output.dense.bias 768\n",
      "text_transformer.encoder.layer.10.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.10.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.11.attention.self.query.weight 589824\n",
      "text_transformer.encoder.layer.11.attention.self.query.bias 768\n",
      "text_transformer.encoder.layer.11.attention.self.key.weight 589824\n",
      "text_transformer.encoder.layer.11.attention.self.key.bias 768\n",
      "text_transformer.encoder.layer.11.attention.self.value.weight 589824\n",
      "text_transformer.encoder.layer.11.attention.self.value.bias 768\n",
      "text_transformer.encoder.layer.11.attention.output.dense.weight 589824\n",
      "text_transformer.encoder.layer.11.attention.output.dense.bias 768\n",
      "text_transformer.encoder.layer.11.attention.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.11.attention.output.LayerNorm.bias 768\n",
      "text_transformer.encoder.layer.11.intermediate.dense.weight 2359296\n",
      "text_transformer.encoder.layer.11.intermediate.dense.bias 3072\n",
      "text_transformer.encoder.layer.11.output.dense.weight 2359296\n",
      "text_transformer.encoder.layer.11.output.dense.bias 768\n",
      "text_transformer.encoder.layer.11.output.LayerNorm.weight 768\n",
      "text_transformer.encoder.layer.11.output.LayerNorm.bias 768\n",
      "text_transformer.pooler.dense.weight 589824\n",
      "text_transformer.pooler.dense.bias 768\n",
      "cross_modal_image_layers.0.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.0.attention.self.query.bias 768\n",
      "cross_modal_image_layers.0.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.0.attention.self.key.bias 768\n",
      "cross_modal_image_layers.0.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.0.attention.self.value.bias 768\n",
      "cross_modal_image_layers.0.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.0.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.0.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.0.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.0.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.0.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.0.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.0.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.0.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.0.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.0.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.0.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.0.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.0.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.0.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.0.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.0.output.dense.weight 2359296\n",
      "cross_modal_image_layers.0.output.dense.bias 768\n",
      "cross_modal_image_layers.0.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.0.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.1.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.1.attention.self.query.bias 768\n",
      "cross_modal_image_layers.1.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.1.attention.self.key.bias 768\n",
      "cross_modal_image_layers.1.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.1.attention.self.value.bias 768\n",
      "cross_modal_image_layers.1.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.1.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.1.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.1.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.1.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.1.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.1.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.1.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.1.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.1.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.1.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.1.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.1.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.1.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.1.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.1.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.1.output.dense.weight 2359296\n",
      "cross_modal_image_layers.1.output.dense.bias 768\n",
      "cross_modal_image_layers.1.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.1.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.2.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.2.attention.self.query.bias 768\n",
      "cross_modal_image_layers.2.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.2.attention.self.key.bias 768\n",
      "cross_modal_image_layers.2.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.2.attention.self.value.bias 768\n",
      "cross_modal_image_layers.2.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.2.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.2.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.2.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.2.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.2.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.2.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.2.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.2.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.2.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.2.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.2.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.2.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.2.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.2.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.2.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.2.output.dense.weight 2359296\n",
      "cross_modal_image_layers.2.output.dense.bias 768\n",
      "cross_modal_image_layers.2.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.2.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.3.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.3.attention.self.query.bias 768\n",
      "cross_modal_image_layers.3.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.3.attention.self.key.bias 768\n",
      "cross_modal_image_layers.3.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.3.attention.self.value.bias 768\n",
      "cross_modal_image_layers.3.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.3.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.3.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.3.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.3.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.3.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.3.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.3.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.3.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.3.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.3.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.3.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.3.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.3.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.3.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.3.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.3.output.dense.weight 2359296\n",
      "cross_modal_image_layers.3.output.dense.bias 768\n",
      "cross_modal_image_layers.3.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.3.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.4.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.4.attention.self.query.bias 768\n",
      "cross_modal_image_layers.4.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.4.attention.self.key.bias 768\n",
      "cross_modal_image_layers.4.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.4.attention.self.value.bias 768\n",
      "cross_modal_image_layers.4.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.4.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.4.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.4.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.4.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.4.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.4.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.4.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.4.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.4.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.4.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.4.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.4.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.4.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.4.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.4.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.4.output.dense.weight 2359296\n",
      "cross_modal_image_layers.4.output.dense.bias 768\n",
      "cross_modal_image_layers.4.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.4.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.5.attention.self.query.weight 589824\n",
      "cross_modal_image_layers.5.attention.self.query.bias 768\n",
      "cross_modal_image_layers.5.attention.self.key.weight 589824\n",
      "cross_modal_image_layers.5.attention.self.key.bias 768\n",
      "cross_modal_image_layers.5.attention.self.value.weight 589824\n",
      "cross_modal_image_layers.5.attention.self.value.bias 768\n",
      "cross_modal_image_layers.5.attention.output.dense.weight 589824\n",
      "cross_modal_image_layers.5.attention.output.dense.bias 768\n",
      "cross_modal_image_layers.5.attention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.5.attention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.5.crossattention.self.query.weight 589824\n",
      "cross_modal_image_layers.5.crossattention.self.query.bias 768\n",
      "cross_modal_image_layers.5.crossattention.self.key.weight 589824\n",
      "cross_modal_image_layers.5.crossattention.self.key.bias 768\n",
      "cross_modal_image_layers.5.crossattention.self.value.weight 589824\n",
      "cross_modal_image_layers.5.crossattention.self.value.bias 768\n",
      "cross_modal_image_layers.5.crossattention.output.dense.weight 589824\n",
      "cross_modal_image_layers.5.crossattention.output.dense.bias 768\n",
      "cross_modal_image_layers.5.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.5.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_image_layers.5.intermediate.dense.weight 2359296\n",
      "cross_modal_image_layers.5.intermediate.dense.bias 3072\n",
      "cross_modal_image_layers.5.output.dense.weight 2359296\n",
      "cross_modal_image_layers.5.output.dense.bias 768\n",
      "cross_modal_image_layers.5.output.LayerNorm.weight 768\n",
      "cross_modal_image_layers.5.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.0.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.0.attention.self.query.bias 768\n",
      "cross_modal_text_layers.0.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.0.attention.self.key.bias 768\n",
      "cross_modal_text_layers.0.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.0.attention.self.value.bias 768\n",
      "cross_modal_text_layers.0.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.0.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.0.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.0.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.0.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.0.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.0.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.0.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.0.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.0.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.0.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.0.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.0.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.0.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.0.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.0.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.0.output.dense.weight 2359296\n",
      "cross_modal_text_layers.0.output.dense.bias 768\n",
      "cross_modal_text_layers.0.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.0.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.1.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.1.attention.self.query.bias 768\n",
      "cross_modal_text_layers.1.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.1.attention.self.key.bias 768\n",
      "cross_modal_text_layers.1.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.1.attention.self.value.bias 768\n",
      "cross_modal_text_layers.1.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.1.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.1.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.1.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.1.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.1.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.1.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.1.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.1.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.1.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.1.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.1.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.1.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.1.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.1.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.1.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.1.output.dense.weight 2359296\n",
      "cross_modal_text_layers.1.output.dense.bias 768\n",
      "cross_modal_text_layers.1.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.1.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.2.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.2.attention.self.query.bias 768\n",
      "cross_modal_text_layers.2.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.2.attention.self.key.bias 768\n",
      "cross_modal_text_layers.2.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.2.attention.self.value.bias 768\n",
      "cross_modal_text_layers.2.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.2.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.2.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.2.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.2.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.2.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.2.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.2.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.2.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.2.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.2.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.2.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.2.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.2.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.2.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.2.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.2.output.dense.weight 2359296\n",
      "cross_modal_text_layers.2.output.dense.bias 768\n",
      "cross_modal_text_layers.2.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.2.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.3.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.3.attention.self.query.bias 768\n",
      "cross_modal_text_layers.3.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.3.attention.self.key.bias 768\n",
      "cross_modal_text_layers.3.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.3.attention.self.value.bias 768\n",
      "cross_modal_text_layers.3.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.3.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.3.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.3.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.3.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.3.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.3.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.3.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.3.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.3.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.3.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.3.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.3.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.3.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.3.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.3.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.3.output.dense.weight 2359296\n",
      "cross_modal_text_layers.3.output.dense.bias 768\n",
      "cross_modal_text_layers.3.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.3.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.4.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.4.attention.self.query.bias 768\n",
      "cross_modal_text_layers.4.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.4.attention.self.key.bias 768\n",
      "cross_modal_text_layers.4.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.4.attention.self.value.bias 768\n",
      "cross_modal_text_layers.4.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.4.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.4.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.4.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.4.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.4.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.4.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.4.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.4.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.4.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.4.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.4.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.4.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.4.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.4.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.4.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.4.output.dense.weight 2359296\n",
      "cross_modal_text_layers.4.output.dense.bias 768\n",
      "cross_modal_text_layers.4.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.4.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.5.attention.self.query.weight 589824\n",
      "cross_modal_text_layers.5.attention.self.query.bias 768\n",
      "cross_modal_text_layers.5.attention.self.key.weight 589824\n",
      "cross_modal_text_layers.5.attention.self.key.bias 768\n",
      "cross_modal_text_layers.5.attention.self.value.weight 589824\n",
      "cross_modal_text_layers.5.attention.self.value.bias 768\n",
      "cross_modal_text_layers.5.attention.output.dense.weight 589824\n",
      "cross_modal_text_layers.5.attention.output.dense.bias 768\n",
      "cross_modal_text_layers.5.attention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.5.attention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.5.crossattention.self.query.weight 589824\n",
      "cross_modal_text_layers.5.crossattention.self.query.bias 768\n",
      "cross_modal_text_layers.5.crossattention.self.key.weight 589824\n",
      "cross_modal_text_layers.5.crossattention.self.key.bias 768\n",
      "cross_modal_text_layers.5.crossattention.self.value.weight 589824\n",
      "cross_modal_text_layers.5.crossattention.self.value.bias 768\n",
      "cross_modal_text_layers.5.crossattention.output.dense.weight 589824\n",
      "cross_modal_text_layers.5.crossattention.output.dense.bias 768\n",
      "cross_modal_text_layers.5.crossattention.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.5.crossattention.output.LayerNorm.bias 768\n",
      "cross_modal_text_layers.5.intermediate.dense.weight 2359296\n",
      "cross_modal_text_layers.5.intermediate.dense.bias 3072\n",
      "cross_modal_text_layers.5.output.dense.weight 2359296\n",
      "cross_modal_text_layers.5.output.dense.bias 768\n",
      "cross_modal_text_layers.5.output.LayerNorm.weight 768\n",
      "cross_modal_text_layers.5.output.LayerNorm.bias 768\n",
      "cross_modal_image_pooler.dense.weight 589824\n",
      "cross_modal_image_pooler.dense.bias 768\n",
      "cross_modal_text_pooler.dense.weight 589824\n",
      "cross_modal_text_pooler.dense.bias 768\n",
      "nlvr2_classifier.0.weight 4718592\n",
      "nlvr2_classifier.0.bias 1536\n",
      "nlvr2_classifier.1.weight 1536\n",
      "nlvr2_classifier.1.bias 1536\n",
      "nlvr2_classifier.3.weight 3072\n",
      "nlvr2_classifier.3.bias 2\n",
      "Total trainable parameters: 324008194\n"
     ]
    }
   ],
   "source": [
    "# Count the trainable parameters\n",
    "sum_param = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())\n",
    "        sum_param += param.numel()\n",
    "    \n",
    "print(f\"Total trainable parameters: {sum_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name                        | Type         | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0  | cross_modal_text_transform  | Linear       | 590 K  | train\n",
      "1  | cross_modal_image_transform | Linear       | 590 K  | train\n",
      "2  | token_type_embeddings       | Embedding    | 2.3 K  | train\n",
      "3  | vit_model                   | CLIP         | 78.9 M | train\n",
      "4  | text_transformer            | RobertaModel | 124 M  | eval \n",
      "5  | cross_modal_image_layers    | ModuleList   | 56.7 M | train\n",
      "6  | cross_modal_text_layers     | ModuleList   | 56.7 M | train\n",
      "7  | cross_modal_image_pooler    | Pooler       | 590 K  | train\n",
      "8  | cross_modal_text_pooler     | Pooler       | 590 K  | train\n",
      "9  | nlvr2_classifier            | Sequential   | 4.7 M  | train\n",
      "10 | train_nlvr2_accuracy        | Accuracy     | 0      | train\n",
      "11 | train_nlvr2_loss            | Scalar       | 0      | train\n",
      "12 | dev_nlvr2_accuracy          | Accuracy     | 0      | train\n",
      "13 | dev_nlvr2_loss              | Scalar       | 0      | train\n",
      "14 | test_nlvr2_accuracy         | Accuracy     | 0      | train\n",
      "15 | test_nlvr2_loss             | Scalar       | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "324 M     Trainable params\n",
      "0         Non-trainable params\n",
      "324 M     Total params\n",
      "1,296.033 Total estimated model params size (MB)\n",
      "465       Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/70 [00:00<?, ?it/s] Executing training_step for task ['nlvr2']\n",
      "Output is obtained: dict_keys(['nlvr2_loss', 'nlvr2_logits', 'nlvr2_labels'])\n",
      "Total loss is 0.0018258416093885899\n",
      "Epoch 0:   1%|▏         | 1/70 [00:00<01:05,  1.06it/s, v_num=9]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'val/the_metric' was not in top 1\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '/tmp/tmph4ubnmdl' -> '/home/mileriso/thesis/result/finetune_nlvr2_seed0_from_meter_nlvr2/version_9/checkpoints/last.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m init_trainer(_config)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:206\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:378\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitoring_callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    377\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitoring_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_num_ready_batches_reached():\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# if we are restarting and the above condition holds, it's because we are reloading an epoch-end checkpoint.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:218\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:326\u001b[0m, in \u001b[0;36mModelCheckpoint.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_every_n_epochs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_every_n_epochs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_topk_checkpoint(trainer, monitor_candidates)\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_last_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_candidates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:696\u001b[0m, in \u001b[0;36mModelCheckpoint._save_last_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_link_checkpoint(trainer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_checkpoint_saved, filepath)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_remove_checkpoint(trainer, previous, filepath):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_checkpoint(trainer, previous)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:390\u001b[0m, in \u001b[0;36mModelCheckpoint._save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_save_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, filepath: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_global_step_saved \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mglobal_step\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_checkpoint_saved \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1365\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving a checkpoint is only possible if a model is attached to the Trainer. Did you call\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer.save_checkpoint()` before calling `Trainer.\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mfit,validate,test,predict}`?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[0;32m-> 1365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer.save_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:490\u001b[0m, in \u001b[0;36mStrategy.save_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_global_zero:\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/lightning_fabric/plugins/io/torch_io.py:58\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     56\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path)\n\u001b[1;32m     57\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_atomic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:89\u001b[0m, in \u001b[0;36m_atomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# We use a transaction here to avoid file corruption if the save gets interrupted\u001b[39;00m\n\u001b[1;32m     88\u001b[0m fs, urlpath \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39murl_to_fs(\u001b[38;5;28mstr\u001b[39m(filepath))\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mtransaction, fs\u001b[38;5;241m.\u001b[39mopen(urlpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(bytesbuffer\u001b[38;5;241m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/fsspec/transaction.py:28\u001b[0m, in \u001b[0;36mTransaction.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"End transaction and commit, if exit is not due to exception\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# only commit if there was no exception\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexc_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39m_intrans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/fsspec/transaction.py:44\u001b[0m, in \u001b[0;36mTransaction.complete\u001b[0;34m(self, commit)\u001b[0m\n\u001b[1;32m     42\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit:\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     f\u001b[38;5;241m.\u001b[39mdiscard()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/fsspec/implementations/local.py:414\u001b[0m, in \u001b[0;36mLocalFileOpener.commit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit:\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only commit if not already set to autocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 414\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/shutil.py:836\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    834\u001b[0m         rmtree(src)\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 836\u001b[0m         \u001b[43mcopy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "File \u001b[0;32m/usr/lib/python3.10/shutil.py:434\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    433\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 434\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/usr/lib/python3.10/shutil.py:267\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _USE_CP_SENDFILE:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[43m_fastcopy_sendfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfsrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dst\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m _GiveupOnFastCopy:\n",
      "File \u001b[0;32m/usr/lib/python3.10/shutil.py:156\u001b[0m, in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _GiveupOnFastCopy(err)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOSPC:  \u001b[38;5;66;03m# filesystem is full\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Give up on first call and if no data was copied.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlseek(outfd, \u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mSEEK_CUR) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/shutil.py:142\u001b[0m, in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         sent \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# ...in oder to have a more informative exception.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         err\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m fsrc\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/tmp/tmph4ubnmdl' -> '/home/mileriso/thesis/result/finetune_nlvr2_seed0_from_meter_nlvr2/version_9/checkpoints/last.ckpt'"
     ]
    }
   ],
   "source": [
    "trainer = init_trainer(_config)\n",
    "trainer.fit(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METERTransformerSS(\n",
       "  (cross_modal_text_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (cross_modal_image_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (token_type_embeddings): Embedding(3, 768)\n",
       "  (vit_model): CLIP(\n",
       "    (visual): VisualTransformer(\n",
       "      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): Sequential(\n",
       "          (0): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_transformer): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantizedLinear(in_features=768, out_features=3072, scale=0.1399097442626953, zero_point=59, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantizedLinear(in_features=3072, out_features=768, scale=0.038905270397663116, zero_point=4, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantizedLinear(in_features=768, out_features=3072, scale=0.1395796239376068, zero_point=60, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantizedLinear(in_features=3072, out_features=768, scale=0.034052856266498566, zero_point=5, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4-11): 8 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cross_modal_image_layers): ModuleList(\n",
       "    (0-5): 6 x BertCrossLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (crossattention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cross_modal_text_layers): ModuleList(\n",
       "    (0-5): 6 x BertCrossLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (crossattention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cross_modal_image_pooler): Pooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (cross_modal_text_pooler): Pooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (nlvr2_classifier): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1536, out_features=2, bias=True)\n",
       "  )\n",
       "  (train_nlvr2_accuracy): Accuracy()\n",
       "  (train_nlvr2_loss): Scalar()\n",
       "  (dev_nlvr2_accuracy): Accuracy()\n",
       "  (dev_nlvr2_loss): Scalar()\n",
       "  (test_nlvr2_accuracy): Accuracy()\n",
       "  (test_nlvr2_loss): Scalar()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(model_qat, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 70/70 [00:09<00:00,  7.75it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   nlvr2/dev/accuracy       0.7943925261497498\n",
      "nlvr2/dev/accuracy_epoch    0.8194444179534912\n",
      "     nlvr2/dev/loss         0.5969018340110779\n",
      "  nlvr2/dev/loss_epoch      0.5920029282569885\n",
      "   nlvr2/test/accuracy       0.843137264251709\n",
      "nlvr2/test/accuracy_epoch   0.8208954930305481\n",
      "     nlvr2/test/loss        0.4254521429538727\n",
      "  nlvr2/test/loss_epoch     0.4254521429538727\n",
      "     val/the_metric         0.8208954930305481\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'nlvr2/dev/loss': 0.5969018340110779,\n",
       "  'nlvr2/dev/accuracy': 0.7943925261497498,\n",
       "  'nlvr2/test/loss': 0.4254521429538727,\n",
       "  'nlvr2/test/accuracy': 0.843137264251709,\n",
       "  'nlvr2/dev/accuracy_epoch': 0.8194444179534912,\n",
       "  'nlvr2/dev/loss_epoch': 0.5920029282569885,\n",
       "  'nlvr2/test/accuracy_epoch': 0.8208954930305481,\n",
       "  'nlvr2/test/loss_epoch': 0.4254521429538727,\n",
       "  'val/the_metric': 0.8208954930305481}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model_qat, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_dynamic' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at ../aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:791 [kernel]\nMeta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dynamic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:748\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:788\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    785\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    787\u001b[0m )\n\u001b[0;32m--> 788\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    790\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1018\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:424\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:301\u001b[0m, in \u001b[0;36mMETERTransformerSS.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m    300\u001b[0m     meter_utils\u001b[38;5;241m.\u001b[39mset_task(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_names\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvqa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:267\u001b[0m, in \u001b[0;36mMETERTransformerSS.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Natural Language for Visual Reasoning 2\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlvr2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_tasks:\n\u001b[0;32m--> 267\u001b[0m     ret\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mobjectives\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_nlvr2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# SNLI Visual Entailment\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnli\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_tasks:\n",
      "File \u001b[0;32m~/thesis/meter/modules/objectives.py:181\u001b[0m, in \u001b[0;36mcompute_nlvr2\u001b[0;34m(pl_module, batch)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_nlvr2\u001b[39m(pl_module, batch):\n\u001b[0;32m--> 181\u001b[0m     infer1 \u001b[38;5;241m=\u001b[39m \u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_token_type_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     infer2 \u001b[38;5;241m=\u001b[39m pl_module\u001b[38;5;241m.\u001b[39minfer(\n\u001b[1;32m    185\u001b[0m         batch, mask_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mask_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, image_token_type_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    186\u001b[0m     )\n\u001b[1;32m    188\u001b[0m     cls_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([infer1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m], infer2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/thesis/meter/modules/meter_module.py:204\u001b[0m, in \u001b[0;36mMETERTransformerSS.infer\u001b[0;34m(self, batch, mask_text, mask_image, image_token_type_idx, img)\u001b[0m\n\u001b[1;32m    202\u001b[0m extend_text_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_transformer\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(text_masks, input_shape, device)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_transformer\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 204\u001b[0m     text_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextend_text_masks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    205\u001b[0m text_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_modal_text_transform(text_embeds)\n\u001b[1;32m    207\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit_model(img)\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:562\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    559\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    560\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 562\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:574\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 574\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:472\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 472\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py:57\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_dynamic(\n\u001b[1;32m     54\u001b[0m             x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params\u001b[38;5;241m.\u001b[39m_packed_params\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m         Y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     61\u001b[0m     Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_dynamic_fp16(\n\u001b[1;32m     62\u001b[0m         x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params\u001b[38;5;241m.\u001b[39m_packed_params\n\u001b[1;32m     63\u001b[0m     )\n",
      "File \u001b[0;32m~/envs/.py10/lib/python3.10/site-packages/torch/_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::linear_dynamic' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_dynamic' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at ../aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp:791 [kernel]\nMeta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]\nAutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\nAutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nTracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]\nAutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]\nAutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "trainer.test(model_dynamic, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.2.output.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.intermediate.dense.bias, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.output.dense.weight, Frozen: False\n",
      "Layer: text_transformer.encoder.layer.3.output.dense.bias, Frozen: False\n"
     ]
    }
   ],
   "source": [
    "print_frozen_layers(model_qat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization | PTQ to 8-bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Quantization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Precision Model:\n",
      "Size of the model (MB): 455.900978\n",
      "Fully Quantized Model:\n",
      "Size of the model (MB): 122.099212\n"
     ]
    }
   ],
   "source": [
    "bit8_linear, bit8_embedding = get_quantization_config(8)\n",
    "bit4_linear, bit4_embedding = get_quantization_config(4)\n",
    "bit2_linear, bit2_embedding = get_quantization_config(2)\n",
    "bit1_linear, bit1_embedding = get_quantization_config(1)\n",
    "\n",
    "\n",
    "print(\"Full Precision Model:\")\n",
    "print_size_of_model(model)\n",
    "\n",
    "model_8 = copy.deepcopy(model)\n",
    "model_4 = copy.deepcopy(model)\n",
    "model_2 = copy.deepcopy(model)\n",
    "model_1 = copy.deepcopy(model)\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_8, {torch.nn.Embedding: bit8_embedding}, dtype=torch.quint8, inplace=True\n",
    ")\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_8, {torch.nn.Linear: bit8_linear, torch.nn.LayerNorm: bit8_linear}, dtype=torch.qint8, inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_4, {torch.nn.Embedding: bit4_embedding}, dtype=torch.quint8, inplace=True\n",
    ")\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_4, {torch.nn.Linear: bit4_linear, torch.nn.LayerNorm: bit2_linear}, dtype=torch.qint8, inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_2, {torch.nn.Embedding: bit2_embedding}, dtype=torch.quint8, inplace=True\n",
    ")\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_2, {torch.nn.Linear: bit2_linear, torch.nn.LayerNorm: bit2_linear}, dtype=torch.qint8, inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_1, {torch.nn.Embedding: bit1_embedding}, dtype=torch.quint8, inplace=True\n",
    ")\n",
    "torch.quantization.quantize_dynamic(\n",
    "    model_1, {torch.nn.Linear: bit1_linear, torch.nn.LayerNorm: bit1_linear}, dtype=torch.qint8, inplace=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Fully Quantized Model:\")\n",
    "print_size_of_model(model_8)\n",
    "\n",
    "# print(f\"Quantized Model with only the {layer_to_quantize} layer:\")\n",
    "# print_size_of_model(_model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the block from the model\n",
    "def get_block(model, block_selection):\n",
    "    attrs = block_selection.split('.')\n",
    "    block = model\n",
    "    for attr in attrs:\n",
    "        if '[' in attr and ']' in attr:\n",
    "            attr_name, index = attr[:-1].split('[')\n",
    "            block = getattr(block, attr_name)[int(index)]\n",
    "        else:\n",
    "            block = getattr(block, attr)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit Relative Change (Mean): 0.1676570475101471\n",
      "4-bit Relative Change (Mean): 0.925999104976654\n",
      "2-bit Relative Change (Mean): 0.9999205470085144\n",
      "8-bit Relative Change (Max): 1.0\n",
      "4-bit Relative Change (Max): 1.0\n",
      "2-bit Relative Change (Max): 1.0\n",
      "8-bit Relative Change Percentage (Mean): 16.765705108642578\n",
      "4-bit Relative Change Percentage (Mean): 92.59990692138672\n",
      "2-bit Relative Change Percentage (Mean): 99.9920425415039\n",
      "Top 8-bit weights with largest relative changes: tensor([ 78,  44,  72,  38,  15, 132, 155, 172, 144,  64])\n",
      "Top 4-bit weights with largest relative changes: tensor([ 8,  9,  4,  3,  5,  1,  0, 10,  2,  6])\n",
      "Top 2-bit weights with largest relative changes: tensor([8, 9, 4, 7, 5, 3, 1, 0, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Define the block selection\n",
    "block_selection = 'transformer.blocks[0].mlp.fc1'\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the blocks from each model\n",
    "    block_full_precision = get_block(model, block_selection)\n",
    "    block_8bit = get_block(model_8, block_selection)\n",
    "    block_4bit = get_block(model_4, block_selection)\n",
    "    block_2bit = get_block(model_2, block_selection)\n",
    "\n",
    "    # Dequantize the quantized weights before performing the operation\n",
    "    weight_full_precision = block_full_precision.weight\n",
    "    weight_8bit = block_8bit.weight().dequantize()  # Dequantize 8-bit weights\n",
    "    weight_4bit = block_4bit.weight().dequantize()  # Dequantize 4-bit weights\n",
    "    weight_2bit = block_2bit.weight().dequantize()  # Dequantize 2-bit weights\n",
    "\n",
    "    # Compute relative changes\n",
    "    relative_change_8bit = torch.abs(weight_full_precision - weight_8bit) / torch.abs(weight_full_precision)\n",
    "    relative_change_4bit = torch.abs(weight_full_precision - weight_4bit) / torch.abs(weight_full_precision)\n",
    "    relative_change_2bit = torch.abs(weight_full_precision - weight_2bit) / torch.abs(weight_full_precision)\n",
    "\n",
    "    # Handle division by zero (if any original weight is zero)\n",
    "    relative_change_8bit[torch.isnan(relative_change_8bit)] = 0  # Set NaN to 0\n",
    "    relative_change_4bit[torch.isnan(relative_change_4bit)] = 0  # Set NaN to 0\n",
    "    relative_change_2bit[torch.isnan(relative_change_2bit)] = 0  # Set NaN to 0\n",
    "\n",
    "    # Convert to percentage (optional)\n",
    "    relative_change_8bit_percent = relative_change_8bit * 100\n",
    "    relative_change_4bit_percent = relative_change_4bit * 100\n",
    "    relative_change_2bit_percent = relative_change_2bit * 100\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"8-bit Relative Change (Mean):\", torch.mean(relative_change_8bit).item())\n",
    "    print(\"4-bit Relative Change (Mean):\", torch.mean(relative_change_4bit).item())\n",
    "    print(\"2-bit Relative Change (Mean):\", torch.mean(relative_change_2bit).item())\n",
    "\n",
    "    print(\"8-bit Relative Change (Max):\", torch.max(relative_change_8bit).item())\n",
    "    print(\"4-bit Relative Change (Max):\", torch.max(relative_change_4bit).item())\n",
    "    print(\"2-bit Relative Change (Max):\", torch.max(relative_change_2bit).item())\n",
    "\n",
    "    print(\"8-bit Relative Change Percentage (Mean):\", torch.mean(relative_change_8bit_percent).item())\n",
    "    print(\"4-bit Relative Change Percentage (Mean):\", torch.mean(relative_change_4bit_percent).item())\n",
    "    print(\"2-bit Relative Change Percentage (Mean):\", torch.mean(relative_change_2bit_percent).item())\n",
    "\n",
    "    # Identify weights with the largest relative changes\n",
    "    top_k = 10  # Number of top weights to identify\n",
    "    top_8bit_indices = torch.topk(relative_change_8bit.flatten(), k=top_k).indices\n",
    "    top_4bit_indices = torch.topk(relative_change_4bit.flatten(), k=top_k).indices\n",
    "    top_2bit_indices = torch.topk(relative_change_2bit.flatten(), k=top_k).indices\n",
    "\n",
    "    print(\"Top 8-bit weights with largest relative changes:\", top_8bit_indices)\n",
    "    print(\"Top 4-bit weights with largest relative changes:\", top_4bit_indices)\n",
    "    print(\"Top 2-bit weights with largest relative changes:\", top_2bit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the block selection\n",
    "block_selection = 'transformer.blocks[0].mlp.fc1'\n",
    "with torch.no_grad():\n",
    "    # Get the blocks from each model\n",
    "    block_full_precision = get_block(model, block_selection)\n",
    "    block_8bit = get_block(model_8, block_selection)\n",
    "    block_4bit = get_block(model_4, block_selection)\n",
    "    block_2bit = get_block(model_2, block_selection)\n",
    "\n",
    "    # Dequantize the quantized weights before performing the operation\n",
    "    weight_full_precision = block_full_precision.weight\n",
    "    weight_8bit = block_8bit.weight().dequantize() # .int_repr().float()\n",
    "    weight_4bit = block_4bit.weight().dequantize() # .int_repr().float()\n",
    "    weight_2bit = block_2bit.weight().dequantize() # .int_repr().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.006750397456343222\n",
      "KL Divergence: 17.704822635573848\n",
      "KL Divergence: 24.204427696510965\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Example: Plot histograms\n",
    "plt.hist(weight_full_precision.flatten().detach().numpy(), bins=50, alpha=0.5, label=\"Original\")\n",
    "# plt.hist(weight_8bit.flatten().detach().numpy(), bins=50, alpha=0.5, label=\"8-bit\")\n",
    "# plt.hist(weight_4bit.flatten().detach().numpy(), bins=50, alpha=0.5, label=\"4-bit\")\n",
    "plt.hist(weight_2bit.flatten().detach().numpy(), bins=50, alpha=0.5, label=\"2-bit\")\n",
    "plt.legend()\n",
    "plt.xlim(-1, 1)\n",
    "plt.show()\n",
    "\n",
    "def compute_kl_divergence(original, quantized, bins=50, epsilon=1e-10):\n",
    "    hist_original, _ = np.histogram(original, bins=bins, density=True)\n",
    "    hist_quantized, _ = np.histogram(quantized, bins=bins, density=True)\n",
    "    \n",
    "    # Add epsilon to avoid zero probabilities\n",
    "    hist_original = hist_original + epsilon\n",
    "    hist_quantized = hist_quantized + epsilon\n",
    "    \n",
    "    # Normalize to ensure valid probability distributions\n",
    "    hist_original = hist_original / np.sum(hist_original)\n",
    "    hist_quantized = hist_quantized / np.sum(hist_quantized)\n",
    "    \n",
    "    return entropy(hist_original, hist_quantized)\n",
    "\n",
    "kl_divergence = compute_kl_divergence(weight_full_precision.flatten().detach().numpy(), weight_8bit.flatten().detach().numpy())\n",
    "print(\"KL Divergence:\", kl_divergence)\n",
    "\n",
    "kl_divergence = compute_kl_divergence(weight_full_precision.flatten().detach().numpy(), weight_4bit.flatten().detach().numpy())\n",
    "print(\"KL Divergence:\", kl_divergence)\n",
    "\n",
    "kl_divergence = compute_kl_divergence(weight_full_precision.flatten().detach().numpy(), weight_2bit.flatten().detach().numpy())\n",
    "print(\"KL Divergence:\", kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of the first layer of the transformer blocks:\n",
      "2.4307639598846436\n",
      "\n",
      "Min value of the first layer of the transformer blocks:\n",
      "-3.06199312210083\n",
      "\n",
      "Average of the first layer of the transformer blocks:\n",
      "3.992090205429122e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value of the first layer of the transformer blocks:\")\n",
    "print(weight_full_precision.max().item())\n",
    "print()\n",
    "\n",
    "print(f\"Min value of the first layer of the transformer blocks:\")\n",
    "print(weight_full_precision.min().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average of the first layer of the transformer blocks:\")\n",
    "print(weight_full_precision.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of the first layer of the transformer blocks:\n",
      "2.4255788326263428\n",
      "\n",
      "Min value of the first layer of the transformer blocks:\n",
      "-3.074000835418701\n",
      "\n",
      "Average of the first layer of the transformer blocks:\n",
      "3.338760870974511e-05\n",
      "\n",
      "Average difference between 8-bit version and full precision:\n",
      "0.005997746717184782\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value of the first layer of the transformer blocks:\")\n",
    "print(weight_8bit.max().item())\n",
    "print()\n",
    "\n",
    "print(f\"Min value of the first layer of the transformer blocks:\")\n",
    "print(weight_8bit.min().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average of the first layer of the transformer blocks:\")\n",
    "print(weight_8bit.mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 8-bit version and full precision:\")\n",
    "print(torch.abs(block_8bit.weight().dequantize() - weight_full_precision).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of the first layer of the transformer blocks:\n",
      "2.449594497680664\n",
      "\n",
      "Min value of the first layer of the transformer blocks:\n",
      "-3.2661259174346924\n",
      "\n",
      "Average of the first layer of the transformer blocks:\n",
      "1.4881919923936948e-05\n",
      "\n",
      "Average difference between 4-bit version and full precision:\n",
      "0.04256489500403404\n",
      "\n",
      "Average difference between 4-bit version and 8-bit version:\n",
      "0.042156487703323364\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value of the first layer of the transformer blocks:\")\n",
    "print(weight_4bit.max().item())\n",
    "print()\n",
    "\n",
    "print(f\"Min value of the first layer of the transformer blocks:\")\n",
    "print(weight_4bit.min().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average of the first layer of the transformer blocks:\")\n",
    "print(weight_4bit.mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 4-bit version and full precision:\")\n",
    "print(torch.abs(weight_4bit - weight_full_precision).mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 4-bit version and 8-bit version:\")\n",
    "print(torch.abs(weight_4bit - weight_8bit).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value of the first layer of the transformer blocks:\n",
      "2.0413286685943604\n",
      "\n",
      "Min value of the first layer of the transformer blocks:\n",
      "-4.082657337188721\n",
      "\n",
      "Average of the first layer of the transformer blocks:\n",
      "-1.7304555512964725e-06\n",
      "\n",
      "Average difference between 2-bit version and full precision:\n",
      "0.0428580567240715\n",
      "\n",
      "Average difference between 2-bit version and 8-bit version:\n",
      "0.042456429451704025\n",
      "\n",
      "Average difference between 2-bit version and 4-bit version:\n",
      "0.001149368821643293\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max value of the first layer of the transformer blocks:\")\n",
    "print(weight_2bit.max().item())\n",
    "print()\n",
    "\n",
    "print(f\"Min value of the first layer of the transformer blocks:\")\n",
    "print(weight_2bit.min().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average of the first layer of the transformer blocks:\")\n",
    "print(weight_2bit.mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 2-bit version and full precision:\")\n",
    "print(torch.abs(weight_2bit - weight_full_precision).mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 2-bit version and 8-bit version:\")\n",
    "print(torch.abs(weight_2bit - weight_8bit).mean().item())\n",
    "print()\n",
    "\n",
    "print(f\"Average difference between 2-bit version and 4-bit version:\")\n",
    "print(torch.abs(weight_2bit - weight_4bit).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after quantization:\n",
      "Size (MB): 122.099212\n",
      "ViLTransformerSS(\n",
      "  (text_embeddings): BertEmbeddings(\n",
      "    (word_embeddings): QuantizedEmbedding(num_embeddings=30522, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (position_embeddings): QuantizedEmbedding(num_embeddings=40, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (token_type_embeddings): QuantizedEmbedding(num_embeddings=2, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (quant): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "  )\n",
      "  (token_type_embeddings): QuantizedEmbedding(num_embeddings=3, embedding_dim=768, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "  (transformer): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): DynamicQuantizedLinear(in_features=768, out_features=2304, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (drop): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (pooler): Pooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (nlvr2_classifier): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=1536, out_features=1536, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): DynamicQuantizedLinear(in_features=1536, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (train_nlvr2_accuracy): Accuracy()\n",
      "  (train_nlvr2_loss): Scalar()\n",
      "  (dev_nlvr2_accuracy): Accuracy()\n",
      "  (dev_nlvr2_loss): Scalar()\n",
      "  (test_nlvr2_accuracy): Accuracy()\n",
      "  (test_nlvr2_loss): Scalar()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import dynamic_quantization as dq\n",
    "\n",
    "default_dynamic = copy.deepcopy(model)\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "        default_dynamic, {torch.nn.Embedding, torch.nn.Conv2d}, dtype=torch.quint8, inplace=True\n",
    "    )\n",
    "\n",
    "torch.quantization.quantize_dynamic(\n",
    "        default_dynamic, {torch.nn.Linear, torch.nn.LayerNorm, torch.nn.Conv2d}, dtype=torch.qint8, inplace=True\n",
    "    )\n",
    "\n",
    "print(\"Size after quantization:\")\n",
    "print_size_of_model(default_dynamic)\n",
    "print(default_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Quantizing the model DYNAMIC =========\n",
      "Size after quantization:\n",
      "Size of the model (MB): 122.099212\n"
     ]
    }
   ],
   "source": [
    "import dynamic_quantization as dq\n",
    "\n",
    "custom_8bit = copy.deepcopy(model)\n",
    "custom_8bit = dq.quantize_model_dynamic(custom_8bit, 8)\n",
    "\n",
    "print(\"Size after quantization:\")\n",
    "print_size_of_model(custom_8bit)\n",
    "# print(model_dynamic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Suite Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization._numeric_suite as ns\n",
    "\n",
    "def compute_error(x, y):\n",
    "    \"\"\"\n",
    "    Signal to Noise Ratio (SNR)    \n",
    "    \"\"\"\n",
    "    Ps = torch.norm(x)\n",
    "    Pn = torch.norm(x-y)\n",
    "    return 20*torch.log10(Ps/Pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 27.66\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 21.96\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 26.82\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 18.22\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 32.16\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 29.15\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 27.63\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 16.82\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 33.97\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 36.37\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 32.58\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 16.36\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 33.17\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 36.98\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 31.92\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 30.59\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 34.69\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 38.77\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 31.58\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 27.02\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 33.92\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 38.16\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 35.53\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 29.81\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 35.89\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 33.90\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 31.89\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 16.10\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 33.92\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 37.94\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 23.23\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 15.11\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 35.26\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 36.08\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 18.87\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 18.36\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 33.13\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 30.52\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 22.55\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 21.04\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 30.75\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 34.32\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 33.32\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 29.17\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 30.99\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 26.40\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 28.97\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 19.50\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 36.56\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 38.95\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 40.77\n",
      "Total error: 1515.32\n",
      "Total inf: 28\n",
      "Max error: 40.76809310913086\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), custom_8bit.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_batch = next(iter(infer_dataloader))\n",
    "# calibration_batch = next(iter(calibrarte_dm.val_dataloader()))\n",
    "# full_batch = next(iter(full_dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(custom_8bit), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 38.81\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 38.69\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -1.60\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 3.39\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - -2.19\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.20\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 3.85\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - -2.46\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -1.73\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 3.21\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - -1.97\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -0.67\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 3.16\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - -2.50\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -1.57\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 0.13\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - -2.05\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -0.66\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 3.54\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - -2.46\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -1.65\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 0.45\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - -1.99\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -0.67\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 3.90\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -2.23\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -1.68\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 0.99\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - -1.25\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -0.83\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 4.14\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -2.07\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -1.69\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 1.50\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - -1.40\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -0.87\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 4.58\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -1.89\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -1.64\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 2.44\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - -1.32\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -0.91\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 4.66\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -2.15\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -1.52\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 6.07\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - -0.54\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -0.87\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 4.99\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - -1.42\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -1.36\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 8.05\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - -0.38\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -0.91\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 4.71\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - -2.26\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -1.24\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 9.07\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - -0.18\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -0.76\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 6.01\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - -0.98\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -0.34\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 9.05\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - 3.20\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - 0.71\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 8.63\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.76\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - 0.41\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 10.08\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 10.70\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - 1.52\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 8.40\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - 6.50\n",
      "75 - transformer.norm.stats\n",
      "75 - 9.95\n",
      "76 - pooler.dense.stats\n",
      "76 - 11.90\n",
      "Total error: 180.13\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 27.66\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 21.96\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 26.82\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 18.22\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 32.16\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 29.15\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 27.63\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 16.82\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 33.97\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 36.37\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 32.58\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 16.36\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 33.17\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 36.98\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 31.92\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 30.59\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 34.69\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 38.77\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 31.58\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 27.02\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 33.92\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 38.16\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 35.53\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 29.81\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 35.89\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 33.90\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 31.89\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 16.10\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 33.92\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 37.94\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 23.23\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 15.11\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 35.26\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 36.08\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 18.87\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 18.36\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 33.13\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 30.52\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 22.55\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 21.04\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 30.75\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 34.32\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 33.32\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 29.17\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 30.99\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 26.40\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 28.97\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 19.50\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 36.56\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 38.95\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 40.77\n",
      "Total error: 1515.32\n",
      "Total inf: 28\n",
      "Max error: 40.76809310913086\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), model_8.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_8), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 38.81\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 38.69\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -1.87\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 3.13\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - -2.44\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.42\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 3.71\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - -2.41\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -1.86\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 3.06\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - -2.14\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -0.78\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 3.07\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - -2.50\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -1.65\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 0.03\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - -2.07\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -0.73\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 3.47\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - -2.55\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -1.74\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 0.31\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - -2.06\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -0.78\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 3.71\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -2.34\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -1.78\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 0.85\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - -1.29\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -0.95\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 3.95\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -2.10\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -1.77\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 1.38\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - -1.42\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -0.97\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 4.47\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -1.92\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -1.72\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 2.34\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - -1.42\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -1.01\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 4.62\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -4.56\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -1.62\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 5.93\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - -0.88\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -0.98\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 4.90\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - -0.90\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -1.46\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 7.89\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - -0.44\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -1.01\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 4.61\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - -1.31\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -1.34\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 8.92\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - 0.01\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -0.83\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 6.02\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - -0.62\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -0.39\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 8.98\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - 1.76\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - 0.46\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 8.36\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.35\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - 0.15\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 9.63\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 2.55\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - 1.39\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 6.98\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - 4.02\n",
      "75 - transformer.norm.stats\n",
      "75 - 6.09\n",
      "76 - pooler.dense.stats\n",
      "76 - -3.00\n",
      "Total error: 139.56\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-bit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 4.57\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 0.87\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 3.18\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 0.22\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 7.66\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 4.76\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 3.51\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 0.10\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 9.37\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 11.76\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 7.98\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 0.05\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 8.57\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 12.35\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 7.33\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 6.00\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 10.08\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 14.17\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 6.98\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 2.84\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 9.31\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 13.54\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 10.93\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 5.26\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 11.29\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 9.31\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 7.29\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 0.17\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 9.30\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 13.32\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 0.78\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 0.09\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 10.64\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 11.48\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 0.12\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 0.10\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 8.53\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 5.93\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 0.53\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 0.38\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 6.15\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 9.72\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 8.71\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 4.98\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 6.48\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 2.43\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 4.49\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 0.62\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 11.96\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 14.34\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 16.09\n",
      "Total error: 336.60\n",
      "Total inf: 28\n",
      "Max error: 16.087890625\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), model_4.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_batch = next(iter(infer_dataloader))\n",
    "# calibration_batch = next(iter(calibrarte_dm.val_dataloader()))\n",
    "# full_batch = next(iter(full_dm.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_4), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 14.32\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - 14.07\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -1.89\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 3.17\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - -0.62\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.70\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 2.84\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - -0.42\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -2.18\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 3.19\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - -3.70\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -1.32\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 2.90\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - -0.18\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -2.15\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 0.48\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - -1.89\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -1.27\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 3.55\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - -0.07\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -2.28\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 0.64\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - -2.12\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -1.36\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 3.62\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -2.47\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -2.39\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 1.03\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - -2.84\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -1.66\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 3.35\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -3.62\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -2.55\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 1.45\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - -2.82\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -1.95\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 3.00\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -4.29\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -2.63\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 2.43\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - -2.55\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -2.13\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 3.03\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -0.59\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -2.59\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 5.74\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - -2.11\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -2.20\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 2.01\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - 0.53\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -2.59\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 7.28\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - -2.04\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -2.34\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 0.48\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - -2.11\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -2.61\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 7.78\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - -2.57\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -2.36\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 2.87\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - -1.64\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -2.42\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 7.05\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - -1.44\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - -2.54\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 3.15\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - -0.11\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - -2.66\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 6.56\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 3.69\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - -2.45\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 2.79\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - -0.90\n",
      "75 - transformer.norm.stats\n",
      "75 - -0.89\n",
      "76 - pooler.dense.stats\n",
      "76 - -1.28\n",
      "Total error: 17.57\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 0.39\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 0.07\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 0.04\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 0.03\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 0.32\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 0.03\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 0.04\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 0.03\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 0.27\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 0.47\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 0.07\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 0.02\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 0.08\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 0.59\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 0.02\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 0.01\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 0.19\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 1.32\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 0.01\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 0.00\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 0.09\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 0.99\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 0.26\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 0.02\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 0.32\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 0.06\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 0.02\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 0.11\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 0.06\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 0.88\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 0.05\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 0.07\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 0.19\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 0.31\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 0.03\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 0.03\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 0.04\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 0.02\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 0.00\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 0.01\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 0.02\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 0.13\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 0.05\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 0.00\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 0.04\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 0.23\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 0.01\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 0.07\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 1.10\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 1.35\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 2.87\n",
      "Total error: 13.46\n",
      "Total inf: 28\n",
      "Max error: 2.8716225624084473\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), model_2.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_2), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 3.22\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - -0.85\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -2.04\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 4.84\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - 0.19\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.89\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 0.47\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - 0.06\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -2.27\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 4.57\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - 0.09\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -1.63\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 0.43\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - 0.00\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -2.29\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 1.54\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - -0.06\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -1.64\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 0.72\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - 0.03\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -2.42\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 2.12\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - -0.07\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -1.65\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 0.55\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -0.06\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -2.51\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 2.25\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - -0.42\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -1.86\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 0.45\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -0.06\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -2.58\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 3.01\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - -0.18\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -1.97\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 1.75\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -0.05\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -2.68\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 3.46\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - -0.01\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -2.09\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 0.51\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -0.02\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -2.72\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 7.11\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - -0.26\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -2.21\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 0.25\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - -0.34\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -1.79\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 8.48\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - -0.07\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -1.66\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - -0.37\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - -12.75\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -0.56\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 9.11\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - 0.21\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -0.85\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - -0.09\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - -12.50\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -0.30\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 6.60\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - -0.01\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - -0.34\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 0.34\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.26\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - -0.49\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 6.93\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - -2.09\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - -1.66\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 0.11\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - 0.05\n",
      "75 - transformer.norm.stats\n",
      "75 - -0.41\n",
      "76 - pooler.dense.stats\n",
      "76 - -0.39\n",
      "Total error: -3.45\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.weight\n",
      "0 - inf\n",
      "1 - transformer.patch_embed.proj.weight\n",
      "1 - inf\n",
      "2 - transformer.blocks.0.norm1.weight\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.attn.qkv._packed_params._packed_params\n",
      "3 - 0.00\n",
      "4 - transformer.blocks.0.attn.proj._packed_params._packed_params\n",
      "4 - 0.00\n",
      "5 - transformer.blocks.0.norm2.weight\n",
      "5 - inf\n",
      "6 - transformer.blocks.0.mlp.fc1._packed_params._packed_params\n",
      "6 - 0.00\n",
      "7 - transformer.blocks.0.mlp.fc2._packed_params._packed_params\n",
      "7 - 0.00\n",
      "8 - transformer.blocks.1.norm1.weight\n",
      "8 - inf\n",
      "9 - transformer.blocks.1.attn.qkv._packed_params._packed_params\n",
      "9 - 0.00\n",
      "10 - transformer.blocks.1.attn.proj._packed_params._packed_params\n",
      "10 - 0.00\n",
      "11 - transformer.blocks.1.norm2.weight\n",
      "11 - inf\n",
      "12 - transformer.blocks.1.mlp.fc1._packed_params._packed_params\n",
      "12 - 0.00\n",
      "13 - transformer.blocks.1.mlp.fc2._packed_params._packed_params\n",
      "13 - 0.00\n",
      "14 - transformer.blocks.2.norm1.weight\n",
      "14 - inf\n",
      "15 - transformer.blocks.2.attn.qkv._packed_params._packed_params\n",
      "15 - 0.00\n",
      "16 - transformer.blocks.2.attn.proj._packed_params._packed_params\n",
      "16 - 0.00\n",
      "17 - transformer.blocks.2.norm2.weight\n",
      "17 - inf\n",
      "18 - transformer.blocks.2.mlp.fc1._packed_params._packed_params\n",
      "18 - 0.00\n",
      "19 - transformer.blocks.2.mlp.fc2._packed_params._packed_params\n",
      "19 - 0.00\n",
      "20 - transformer.blocks.3.norm1.weight\n",
      "20 - inf\n",
      "21 - transformer.blocks.3.attn.qkv._packed_params._packed_params\n",
      "21 - 0.00\n",
      "22 - transformer.blocks.3.attn.proj._packed_params._packed_params\n",
      "22 - 0.00\n",
      "23 - transformer.blocks.3.norm2.weight\n",
      "23 - inf\n",
      "24 - transformer.blocks.3.mlp.fc1._packed_params._packed_params\n",
      "24 - 0.00\n",
      "25 - transformer.blocks.3.mlp.fc2._packed_params._packed_params\n",
      "25 - 0.00\n",
      "26 - transformer.blocks.4.norm1.weight\n",
      "26 - inf\n",
      "27 - transformer.blocks.4.attn.qkv._packed_params._packed_params\n",
      "27 - 0.00\n",
      "28 - transformer.blocks.4.attn.proj._packed_params._packed_params\n",
      "28 - 0.00\n",
      "29 - transformer.blocks.4.norm2.weight\n",
      "29 - inf\n",
      "30 - transformer.blocks.4.mlp.fc1._packed_params._packed_params\n",
      "30 - 0.00\n",
      "31 - transformer.blocks.4.mlp.fc2._packed_params._packed_params\n",
      "31 - 0.00\n",
      "32 - transformer.blocks.5.norm1.weight\n",
      "32 - inf\n",
      "33 - transformer.blocks.5.attn.qkv._packed_params._packed_params\n",
      "33 - 0.00\n",
      "34 - transformer.blocks.5.attn.proj._packed_params._packed_params\n",
      "34 - 0.00\n",
      "35 - transformer.blocks.5.norm2.weight\n",
      "35 - inf\n",
      "36 - transformer.blocks.5.mlp.fc1._packed_params._packed_params\n",
      "36 - 0.00\n",
      "37 - transformer.blocks.5.mlp.fc2._packed_params._packed_params\n",
      "37 - 0.00\n",
      "38 - transformer.blocks.6.norm1.weight\n",
      "38 - inf\n",
      "39 - transformer.blocks.6.attn.qkv._packed_params._packed_params\n",
      "39 - 0.00\n",
      "40 - transformer.blocks.6.attn.proj._packed_params._packed_params\n",
      "40 - 0.00\n",
      "41 - transformer.blocks.6.norm2.weight\n",
      "41 - inf\n",
      "42 - transformer.blocks.6.mlp.fc1._packed_params._packed_params\n",
      "42 - 0.00\n",
      "43 - transformer.blocks.6.mlp.fc2._packed_params._packed_params\n",
      "43 - 0.00\n",
      "44 - transformer.blocks.7.norm1.weight\n",
      "44 - inf\n",
      "45 - transformer.blocks.7.attn.qkv._packed_params._packed_params\n",
      "45 - 0.00\n",
      "46 - transformer.blocks.7.attn.proj._packed_params._packed_params\n",
      "46 - 0.00\n",
      "47 - transformer.blocks.7.norm2.weight\n",
      "47 - inf\n",
      "48 - transformer.blocks.7.mlp.fc1._packed_params._packed_params\n",
      "48 - 0.00\n",
      "49 - transformer.blocks.7.mlp.fc2._packed_params._packed_params\n",
      "49 - 0.00\n",
      "50 - transformer.blocks.8.norm1.weight\n",
      "50 - inf\n",
      "51 - transformer.blocks.8.attn.qkv._packed_params._packed_params\n",
      "51 - 0.00\n",
      "52 - transformer.blocks.8.attn.proj._packed_params._packed_params\n",
      "52 - 0.00\n",
      "53 - transformer.blocks.8.norm2.weight\n",
      "53 - inf\n",
      "54 - transformer.blocks.8.mlp.fc1._packed_params._packed_params\n",
      "54 - 0.00\n",
      "55 - transformer.blocks.8.mlp.fc2._packed_params._packed_params\n",
      "55 - 0.00\n",
      "56 - transformer.blocks.9.norm1.weight\n",
      "56 - inf\n",
      "57 - transformer.blocks.9.attn.qkv._packed_params._packed_params\n",
      "57 - 0.00\n",
      "58 - transformer.blocks.9.attn.proj._packed_params._packed_params\n",
      "58 - 0.00\n",
      "59 - transformer.blocks.9.norm2.weight\n",
      "59 - inf\n",
      "60 - transformer.blocks.9.mlp.fc1._packed_params._packed_params\n",
      "60 - 0.00\n",
      "61 - transformer.blocks.9.mlp.fc2._packed_params._packed_params\n",
      "61 - 0.00\n",
      "62 - transformer.blocks.10.norm1.weight\n",
      "62 - inf\n",
      "63 - transformer.blocks.10.attn.qkv._packed_params._packed_params\n",
      "63 - 0.00\n",
      "64 - transformer.blocks.10.attn.proj._packed_params._packed_params\n",
      "64 - 0.00\n",
      "65 - transformer.blocks.10.norm2.weight\n",
      "65 - inf\n",
      "66 - transformer.blocks.10.mlp.fc1._packed_params._packed_params\n",
      "66 - 0.00\n",
      "67 - transformer.blocks.10.mlp.fc2._packed_params._packed_params\n",
      "67 - 0.00\n",
      "68 - transformer.blocks.11.norm1.weight\n",
      "68 - inf\n",
      "69 - transformer.blocks.11.attn.qkv._packed_params._packed_params\n",
      "69 - 0.00\n",
      "70 - transformer.blocks.11.attn.proj._packed_params._packed_params\n",
      "70 - 0.00\n",
      "71 - transformer.blocks.11.norm2.weight\n",
      "71 - inf\n",
      "72 - transformer.blocks.11.mlp.fc1._packed_params._packed_params\n",
      "72 - 0.00\n",
      "73 - transformer.blocks.11.mlp.fc2._packed_params._packed_params\n",
      "73 - 0.00\n",
      "74 - transformer.norm.weight\n",
      "74 - inf\n",
      "75 - pooler.dense._packed_params._packed_params\n",
      "75 - 0.00\n",
      "76 - nlvr2_classifier.0._packed_params._packed_params\n",
      "76 - 0.00\n",
      "77 - nlvr2_classifier.1.weight\n",
      "77 - inf\n",
      "78 - nlvr2_classifier.3._packed_params._packed_params\n",
      "78 - 0.00\n",
      "Total error: 0.00\n",
      "Total inf: 28\n",
      "Max error: 0\n"
     ]
    }
   ],
   "source": [
    "# ======== Dynamic quantization comparison ========\n",
    "wt_compare_dict_dynamic = ns.compare_weights(model.state_dict(), model_1.state_dict())\n",
    "\n",
    "# print('keys of wt_compare_dict:')\n",
    "# print(wt_compare_dict_dynamic.keys())\n",
    "\n",
    "# key = 'text_embeddings.LayerNorm.weight'\n",
    "\n",
    "total_error = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for i, key in enumerate(wt_compare_dict_dynamic):\n",
    "    if wt_compare_dict_dynamic[key]['quantized'].is_quantized:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'].dequantize())\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "        \n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "    else:\n",
    "        err = compute_error(wt_compare_dict_dynamic[key]['float'], wt_compare_dict_dynamic[key]['quantized'])\n",
    "        \n",
    "        print(f\"{i} - {key}\")\n",
    "        print(f\"{i} - {err:.2f}\")\n",
    "\n",
    "        if not torch.isinf(err):\n",
    "            total_error += err\n",
    "            if err > max_err:\n",
    "                max_err = err\n",
    "        else:\n",
    "            inf_count += 1\n",
    "\n",
    "print(f\"Total error: {total_error:.2f}\")\n",
    "print(f\"Total inf: {inf_count}\")\n",
    "print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text_embeddings.LayerNorm.stats', 'text_embeddings.quant.stats', 'transformer.patch_embed.proj.stats', 'transformer.blocks.0.norm1.stats', 'transformer.blocks.0.attn.qkv.stats', 'transformer.blocks.0.attn.proj.stats', 'transformer.blocks.0.norm2.stats', 'transformer.blocks.0.mlp.fc1.stats', 'transformer.blocks.0.mlp.fc2.stats', 'transformer.blocks.1.norm1.stats', 'transformer.blocks.1.attn.qkv.stats', 'transformer.blocks.1.attn.proj.stats', 'transformer.blocks.1.norm2.stats', 'transformer.blocks.1.mlp.fc1.stats', 'transformer.blocks.1.mlp.fc2.stats', 'transformer.blocks.2.norm1.stats', 'transformer.blocks.2.attn.qkv.stats', 'transformer.blocks.2.attn.proj.stats', 'transformer.blocks.2.norm2.stats', 'transformer.blocks.2.mlp.fc1.stats', 'transformer.blocks.2.mlp.fc2.stats', 'transformer.blocks.3.norm1.stats', 'transformer.blocks.3.attn.qkv.stats', 'transformer.blocks.3.attn.proj.stats', 'transformer.blocks.3.norm2.stats', 'transformer.blocks.3.mlp.fc1.stats', 'transformer.blocks.3.mlp.fc2.stats', 'transformer.blocks.4.norm1.stats', 'transformer.blocks.4.attn.qkv.stats', 'transformer.blocks.4.attn.proj.stats', 'transformer.blocks.4.norm2.stats', 'transformer.blocks.4.mlp.fc1.stats', 'transformer.blocks.4.mlp.fc2.stats', 'transformer.blocks.5.norm1.stats', 'transformer.blocks.5.attn.qkv.stats', 'transformer.blocks.5.attn.proj.stats', 'transformer.blocks.5.norm2.stats', 'transformer.blocks.5.mlp.fc1.stats', 'transformer.blocks.5.mlp.fc2.stats', 'transformer.blocks.6.norm1.stats', 'transformer.blocks.6.attn.qkv.stats', 'transformer.blocks.6.attn.proj.stats', 'transformer.blocks.6.norm2.stats', 'transformer.blocks.6.mlp.fc1.stats', 'transformer.blocks.6.mlp.fc2.stats', 'transformer.blocks.7.norm1.stats', 'transformer.blocks.7.attn.qkv.stats', 'transformer.blocks.7.attn.proj.stats', 'transformer.blocks.7.norm2.stats', 'transformer.blocks.7.mlp.fc1.stats', 'transformer.blocks.7.mlp.fc2.stats', 'transformer.blocks.8.norm1.stats', 'transformer.blocks.8.attn.qkv.stats', 'transformer.blocks.8.attn.proj.stats', 'transformer.blocks.8.norm2.stats', 'transformer.blocks.8.mlp.fc1.stats', 'transformer.blocks.8.mlp.fc2.stats', 'transformer.blocks.9.norm1.stats', 'transformer.blocks.9.attn.qkv.stats', 'transformer.blocks.9.attn.proj.stats', 'transformer.blocks.9.norm2.stats', 'transformer.blocks.9.mlp.fc1.stats', 'transformer.blocks.9.mlp.fc2.stats', 'transformer.blocks.10.norm1.stats', 'transformer.blocks.10.attn.qkv.stats', 'transformer.blocks.10.attn.proj.stats', 'transformer.blocks.10.norm2.stats', 'transformer.blocks.10.mlp.fc1.stats', 'transformer.blocks.10.mlp.fc2.stats', 'transformer.blocks.11.norm1.stats', 'transformer.blocks.11.attn.qkv.stats', 'transformer.blocks.11.attn.proj.stats', 'transformer.blocks.11.norm2.stats', 'transformer.blocks.11.mlp.fc1.stats', 'transformer.blocks.11.mlp.fc2.stats', 'transformer.norm.stats', 'pooler.dense.stats'])\n"
     ]
    }
   ],
   "source": [
    "# act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_dynamic), full_batch)\n",
    "act_compare_dict_dynamic = ns.compare_model_outputs(copy.deepcopy(model), copy.deepcopy(model_1), infer_batch)\n",
    "print(act_compare_dict_dynamic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - text_embeddings.LayerNorm.stats\n",
      "0 - 2.06\n",
      "1 - text_embeddings.quant.stats\n",
      "1 - -12.40\n",
      "2 - transformer.patch_embed.proj.stats\n",
      "2 - inf\n",
      "3 - transformer.blocks.0.norm1.stats\n",
      "3 - -2.05\n",
      "4 - transformer.blocks.0.attn.qkv.stats\n",
      "4 - 5.05\n",
      "5 - transformer.blocks.0.attn.proj.stats\n",
      "5 - 0.34\n",
      "6 - transformer.blocks.0.norm2.stats\n",
      "6 - -1.95\n",
      "7 - transformer.blocks.0.mlp.fc1.stats\n",
      "7 - 0.40\n",
      "8 - transformer.blocks.0.mlp.fc2.stats\n",
      "8 - 0.03\n",
      "9 - transformer.blocks.1.norm1.stats\n",
      "9 - -2.34\n",
      "10 - transformer.blocks.1.attn.qkv.stats\n",
      "10 - 4.93\n",
      "11 - transformer.blocks.1.attn.proj.stats\n",
      "11 - 0.09\n",
      "12 - transformer.blocks.1.norm2.stats\n",
      "12 - -1.64\n",
      "13 - transformer.blocks.1.mlp.fc1.stats\n",
      "13 - 0.30\n",
      "14 - transformer.blocks.1.mlp.fc2.stats\n",
      "14 - 0.00\n",
      "15 - transformer.blocks.2.norm1.stats\n",
      "15 - -2.38\n",
      "16 - transformer.blocks.2.attn.qkv.stats\n",
      "16 - 1.82\n",
      "17 - transformer.blocks.2.attn.proj.stats\n",
      "17 - 0.04\n",
      "18 - transformer.blocks.2.norm2.stats\n",
      "18 - -1.67\n",
      "19 - transformer.blocks.2.mlp.fc1.stats\n",
      "19 - 0.29\n",
      "20 - transformer.blocks.2.mlp.fc2.stats\n",
      "20 - -0.01\n",
      "21 - transformer.blocks.3.norm1.stats\n",
      "21 - -2.52\n",
      "22 - transformer.blocks.3.attn.qkv.stats\n",
      "22 - 2.19\n",
      "23 - transformer.blocks.3.attn.proj.stats\n",
      "23 - 0.03\n",
      "24 - transformer.blocks.3.norm2.stats\n",
      "24 - -1.71\n",
      "25 - transformer.blocks.3.mlp.fc1.stats\n",
      "25 - 0.28\n",
      "26 - transformer.blocks.3.mlp.fc2.stats\n",
      "26 - -0.05\n",
      "27 - transformer.blocks.4.norm1.stats\n",
      "27 - -2.60\n",
      "28 - transformer.blocks.4.attn.qkv.stats\n",
      "28 - 2.58\n",
      "29 - transformer.blocks.4.attn.proj.stats\n",
      "29 - 0.02\n",
      "30 - transformer.blocks.4.norm2.stats\n",
      "30 - -1.90\n",
      "31 - transformer.blocks.4.mlp.fc1.stats\n",
      "31 - 0.29\n",
      "32 - transformer.blocks.4.mlp.fc2.stats\n",
      "32 - -0.06\n",
      "33 - transformer.blocks.5.norm1.stats\n",
      "33 - -2.67\n",
      "34 - transformer.blocks.5.attn.qkv.stats\n",
      "34 - 3.19\n",
      "35 - transformer.blocks.5.attn.proj.stats\n",
      "35 - 0.03\n",
      "36 - transformer.blocks.5.norm2.stats\n",
      "36 - -2.01\n",
      "37 - transformer.blocks.5.mlp.fc1.stats\n",
      "37 - 0.26\n",
      "38 - transformer.blocks.5.mlp.fc2.stats\n",
      "38 - -0.02\n",
      "39 - transformer.blocks.6.norm1.stats\n",
      "39 - -2.71\n",
      "40 - transformer.blocks.6.attn.qkv.stats\n",
      "40 - 4.13\n",
      "41 - transformer.blocks.6.attn.proj.stats\n",
      "41 - 0.02\n",
      "42 - transformer.blocks.6.norm2.stats\n",
      "42 - -2.14\n",
      "43 - transformer.blocks.6.mlp.fc1.stats\n",
      "43 - 0.23\n",
      "44 - transformer.blocks.6.mlp.fc2.stats\n",
      "44 - -0.00\n",
      "45 - transformer.blocks.7.norm1.stats\n",
      "45 - -2.71\n",
      "46 - transformer.blocks.7.attn.qkv.stats\n",
      "46 - 7.23\n",
      "47 - transformer.blocks.7.attn.proj.stats\n",
      "47 - 0.01\n",
      "48 - transformer.blocks.7.norm2.stats\n",
      "48 - -2.18\n",
      "49 - transformer.blocks.7.mlp.fc1.stats\n",
      "49 - 0.18\n",
      "50 - transformer.blocks.7.mlp.fc2.stats\n",
      "50 - -0.00\n",
      "51 - transformer.blocks.8.norm1.stats\n",
      "51 - -2.67\n",
      "52 - transformer.blocks.8.attn.qkv.stats\n",
      "52 - 8.87\n",
      "53 - transformer.blocks.8.attn.proj.stats\n",
      "53 - 0.00\n",
      "54 - transformer.blocks.8.norm2.stats\n",
      "54 - -2.26\n",
      "55 - transformer.blocks.8.mlp.fc1.stats\n",
      "55 - 0.15\n",
      "56 - transformer.blocks.8.mlp.fc2.stats\n",
      "56 - 0.00\n",
      "57 - transformer.blocks.9.norm1.stats\n",
      "57 - -2.56\n",
      "58 - transformer.blocks.9.attn.qkv.stats\n",
      "58 - 9.63\n",
      "59 - transformer.blocks.9.attn.proj.stats\n",
      "59 - 0.15\n",
      "60 - transformer.blocks.9.norm2.stats\n",
      "60 - -2.12\n",
      "61 - transformer.blocks.9.mlp.fc1.stats\n",
      "61 - 0.09\n",
      "62 - transformer.blocks.9.mlp.fc2.stats\n",
      "62 - 0.03\n",
      "63 - transformer.blocks.10.norm1.stats\n",
      "63 - -2.13\n",
      "64 - transformer.blocks.10.attn.qkv.stats\n",
      "64 - 6.19\n",
      "65 - transformer.blocks.10.attn.proj.stats\n",
      "65 - -0.01\n",
      "66 - transformer.blocks.10.norm2.stats\n",
      "66 - -1.91\n",
      "67 - transformer.blocks.10.mlp.fc1.stats\n",
      "67 - 0.04\n",
      "68 - transformer.blocks.10.mlp.fc2.stats\n",
      "68 - 0.03\n",
      "69 - transformer.blocks.11.norm1.stats\n",
      "69 - -2.11\n",
      "70 - transformer.blocks.11.attn.qkv.stats\n",
      "70 - 7.11\n",
      "71 - transformer.blocks.11.attn.proj.stats\n",
      "71 - 0.04\n",
      "72 - transformer.blocks.11.norm2.stats\n",
      "72 - -4.14\n",
      "73 - transformer.blocks.11.mlp.fc1.stats\n",
      "73 - 0.12\n",
      "74 - transformer.blocks.11.mlp.fc2.stats\n",
      "74 - -0.02\n",
      "75 - transformer.norm.stats\n",
      "75 - 0.39\n",
      "76 - pooler.dense.stats\n",
      "76 - 0.10\n",
      "Total error: 1.38\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "inf_count = 0\n",
    "max_err = 0\n",
    "for idx, key in enumerate(act_compare_dict_dynamic):\n",
    "    err = compute_error(act_compare_dict_dynamic[key]['float'][0][0], act_compare_dict_dynamic[key]['quantized'][0][0])\n",
    "    # print(type(err))\n",
    "    if torch.isinf(err):\n",
    "        inf_count += 1\n",
    "    else:\n",
    "        total_err += err\n",
    "        if err > max_err:\n",
    "            max_err = err\n",
    "    print(f\"{idx} - {key}\")\n",
    "    print(f\"{idx} - {err:.2f}\")\n",
    "\n",
    "print(f\"Total error: {total_err:.2f}\")\n",
    "# print(f\"Total inf: {inf_count}\")\n",
    "# print(f\"Max error: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
